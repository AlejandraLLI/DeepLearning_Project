{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSiA 432 - Deep Learning - Final Project\n",
    "## Group: Alejandra Lelo de Larrea Ibarra, Bannasorn Paspanthong, Ruben Nakano, Samuel Swain\n",
    "# Style Transfering\n",
    "\n",
    "Reference: https://anderfernandez.com/en/blog/how-to-code-neural-style-transfer-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import Model\n",
    "from keras.utils import get_file, plot_model\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.applications import vgg19\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to activate GPUs \n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print (f'Found GPU at {device_name}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Images \n",
    "#Ccontent_img = \"https://tourism.euskadi.eus/contenidos/d_destinos_turisticos/0000004981_d2_rec_turismo/en_4981/images/CT_cabecerabilbaoguggen.jpg\"\n",
    "#content_img = \"https://scontent-ord5-2.xx.fbcdn.net/v/t39.30808-6/278359286_5531142070232702_580676501400778849_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=09cbfe&_nc_ohc=Tbe2XBdOZyAAX_65zMT&_nc_ht=scontent-ord5-2.xx&oh=00_AfDrdgtCpdzgbBWIPp1aa3hEs4o0GJiACTUA6dyhVJ4aoA&oe=64577035\"\n",
    "#style_img = \"https://i.imgur.com/9ooB60I.jpg\"\n",
    "\n",
    "#content_img_path = get_file(fname = \"a&jl.jpg\", origin = content_img)\n",
    "#style_img_path = get_file(fname = \"skyscraper.jpg\", origin = style_img)\n",
    "\n",
    "#content_img_path = \"../02_Data/face_age/010/755.png\"\n",
    "content_img_path = \"../02_Data/face_age/010/7012.png\"\n",
    "#style_img_path = \"../02_Data/face_age/080/262.png\"\n",
    "#style_img_path = \"../02_Data/Vilma picapiedra.png\"\n",
    "#style_img_path = \"../02_Data/Barbie.jpeg\"\n",
    "#style_img_path = \"../02_Data/WonderWomen.webp\"\n",
    "#style_img_path = '../02_Data/Voldemort.jpeg'\n",
    "style_img_path = '../02_Data/Palpatin.jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the images \n",
    "base = plt.imread(content_img_path)\n",
    "style = plt.imread(style_img_path)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (15,15))\n",
    "ax[0].imshow(base)\n",
    "ax[1].imshow(style)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram-Matrix & Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_style(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "def cost_content(base, combination): \n",
    "    return tf.reduce_sum(tf.square(combination-base))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-trained model VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VGG19\n",
    "model = vgg19.VGG19(weights = \"imagenet\", include_top = False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract the value for some layers ---\n",
    "# List of model layers\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# Extract features \n",
    "feature_extractor = Model(inputs = model.inputs, outputs = outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layers to extract for style - Use first convolution of each block to calculate loss. \n",
    "style_layers = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "\n",
    "# Define layers to extract for the content - Use the second convolution of the last block.\n",
    "content_layers = \"block5_conv2\"\n",
    "\n",
    "# Initialize weights. \n",
    "content_weight = 2.5e-8\n",
    "style_weight = 1e-6\n",
    "\n",
    "\n",
    "def loss_function(combination_image, base_image, style_reference_image):\n",
    "\n",
    "    # 1. Combine all the images in the same tensioner.\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "\n",
    "    # 2. Get the values in all the layers for the three images.\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    #3. Inicializar the loss in zero\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # 4. Extract the content layers + content loss\n",
    "    layer_features = features[content_layers]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "\n",
    "    loss = loss + content_weight * cost_content(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    # 5. Extract the style layers + style loss\n",
    "    for layer_name in style_layers:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = cost_style(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layers)) * sl\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Phase: Optimize Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GradientTape to calculate gradient (optimal values)\n",
    "@tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_function(combination_image, base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and deprocessing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give images the format required by the network. \n",
    "# VGG19 has a preprocess_input function\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures into appropriate tensors\n",
    "\n",
    "    # Load the image\n",
    "    #img = keras.preprocessing.image.load_img(\n",
    "    img = tf.keras.utils.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "\n",
    "    # Convert image to an array \n",
    "    #img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = tf.keras.utils.img_to_array(img)\n",
    "    \n",
    "    # Group images in a single array of dim (3 ,width, height, 3)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # Pre-process: substract mean of RGB vaues. \n",
    "    img = vgg19.preprocess_input(img)\n",
    "\n",
    "    # convert to tensor\n",
    "    return tf.convert_to_tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert the processing of images \n",
    "\n",
    "def deprocess_image(x):\n",
    "\n",
    "    # Convert tensor into array\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "\n",
    "    # Add average value for each channel\n",
    "    # Values can be found here: https://github.com/DeepVoltaire/AutoAugment/issues/4\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "\n",
    "    # Convert from BGR a RGB.\n",
    "    x = x[:, :, ::-1]\n",
    "\n",
    "    # Make values between 0 and 255\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Of Neural Style Transfer Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparams: initial learning rate, decay steps, decay rate. \n",
    "ilr=100.0\n",
    "ds=100\n",
    "dr=0.96\n",
    "\n",
    "# Define number of iterations \n",
    "iterations = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result every n iterations (If required)\n",
    "def result_saver(iteration):\n",
    "  # Create name\n",
    "  now = datetime.now()\n",
    "  now = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "  image_name = str(i) + '_' + str(now)+\"_image\" + '.png'\n",
    "\n",
    "  # Save image\n",
    "  img = deprocess_image(combination_image.numpy())\n",
    "  #keras.preprocessing.image.save_img(image_name, img)\n",
    "  tf.keras.utils.save_img(image_name,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define width and height \n",
    "#width, height = keras.preprocessing.image.load_img(base_img_path).size\n",
    "width, height = tf.keras.utils.load_img(content_img_path).size\n",
    "\n",
    "# Define number of rows and columns \n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)\n",
    "\n",
    "# Set optimizer \n",
    "optimizer = SGD(\n",
    "    #keras.optimizers.schedules.ExponentialDecay(\n",
    "    tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=ilr, decay_steps=ds, decay_rate=dr\n",
    "    )\n",
    ")\n",
    "\n",
    "# Preprocess content image, style and combination (content)\n",
    "content_image = preprocess_image(content_img_path)\n",
    "style_reference_image = preprocess_image(style_img_path)\n",
    "combination_image = tf.Variable(preprocess_image(content_img_path))\n",
    "\n",
    "# Loop to get gradients \n",
    "for i in range(1, iterations + 1):\n",
    "    # update loss and gradient\n",
    "    loss, grads = compute_loss_and_grads(\n",
    "        combination_image, content_image, style_reference_image\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "\n",
    "    # Print progress every 500 iterations\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        result_saver(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as im \n",
    "\n",
    "# Print final image \n",
    "final_img = deprocess_image(combination_image.numpy())\n",
    "im.fromarray(final_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSiA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
