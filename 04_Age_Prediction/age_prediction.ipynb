{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 09:29:43.111048: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-19 09:29:43.281963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 09:29:46.125984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, \n",
    "                          Dense, Dropout, BatchNormalization, \n",
    "                          LeakyReLU, DepthwiseConv2D, MaxPooling2D,\n",
    "                          Add, Input, Activation, GlobalAveragePooling2D,\n",
    "                          Multiply, Reshape)\n",
    "\n",
    "\n",
    "from keras.utils import get_file\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "\n",
    "# Set Tensorflow log level to error \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Set logger level to ERROR\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)  # Set logger level to ERROR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sets\n",
    "raw_train_path = \"../02_Data/face_age_data/face_age_balanced_train\"\n",
    "aug_train_path = \"../02_Data/face_age_data/augmented_data_train\" \n",
    "\n",
    "\n",
    "# Validation set\n",
    "raw_val_path = \"../02_Data/face_age_data/face_age_balanced_val\"\n",
    "\n",
    "# Testing set\n",
    "raw_test_path = \"../02_Data/face_age_data/face_age_balanced_test\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folders, img_size=(200, 200)):\n",
    "    X = []\n",
    "    y = []\n",
    "    for folder_path in folders:\n",
    "        for folder in os.listdir(folder_path):\n",
    "            if os.path.isdir(os.path.join(folder_path, folder)):\n",
    "                age = int(folder.replace(\"aug_\", \"\"))\n",
    "                for file in os.listdir(os.path.join(folder_path, folder)):\n",
    "                    img_path = os.path.join(folder_path, folder, file)\n",
    "                    img = Image.open(img_path)\n",
    "                    img = img.resize(img_size)\n",
    "                    img = np.array(img)\n",
    "                    X.append(img)\n",
    "                    y.append(age)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (200, 200)\n",
    "\n",
    "# # Load training set\n",
    "# folder_paths_train = [\n",
    "#     raw_train_path, \n",
    "#     aug_train_path\n",
    "#     ]\n",
    "# X_train, y_train = load_data(folder_paths_train, img_size)\n",
    "# X_train = X_train / 255.0  # Normalize pixel values\n",
    "\n",
    "# Load validation set\n",
    "folder_paths_val = [raw_val_path]\n",
    "X_val, y_val = load_data(folder_paths_val, img_size)\n",
    "X_val = X_val / 255.0  # Normalize pixel values\n",
    "\n",
    "# Load testing set\n",
    "folder_paths_test = [raw_test_path]\n",
    "X_test, y_test = load_data(folder_paths_test, img_size)\n",
    "X_test = X_test / 255.0  # Normalize pixel values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pixel value in X_train after normalization: 1.0\n",
      "Min pixel value in X_train after normalization: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load raw training set\n",
    "X_raw_train, y_raw_train = load_data([raw_train_path], img_size)\n",
    "X_raw_train = X_raw_train / 255.0  # Normalize pixel values\n",
    "\n",
    "# Load augmented training set\n",
    "X_aug_train, y_aug_train = load_data([aug_train_path], img_size)\n",
    "X_aug_train = X_aug_train / 255.0  # Normalize pixel values\n",
    "\n",
    "# Concatenate raw and augmented training set\n",
    "X_train = np.concatenate((X_raw_train, X_aug_train), axis=0)\n",
    "y_train = np.concatenate((y_raw_train, y_aug_train), axis=0)\n",
    "\n",
    "# Now check the maximum and minimum pixel values in X_train\n",
    "print(\"Max pixel value in X_train after normalization:\", np.max(X_train))\n",
    "print(\"Min pixel value in X_train after normalization:\", np.min(X_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (45000, 200, 200, 3) \n",
      " - 45000: Number of images in the dataset \n",
      " - 200: Height of each image \n",
      " - 200: Width of each image \n",
      " - 3: Number of channels of each image (Red, Green, and Blue)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Shape of X: {X_train.shape}\", \"\\n\",\n",
    "    f\"- {X_train.shape[0]}: Number of images in the dataset\", \"\\n\",\n",
    "    f\"- {X_train.shape[1]}: Height of each image\", \"\\n\",\n",
    "    f\"- {X_train.shape[2]}: Width of each image\", \"\\n\",\n",
    "    f\"- {X_train.shape[3]}: Number of channels of each image (Red, Green, and Blue)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [32 32 32 ... 58 58 58]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels: {y_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of arrays: \n",
      " - X_train shape: (45000, 200, 200, 3) \n",
      " - y_train shape: (45000,) \n",
      " - X_val shape: (1400, 200, 200, 3) \n",
      " - y_val shape: (1400,) \n",
      " - X_test shape: (1750, 200, 200, 3) \n",
      " - y_test shape: (1750,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Size of arrays:\", \"\\n\",\n",
    "    f\"- X_train shape: {X_train.shape}\", \"\\n\",\n",
    "    f\"- y_train shape: {y_train.shape}\", \"\\n\",\n",
    "    f\"- X_val shape: {X_val.shape}\", \"\\n\",\n",
    "    f\"- y_val shape: {y_val.shape}\", \"\\n\",\n",
    "    f\"- X_test shape: {X_test.shape}\", \"\\n\",\n",
    "    f\"- y_test shape: {y_test.shape}\", \"\\n\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  5\n",
      "Using GPUs:  ['/physical_device:GPU:0', '/physical_device:GPU:1', '/physical_device:GPU:2', '/physical_device:GPU:3', '/physical_device:GPU:4']\n",
      "['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3', '/device:GPU:4']\n"
     ]
    }
   ],
   "source": [
    "def set_gpus(*gpu_indices):\n",
    "    # Get list of GPUs\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    print(\"Num GPUs Available: \", len(gpus))\n",
    "\n",
    "    if gpus:\n",
    "        visible_gpus = [gpus[i] for i in gpu_indices]\n",
    "        try:\n",
    "            # Only use specified GPUs and ignore the others\n",
    "            tf.config.experimental.set_visible_devices(visible_gpus, 'GPU')\n",
    "            for gpu in visible_gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"Using GPUs: \", [gpu.name for gpu in visible_gpus])\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    return visible_gpus\n",
    "\n",
    "# Call the function with the indices of the GPUs you want to use\n",
    "visible_gpus = set_gpus(0, 1, 2, 3, 4)  # Use the second, third, fourth, and fifth GPU\n",
    "\n",
    "# Extract names of the GPUs being used\n",
    "gpu_names = ['/device:GPU:' + gpu.name.split(':')[-1] for gpu in visible_gpus]\n",
    "print(gpu_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "optimizer = \"adam\"\n",
    "loss = \"mean_squared_error\"\n",
    "metrics = ['mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 09:34:34.853062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 36265 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2023-05-19 09:34:34.853882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46669 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:1b:00.0, compute capability: 8.6\n",
      "2023-05-19 09:34:34.854475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 46669 MB memory:  -> device: 2, name: NVIDIA RTX A6000, pci bus id: 0000:88:00.0, compute capability: 8.6\n",
      "2023-05-19 09:34:34.855118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 46669 MB memory:  -> device: 3, name: NVIDIA RTX A6000, pci bus id: 0000:89:00.0, compute capability: 8.6\n",
      "2023-05-19 09:34:34.855581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 46669 MB memory:  -> device: 4, name: NVIDIA RTX A6000, pci bus id: 0000:b1:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "# Create a MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_names)\n",
    "\n",
    "# 16\n",
    "# .h5\n",
    "# Augmented + Raw data\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (7, 7), padding='same', input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(128, (5, 5), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(1024),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(64),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# Open the strategy scope\n",
    "with strategy.scope():\n",
    "    input_shape = (img_size[0], img_size[1], 3)\n",
    "    model = create_model(input_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback to save the model's weights\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"../04_Age_Prediction/08_models/best_model.h5\", save_best_only=True)\n",
    "\n",
    "# Create a callback that stops the training if there is no improvement in the validation loss for 15 consecutive epochs\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 09:36:02.582212: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [45000]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-05-19 09:36:02.582577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [45000]\n",
      "\t [[{{node Placeholder/_11}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 09:36:17.899369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-19 09:36:20.071575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-19 09:36:22.500834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-19 09:36:25.035293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-19 09:36:26.978498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-19 09:36:27.565076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-19 09:36:31.002154: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f9059c60720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-19 09:36:31.002205: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-05-19 09:36:31.002218: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-05-19 09:36:31.002228: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-05-19 09:36:31.002237: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-05-19 09:36:31.002246: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (4): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-05-19 09:36:31.014521: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-19 09:36:31.227208: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - ETA: 0s - loss: 1009.4771 - mae: 24.6954"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 09:37:34.886301: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [1750]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-05-19 09:37:34.886610: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [1750]\n",
      "\t [[{{node Placeholder/_11}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 97s 47ms/step - loss: 1009.4771 - mae: 24.6954 - val_loss: 186.8329 - val_mae: 10.5188\n",
      "Epoch 2/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 303.4913 - mae: 13.7771 - val_loss: 255.7952 - val_mae: 12.5859\n",
      "Epoch 3/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 264.2071 - mae: 12.8367 - val_loss: 190.3511 - val_mae: 10.5854\n",
      "Epoch 4/1000\n",
      "1407/1407 [==============================] - 60s 42ms/step - loss: 247.4597 - mae: 12.4124 - val_loss: 107.2017 - val_mae: 7.9611\n",
      "Epoch 5/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 235.8208 - mae: 12.1309 - val_loss: 117.6483 - val_mae: 8.5901\n",
      "Epoch 6/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 228.3560 - mae: 11.9184 - val_loss: 119.5101 - val_mae: 8.3382\n",
      "Epoch 7/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 222.0118 - mae: 11.7467 - val_loss: 103.2419 - val_mae: 7.7737\n",
      "Epoch 8/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 217.3979 - mae: 11.6471 - val_loss: 115.0733 - val_mae: 8.0711\n",
      "Epoch 9/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 207.5612 - mae: 11.3766 - val_loss: 107.6681 - val_mae: 7.6797\n",
      "Epoch 10/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 211.6757 - mae: 11.5174 - val_loss: 92.1772 - val_mae: 7.3453\n",
      "Epoch 11/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 208.4482 - mae: 11.4561 - val_loss: 98.7562 - val_mae: 7.6521\n",
      "Epoch 12/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 204.9203 - mae: 11.2922 - val_loss: 86.2997 - val_mae: 7.0192\n",
      "Epoch 13/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 203.0489 - mae: 11.2540 - val_loss: 89.0382 - val_mae: 7.1399\n",
      "Epoch 14/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 199.0384 - mae: 11.1551 - val_loss: 100.1821 - val_mae: 7.6517\n",
      "Epoch 15/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 197.8867 - mae: 11.1259 - val_loss: 89.9532 - val_mae: 7.1818\n",
      "Epoch 16/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 195.2923 - mae: 11.0465 - val_loss: 105.9153 - val_mae: 7.9310\n",
      "Epoch 17/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 196.2132 - mae: 11.0835 - val_loss: 95.9049 - val_mae: 7.4536\n",
      "Epoch 18/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 194.5252 - mae: 11.0144 - val_loss: 91.9370 - val_mae: 7.2352\n",
      "Epoch 19/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 187.9038 - mae: 10.8385 - val_loss: 109.8256 - val_mae: 8.1127\n",
      "Epoch 20/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 189.1681 - mae: 10.8994 - val_loss: 98.1102 - val_mae: 7.4370\n",
      "Epoch 21/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 190.3088 - mae: 10.9205 - val_loss: 88.8033 - val_mae: 7.1222\n",
      "Epoch 22/1000\n",
      "1407/1407 [==============================] - 57s 40ms/step - loss: 186.5851 - mae: 10.7992 - val_loss: 89.8047 - val_mae: 7.2502\n",
      "Epoch 23/1000\n",
      "1407/1407 [==============================] - 60s 42ms/step - loss: 182.8141 - mae: 10.7049 - val_loss: 84.3633 - val_mae: 7.0084\n",
      "Epoch 24/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 182.7946 - mae: 10.6763 - val_loss: 130.3424 - val_mae: 8.7698\n",
      "Epoch 25/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 180.2073 - mae: 10.6364 - val_loss: 106.3849 - val_mae: 7.8162\n",
      "Epoch 26/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 183.4905 - mae: 10.7275 - val_loss: 86.5760 - val_mae: 7.0794\n",
      "Epoch 27/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 183.1854 - mae: 10.7172 - val_loss: 96.1219 - val_mae: 7.5625\n",
      "Epoch 28/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 180.9291 - mae: 10.6824 - val_loss: 93.3205 - val_mae: 7.3026\n",
      "Epoch 29/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 181.7179 - mae: 10.7032 - val_loss: 87.4291 - val_mae: 7.0422\n",
      "Epoch 30/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 179.6555 - mae: 10.6466 - val_loss: 103.0695 - val_mae: 7.8595\n",
      "Epoch 31/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 181.4662 - mae: 10.6864 - val_loss: 85.4361 - val_mae: 7.0015\n",
      "Epoch 32/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 180.3242 - mae: 10.6489 - val_loss: 84.0844 - val_mae: 6.9714\n",
      "Epoch 33/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 179.2955 - mae: 10.6124 - val_loss: 87.7100 - val_mae: 7.0496\n",
      "Epoch 34/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 178.5788 - mae: 10.6202 - val_loss: 85.2275 - val_mae: 6.9802\n",
      "Epoch 35/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 175.1943 - mae: 10.4934 - val_loss: 90.2069 - val_mae: 7.2160\n",
      "Epoch 36/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 175.6284 - mae: 10.5115 - val_loss: 90.5225 - val_mae: 7.1903\n",
      "Epoch 37/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 176.7615 - mae: 10.5588 - val_loss: 100.7544 - val_mae: 7.6104\n",
      "Epoch 38/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 177.3165 - mae: 10.5683 - val_loss: 87.1623 - val_mae: 7.1343\n",
      "Epoch 39/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 174.4092 - mae: 10.4687 - val_loss: 100.7140 - val_mae: 7.8017\n",
      "Epoch 40/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 173.7509 - mae: 10.4805 - val_loss: 88.9793 - val_mae: 7.1447\n",
      "Epoch 41/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 176.2725 - mae: 10.5451 - val_loss: 86.5545 - val_mae: 7.0829\n",
      "Epoch 42/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 174.8677 - mae: 10.5009 - val_loss: 88.1666 - val_mae: 7.1980\n",
      "Epoch 43/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 173.7294 - mae: 10.4480 - val_loss: 83.2025 - val_mae: 6.9611\n",
      "Epoch 44/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 167.8436 - mae: 10.2994 - val_loss: 89.9049 - val_mae: 7.1567\n",
      "Epoch 45/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 172.2405 - mae: 10.4237 - val_loss: 100.4772 - val_mae: 7.7343\n",
      "Epoch 46/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 172.6185 - mae: 10.4617 - val_loss: 83.1594 - val_mae: 6.9716\n",
      "Epoch 47/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 171.3496 - mae: 10.4265 - val_loss: 82.1023 - val_mae: 6.8457\n",
      "Epoch 48/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 172.1116 - mae: 10.4494 - val_loss: 88.7278 - val_mae: 7.2014\n",
      "Epoch 49/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 172.0436 - mae: 10.3917 - val_loss: 84.0091 - val_mae: 6.9613\n",
      "Epoch 50/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 167.9208 - mae: 10.3240 - val_loss: 85.8617 - val_mae: 7.0413\n",
      "Epoch 51/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 165.0741 - mae: 10.2270 - val_loss: 81.0945 - val_mae: 6.8826\n",
      "Epoch 52/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 165.2032 - mae: 10.2054 - val_loss: 81.0452 - val_mae: 6.8555\n",
      "Epoch 53/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 169.9329 - mae: 10.3455 - val_loss: 92.8964 - val_mae: 7.3908\n",
      "Epoch 54/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 164.6419 - mae: 10.1884 - val_loss: 79.9427 - val_mae: 6.7944\n",
      "Epoch 55/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 168.4000 - mae: 10.3093 - val_loss: 82.6682 - val_mae: 6.9528\n",
      "Epoch 56/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 170.7777 - mae: 10.3954 - val_loss: 79.2934 - val_mae: 6.8078\n",
      "Epoch 57/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 168.6461 - mae: 10.3188 - val_loss: 81.2443 - val_mae: 6.9073\n",
      "Epoch 58/1000\n",
      "1407/1407 [==============================] - 57s 40ms/step - loss: 170.3294 - mae: 10.3614 - val_loss: 81.3384 - val_mae: 6.8352\n",
      "Epoch 59/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 167.0498 - mae: 10.2967 - val_loss: 88.9904 - val_mae: 7.2319\n",
      "Epoch 60/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 165.8457 - mae: 10.2376 - val_loss: 85.1499 - val_mae: 6.9946\n",
      "Epoch 61/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 162.4111 - mae: 10.1244 - val_loss: 98.1687 - val_mae: 7.5480\n",
      "Epoch 62/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 165.8197 - mae: 10.2459 - val_loss: 83.4305 - val_mae: 6.9341\n",
      "Epoch 63/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 166.2055 - mae: 10.2537 - val_loss: 88.5344 - val_mae: 7.1927\n",
      "Epoch 64/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 163.0092 - mae: 10.1338 - val_loss: 82.0852 - val_mae: 6.9575\n",
      "Epoch 65/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 164.9731 - mae: 10.2274 - val_loss: 81.0026 - val_mae: 6.8954\n",
      "Epoch 66/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 160.4271 - mae: 10.0566 - val_loss: 83.2694 - val_mae: 6.9617\n",
      "Epoch 67/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 165.9791 - mae: 10.2435 - val_loss: 83.4148 - val_mae: 6.9283\n",
      "Epoch 68/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 164.8570 - mae: 10.2092 - val_loss: 88.5011 - val_mae: 7.1858\n",
      "Epoch 69/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 165.0719 - mae: 10.2272 - val_loss: 79.1824 - val_mae: 6.7596\n",
      "Epoch 70/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 167.3522 - mae: 10.2883 - val_loss: 89.0865 - val_mae: 7.1556\n",
      "Epoch 71/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 161.3449 - mae: 10.0870 - val_loss: 82.7702 - val_mae: 6.9181\n",
      "Epoch 72/1000\n",
      "1407/1407 [==============================] - 57s 40ms/step - loss: 160.5762 - mae: 10.0973 - val_loss: 82.7223 - val_mae: 6.9547\n",
      "Epoch 73/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 158.3810 - mae: 9.9895 - val_loss: 82.0617 - val_mae: 6.9417\n",
      "Epoch 74/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 159.3370 - mae: 10.0525 - val_loss: 80.4724 - val_mae: 6.8518\n",
      "Epoch 75/1000\n",
      "1407/1407 [==============================] - 58s 42ms/step - loss: 161.6034 - mae: 10.1156 - val_loss: 78.9126 - val_mae: 6.8118\n",
      "Epoch 76/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 159.9475 - mae: 10.0492 - val_loss: 80.5478 - val_mae: 6.8032\n",
      "Epoch 77/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 159.1513 - mae: 10.0448 - val_loss: 82.4797 - val_mae: 6.9104\n",
      "Epoch 78/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 157.5151 - mae: 9.9860 - val_loss: 83.7829 - val_mae: 7.0069\n",
      "Epoch 79/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 158.1381 - mae: 9.9995 - val_loss: 81.8858 - val_mae: 6.9005\n",
      "Epoch 80/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 160.3635 - mae: 10.0690 - val_loss: 80.4113 - val_mae: 6.8544\n",
      "Epoch 81/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 158.3012 - mae: 10.0051 - val_loss: 84.7237 - val_mae: 7.1202\n",
      "Epoch 82/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 160.1582 - mae: 10.0673 - val_loss: 83.6964 - val_mae: 6.9851\n",
      "Epoch 83/1000\n",
      "1407/1407 [==============================] - 57s 40ms/step - loss: 160.6562 - mae: 10.0948 - val_loss: 81.3186 - val_mae: 6.8342\n",
      "Epoch 84/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 158.9658 - mae: 10.0151 - val_loss: 80.5423 - val_mae: 6.8782\n",
      "Epoch 85/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 157.0148 - mae: 9.9796 - val_loss: 92.5700 - val_mae: 7.3379\n",
      "Epoch 86/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 168.0012 - mae: 10.2864 - val_loss: 79.8968 - val_mae: 6.7721\n",
      "Epoch 87/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 157.4472 - mae: 9.9768 - val_loss: 78.4866 - val_mae: 6.7889\n",
      "Epoch 88/1000\n",
      "1407/1407 [==============================] - 57s 40ms/step - loss: 156.8057 - mae: 9.9412 - val_loss: 79.1895 - val_mae: 6.8054\n",
      "Epoch 89/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 156.2758 - mae: 9.9602 - val_loss: 79.4818 - val_mae: 6.8177\n",
      "Epoch 90/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 156.6047 - mae: 9.9734 - val_loss: 78.4171 - val_mae: 6.7574\n",
      "Epoch 91/1000\n",
      "1407/1407 [==============================] - 57s 40ms/step - loss: 154.7955 - mae: 9.9044 - val_loss: 78.4477 - val_mae: 6.7870\n",
      "Epoch 92/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 158.7759 - mae: 10.0369 - val_loss: 76.9741 - val_mae: 6.6999\n",
      "Epoch 93/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 154.6956 - mae: 9.9159 - val_loss: 76.5007 - val_mae: 6.6771\n",
      "Epoch 94/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 155.2337 - mae: 9.9183 - val_loss: 77.1349 - val_mae: 6.6655\n",
      "Epoch 95/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 157.6623 - mae: 10.0379 - val_loss: 84.8102 - val_mae: 7.1578\n",
      "Epoch 96/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 157.0420 - mae: 9.9538 - val_loss: 79.7633 - val_mae: 6.8526\n",
      "Epoch 97/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 154.2570 - mae: 9.9036 - val_loss: 78.9838 - val_mae: 6.7675\n",
      "Epoch 98/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 155.6444 - mae: 9.9302 - val_loss: 79.8225 - val_mae: 6.8209\n",
      "Epoch 99/1000\n",
      "1407/1407 [==============================] - 57s 40ms/step - loss: 155.9075 - mae: 9.9431 - val_loss: 79.9331 - val_mae: 6.8016\n",
      "Epoch 100/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 157.1828 - mae: 10.0094 - val_loss: 78.9814 - val_mae: 6.7537\n",
      "Epoch 101/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 153.6671 - mae: 9.9055 - val_loss: 79.2007 - val_mae: 6.8200\n",
      "Epoch 102/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 155.5851 - mae: 9.9490 - val_loss: 82.7530 - val_mae: 6.9868\n",
      "Epoch 103/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 156.1706 - mae: 9.9593 - val_loss: 84.3960 - val_mae: 7.0572\n",
      "Epoch 104/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 154.3778 - mae: 9.9006 - val_loss: 78.0315 - val_mae: 6.7337\n",
      "Epoch 105/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 154.9563 - mae: 9.9063 - val_loss: 79.4335 - val_mae: 6.8490\n",
      "Epoch 106/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 153.1558 - mae: 9.8754 - val_loss: 77.8635 - val_mae: 6.7523\n",
      "Epoch 107/1000\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 151.1472 - mae: 9.8214 - val_loss: 76.2378 - val_mae: 6.7211\n",
      "Epoch 108/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 154.4930 - mae: 9.9124 - val_loss: 79.5016 - val_mae: 6.8203\n",
      "Epoch 109/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 154.2022 - mae: 9.8651 - val_loss: 81.5557 - val_mae: 6.8634\n",
      "Epoch 110/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 154.0903 - mae: 9.8910 - val_loss: 82.9557 - val_mae: 7.0054\n",
      "Epoch 111/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 153.8055 - mae: 9.8991 - val_loss: 90.7035 - val_mae: 7.1786\n",
      "Epoch 112/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 148.0693 - mae: 9.6744 - val_loss: 81.9994 - val_mae: 6.9549\n",
      "Epoch 113/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 153.1369 - mae: 9.8503 - val_loss: 80.6367 - val_mae: 6.8803\n",
      "Epoch 114/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 150.8824 - mae: 9.7946 - val_loss: 78.0974 - val_mae: 6.7604\n",
      "Epoch 115/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 149.8407 - mae: 9.7475 - val_loss: 79.0879 - val_mae: 6.7611\n",
      "Epoch 116/1000\n",
      "1407/1407 [==============================] - 57s 40ms/step - loss: 149.6015 - mae: 9.7579 - val_loss: 80.9038 - val_mae: 6.9304\n",
      "Epoch 117/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 151.7575 - mae: 9.8061 - val_loss: 78.7577 - val_mae: 6.8057\n",
      "Epoch 118/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 152.9313 - mae: 9.8865 - val_loss: 82.8191 - val_mae: 6.9662\n",
      "Epoch 119/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 152.6069 - mae: 9.8320 - val_loss: 80.4502 - val_mae: 6.8849\n",
      "Epoch 120/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 152.4213 - mae: 9.8328 - val_loss: 83.5177 - val_mae: 6.9921\n",
      "Epoch 121/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 149.8928 - mae: 9.7551 - val_loss: 76.4601 - val_mae: 6.6774\n",
      "Epoch 122/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 151.0682 - mae: 9.8037 - val_loss: 80.3626 - val_mae: 6.8759\n",
      "Epoch 123/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 154.6038 - mae: 9.8869 - val_loss: 79.1194 - val_mae: 6.8349\n",
      "Epoch 124/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 150.8925 - mae: 9.8009 - val_loss: 76.4544 - val_mae: 6.6591\n",
      "Epoch 125/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 150.0925 - mae: 9.7500 - val_loss: 79.4619 - val_mae: 6.7723\n",
      "Epoch 126/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 149.9363 - mae: 9.7599 - val_loss: 82.5082 - val_mae: 6.8491\n",
      "Epoch 127/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 151.9747 - mae: 9.8251 - val_loss: 79.6762 - val_mae: 6.7827\n",
      "Epoch 128/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 151.0896 - mae: 9.7928 - val_loss: 78.6495 - val_mae: 6.7852\n",
      "Epoch 129/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 148.3972 - mae: 9.6986 - val_loss: 81.4919 - val_mae: 6.9180\n",
      "Epoch 130/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 148.1366 - mae: 9.6906 - val_loss: 80.7136 - val_mae: 6.9477\n",
      "Epoch 131/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 152.9203 - mae: 9.8762 - val_loss: 80.8162 - val_mae: 6.9421\n",
      "Epoch 132/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 151.6513 - mae: 9.8583 - val_loss: 90.0193 - val_mae: 7.1158\n",
      "Epoch 133/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 150.4566 - mae: 9.7505 - val_loss: 78.0708 - val_mae: 6.7627\n",
      "Epoch 134/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 146.0520 - mae: 9.6247 - val_loss: 78.9451 - val_mae: 6.7730\n",
      "Epoch 135/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 151.3561 - mae: 9.8181 - val_loss: 80.1068 - val_mae: 6.8375\n",
      "Epoch 136/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 151.3504 - mae: 9.8123 - val_loss: 79.3045 - val_mae: 6.8300\n",
      "Epoch 137/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 147.6177 - mae: 9.6999 - val_loss: 81.6877 - val_mae: 6.8896\n",
      "Epoch 138/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 149.5903 - mae: 9.7554 - val_loss: 80.0110 - val_mae: 6.8596\n",
      "Epoch 139/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 149.6256 - mae: 9.7961 - val_loss: 78.4748 - val_mae: 6.7696\n",
      "Epoch 140/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 147.3853 - mae: 9.6882 - val_loss: 78.7665 - val_mae: 6.7309\n",
      "Epoch 141/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 150.5511 - mae: 9.7803 - val_loss: 79.5961 - val_mae: 6.8010\n",
      "Epoch 142/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 146.4815 - mae: 9.6398 - val_loss: 80.2736 - val_mae: 6.7876\n",
      "Epoch 143/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 147.6945 - mae: 9.7063 - val_loss: 79.5490 - val_mae: 6.7761\n",
      "Epoch 144/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 146.3423 - mae: 9.6283 - val_loss: 83.2048 - val_mae: 6.9443\n",
      "Epoch 145/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 150.4813 - mae: 9.7983 - val_loss: 85.1879 - val_mae: 7.0479\n",
      "Epoch 146/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 149.0832 - mae: 9.7459 - val_loss: 81.6951 - val_mae: 6.7865\n",
      "Epoch 147/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 145.4344 - mae: 9.6176 - val_loss: 78.7085 - val_mae: 6.7191\n",
      "Epoch 148/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 147.9320 - mae: 9.7141 - val_loss: 78.8577 - val_mae: 6.7144\n",
      "Epoch 149/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 147.5961 - mae: 9.6670 - val_loss: 79.9170 - val_mae: 6.8077\n",
      "Epoch 150/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 148.8763 - mae: 9.7320 - val_loss: 80.0517 - val_mae: 6.7826\n",
      "Epoch 151/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 146.3452 - mae: 9.6213 - val_loss: 80.5972 - val_mae: 6.8655\n",
      "Epoch 152/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 145.9724 - mae: 9.6372 - val_loss: 78.3161 - val_mae: 6.7661\n",
      "Epoch 153/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 146.9291 - mae: 9.7035 - val_loss: 82.0979 - val_mae: 6.9058\n",
      "Epoch 154/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 146.3083 - mae: 9.6661 - val_loss: 87.9919 - val_mae: 7.2475\n",
      "Epoch 155/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 148.7318 - mae: 9.7329 - val_loss: 86.5129 - val_mae: 7.1071\n",
      "Epoch 156/1000\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 150.0626 - mae: 9.7711 - val_loss: 77.6587 - val_mae: 6.7305\n",
      "Epoch 157/1000\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 147.2851 - mae: 9.6986 - val_loss: 84.9327 - val_mae: 7.0615\n"
     ]
    }
   ],
   "source": [
    "# Train the model with your data\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, Load, and Display Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model attributes\n",
    "timestamp = int(time.time())\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "description = f\"{timestamp}\"\n",
    "input_shape = str(X_train.shape[1:])\n",
    "num_params = model.count_params()\n",
    "epochs = len(history.history['loss'])\n",
    "optimizer = optimizer\n",
    "loss_function = loss\n",
    "train_mae = min(history.history['mae'])\n",
    "validation_mae = min(history.history['val_mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ../04_Age_Prediction/08_models/1684498043.h5\n"
     ]
    }
   ],
   "source": [
    "def append_model_history_to_file(file_name, description, input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae):\n",
    "    train_mae = round(train_mae, 6)\n",
    "    validation_mae = round(validation_mae, 6)\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        with open(file_name, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['Model', 'Input Shape', 'Model Params', 'Epochs', 'Batch Size', 'Optimizer', 'Loss Function', 'Train MAE', 'Validation MAE'])\n",
    "\n",
    "    with open(file_name, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        try:\n",
    "            next(csv_reader)  # Skip the header row\n",
    "        except StopIteration:\n",
    "            pass  # The CSV file is empty or only contains the header row\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if list(map(str, row[1:])) == list(map(str, [input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])):\n",
    "                print(\"Entry with the same parameters already exists.\")\n",
    "                return False\n",
    "\n",
    "    with open(file_name, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([description, input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])\n",
    "\n",
    "    return True\n",
    "\n",
    "new_entry_added = append_model_history_to_file(file_name, description, input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae)\n",
    "\n",
    "# Save model\n",
    "def save_model_architecture(model, file_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "if new_entry_added:\n",
    "    model_file = f\"../04_Age_Prediction/08_models/{description}.h5\"\n",
    "    model.save(model_file)\n",
    "    print(f\"Model saved at {model_file}\")\n",
    "else:\n",
    "    print(\"Model not saved as an entry with the same parameters already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input Shape</th>\n",
       "      <th>Model Params</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Loss Function</th>\n",
       "      <th>Train MAE</th>\n",
       "      <th>Validation MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1684111562</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>8770497</td>\n",
       "      <td>155</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>12.435976</td>\n",
       "      <td>8.629502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1684118998</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>33625217</td>\n",
       "      <td>132</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.202019</td>\n",
       "      <td>6.590096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1684126499</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>18144705</td>\n",
       "      <td>170</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.910834</td>\n",
       "      <td>6.491637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1684130813</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>9872321</td>\n",
       "      <td>213</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.675910</td>\n",
       "      <td>7.109921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1684133411</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>10344897</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.852635</td>\n",
       "      <td>7.511125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1684148562</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>77897345</td>\n",
       "      <td>212</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.725429</td>\n",
       "      <td>7.512293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1684208696</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>82101291</td>\n",
       "      <td>312</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.901710</td>\n",
       "      <td>7.452464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1684212422</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>11331745</td>\n",
       "      <td>108</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.285194</td>\n",
       "      <td>7.056834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1684219988</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>18217393</td>\n",
       "      <td>219</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1.237246</td>\n",
       "      <td>5.716532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1684300148</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>8770497</td>\n",
       "      <td>152</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.255740</td>\n",
       "      <td>6.360036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1684320186</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>33625217</td>\n",
       "      <td>75</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>8.891773</td>\n",
       "      <td>6.597941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1684394481</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>18144705</td>\n",
       "      <td>81</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>7.961771</td>\n",
       "      <td>5.572220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1684407927</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>18217393</td>\n",
       "      <td>79</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>2.391651</td>\n",
       "      <td>5.277592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1684485106</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>9872321</td>\n",
       "      <td>123</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.606898</td>\n",
       "      <td>6.405786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1684498043</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>10344897</td>\n",
       "      <td>157</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.617646</td>\n",
       "      <td>6.659074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model    Input Shape  Model Params  Epochs  Batch Size Optimizer  \\\n",
       "0   1684111562  (200, 200, 3)       8770497     155          32      adam   \n",
       "1   1684118998  (200, 200, 3)      33625217     132          32      adam   \n",
       "2   1684126499  (200, 200, 3)      18144705     170          32      adam   \n",
       "3   1684130813  (200, 200, 3)       9872321     213          32      adam   \n",
       "4   1684133411  (200, 200, 3)      10344897     168          32      adam   \n",
       "5   1684148562  (200, 200, 3)      77897345     212          32      adam   \n",
       "6   1684208696  (200, 200, 3)      82101291     312          32      adam   \n",
       "7   1684212422  (200, 200, 3)      11331745     108          32      adam   \n",
       "8   1684219988  (200, 200, 3)      18217393     219          32      adam   \n",
       "9   1684300148  (200, 200, 3)       8770497     152          32      adam   \n",
       "10  1684320186  (200, 200, 3)      33625217      75          32      adam   \n",
       "11  1684394481  (200, 200, 3)      18144705      81          32      adam   \n",
       "12  1684407927  (200, 200, 3)      18217393      79          32      adam   \n",
       "13  1684485106  (200, 200, 3)       9872321     123          32      adam   \n",
       "14  1684498043  (200, 200, 3)      10344897     157          32      adam   \n",
       "\n",
       "         Loss Function  Train MAE  Validation MAE  \n",
       "0   mean_squared_error  12.435976        8.629502  \n",
       "1   mean_squared_error  10.202019        6.590096  \n",
       "2   mean_squared_error   9.910834        6.491637  \n",
       "3   mean_squared_error  10.675910        7.109921  \n",
       "4   mean_squared_error  10.852635        7.511125  \n",
       "5   mean_squared_error  10.725429        7.512293  \n",
       "6   mean_squared_error   9.901710        7.452464  \n",
       "7   mean_squared_error  10.285194        7.056834  \n",
       "8   mean_squared_error   1.237246        5.716532  \n",
       "9   mean_squared_error  10.255740        6.360036  \n",
       "10  mean_squared_error   8.891773        6.597941  \n",
       "11  mean_squared_error   7.961771        5.572220  \n",
       "12  mean_squared_error   2.391651        5.277592  \n",
       "13  mean_squared_error   9.606898        6.405786  \n",
       "14  mean_squared_error   9.617646        6.659074  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_to_dataframe(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"File does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "df = load_data_to_dataframe(file_name)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Specify model to load\n",
    "# model_to_load = \"1683270923\"\n",
    "\n",
    "# def load_and_display_model_architecture(file_name):\n",
    "#     if not os.path.exists(file_name):\n",
    "#         print(\"File does not exist.\")\n",
    "#         return None\n",
    "\n",
    "#     with open(file_name, \"r\") as json_file:\n",
    "#         model_json = json_file.read()\n",
    "    \n",
    "#     model = model_from_json(model_json)\n",
    "#     model.summary()\n",
    "\n",
    "#     return model\n",
    "\n",
    "# model_architecture_file = f\"../04_Age_Prediction/models/{model_to_load}.json\"\n",
    "# model = load_and_display_model_architecture(model_architecture_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
