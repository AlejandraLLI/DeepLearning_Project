{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 05:00:09.458873: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-16 05:00:09.511777: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 05:00:12.044942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, \n",
    "                          Dense, Dropout, BatchNormalization, \n",
    "                          LeakyReLU, DepthwiseConv2D, MaxPooling2D,\n",
    "                          Add, Input, Activation, GlobalAveragePooling2D,\n",
    "                          Multiply, Reshape)\n",
    "\n",
    "\n",
    "from keras.utils import get_file\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "\n",
    "# Set Tensorflow log level to error \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Set logger level to ERROR\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)  # Set logger level to ERROR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sets\n",
    "raw_train_path = \"../02_Data/face_age_data/face_age_balanced_train\"\n",
    "aug_train_path = \"../02_Data/face_age_data/augmented_data_train\" \n",
    "\n",
    "\n",
    "# Validation set\n",
    "raw_val_path = \"../02_Data/face_age_data/face_age_balanced_val\"\n",
    "\n",
    "# Testing set\n",
    "raw_test_path = \"../02_Data/face_age_data/face_age_balanced_test\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folders, img_size=(200, 200)):\n",
    "    X = []\n",
    "    y = []\n",
    "    for folder_path in folders:\n",
    "        for folder in os.listdir(folder_path):\n",
    "            if os.path.isdir(os.path.join(folder_path, folder)):\n",
    "                age = int(folder.replace(\"aug_\", \"\"))\n",
    "                for file in os.listdir(os.path.join(folder_path, folder)):\n",
    "                    img_path = os.path.join(folder_path, folder, file)\n",
    "                    img = Image.open(img_path)\n",
    "                    img = img.resize(img_size)\n",
    "                    img = np.array(img)\n",
    "                    X.append(img)\n",
    "                    y.append(age)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (200, 200)\n",
    "\n",
    "# Load training set\n",
    "folder_paths_train = [\n",
    "    raw_train_path, \n",
    "    # aug_train_path\n",
    "    ]\n",
    "X_train, y_train = load_data(folder_paths_train, img_size)\n",
    "X_train = X_train / 255.0  # Normalize pixel values\n",
    "\n",
    "# Load validation set\n",
    "folder_paths_val = [raw_val_path]\n",
    "X_val, y_val = load_data(folder_paths_val, img_size)\n",
    "X_val = X_val / 255.0  # Normalize pixel values\n",
    "\n",
    "# Load testing set\n",
    "folder_paths_test = [raw_test_path]\n",
    "X_test, y_test = load_data(folder_paths_test, img_size)\n",
    "X_test = X_test / 255.0  # Normalize pixel values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (5598, 200, 200, 3) \n",
      " - 5598: Number of images in the dataset \n",
      " - 200: Height of each image \n",
      " - 200: Width of each image \n",
      " - 3: Number of channels of each image (Red, Green, and Blue)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Shape of X: {X_train.shape}\", \"\\n\",\n",
    "    f\"- {X_train.shape[0]}: Number of images in the dataset\", \"\\n\",\n",
    "    f\"- {X_train.shape[1]}: Height of each image\", \"\\n\",\n",
    "    f\"- {X_train.shape[2]}: Width of each image\", \"\\n\",\n",
    "    f\"- {X_train.shape[3]}: Number of channels of each image (Red, Green, and Blue)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [32 32 32 ... 58 58 58]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels: {y_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of arrays: \n",
      " - X_train shape: (5598, 200, 200, 3) \n",
      " - y_train shape: (5598,) \n",
      " - X_val shape: (1400, 200, 200, 3) \n",
      " - y_val shape: (1400,) \n",
      " - X_test shape: (1750, 200, 200, 3) \n",
      " - y_test shape: (1750,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Size of arrays:\", \"\\n\",\n",
    "    f\"- X_train shape: {X_train.shape}\", \"\\n\",\n",
    "    f\"- y_train shape: {y_train.shape}\", \"\\n\",\n",
    "    f\"- X_val shape: {X_val.shape}\", \"\\n\",\n",
    "    f\"- y_val shape: {y_val.shape}\", \"\\n\",\n",
    "    f\"- X_test shape: {X_test.shape}\", \"\\n\",\n",
    "    f\"- y_test shape: {y_test.shape}\", \"\\n\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  8\n",
      "['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3', '/device:GPU:4', '/device:GPU:5', '/device:GPU:6', '/device:GPU:7']\n"
     ]
    }
   ],
   "source": [
    "# Check if GPUs are available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Get list of GPUS\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Extract names\n",
    "gpu_names = ['/device:GPU:' + gpu.name.split(':')[-1] for gpu in gpus]\n",
    "\n",
    "print(gpu_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "optimizer = \"adam\"\n",
    "loss = \"mean_squared_error\"\n",
    "metrics = ['mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 05:00:50.129851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "2023-05-16 05:00:50.131006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9599 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5\n",
      "2023-05-16 05:00:50.132072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9599 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5\n",
      "2023-05-16 05:00:50.133107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9599 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5\n",
      "2023-05-16 05:00:50.134068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 9599 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:88:00.0, compute capability: 7.5\n",
      "2023-05-16 05:00:50.135058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 9599 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:89:00.0, compute capability: 7.5\n",
      "2023-05-16 05:00:50.135975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 9599 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b1:00.0, compute capability: 7.5\n",
      "2023-05-16 05:00:50.136916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 9599 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b2:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "# Create a MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_names)\n",
    "\n",
    "def Conv_BN(x, filters, kernel_size, strides=1):\n",
    "    x = Conv2D(filters, kernel_size, strides=strides, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def SE_Block(input, ratio=4):\n",
    "    num_filters = input.shape[-1]\n",
    "    x = GlobalAveragePooling2D()(input)\n",
    "    x = Dense(num_filters//ratio, activation='relu')(x)\n",
    "    x = Dense(num_filters, activation='sigmoid')(x)\n",
    "    x = Reshape((1, 1, num_filters))(x)\n",
    "    out = Multiply()([input, x])\n",
    "    return out\n",
    "\n",
    "def MBConv(x, in_channels, out_channels, expansion_factor, stride, k, se_ratio=0.25, id_skip=True):\n",
    "    x = Conv_BN(x, in_channels*expansion_factor, (1, 1))\n",
    "    x = Activation('relu')(x)\n",
    "    x = DepthwiseConv2D(k, strides=(stride, stride), depth_multiplier=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if se_ratio:\n",
    "        x = SE_Block(x, ratio=se_ratio)\n",
    "\n",
    "    x = Conv2D(out_channels, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    if id_skip:\n",
    "        if in_channels == out_channels and stride == 1:\n",
    "            x = Add()([x, input])\n",
    "    return x\n",
    "\n",
    "def create_model(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Conv_BN(input, 32, (3, 3), strides=2)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MBConv(x, in_channels=32, out_channels=16, expansion_factor=1, stride=1, k=3)\n",
    "    x = MBConv(x, in_channels=16, out_channels=24, expansion_factor=6, stride=2, k=3)\n",
    "    x = MBConv(x, in_channels=24, out_channels=40, expansion_factor=6, stride=2, k=5)\n",
    "    x = MBConv(x, in_channels=40, out_channels=80, expansion_factor=6, stride=2, k=3)\n",
    "    x = MBConv(x, in_channels=80, out_channels=112, expansion_factor=6, stride=1, k=5)\n",
    "    x = MBConv(x, in_channels=112, out_channels=192, expansion_factor=6, stride=2, k=5)\n",
    "    x = MBConv(x, in_channels=192, out_channels=320, expansion_factor=6, stride=1, k=3)\n",
    "\n",
    "    x = Conv_BN(x, 1280, (1, 1))\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    output = Dense(1)(x)  # For regression, single output with no activation function\n",
    "\n",
    "    model = Model(input, output)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)  # Suitable for regression\n",
    "\n",
    "    return model\n",
    "\n",
    "# Open the strategy scope\n",
    "with strategy.scope():\n",
    "    input_shape = (img_size[0], img_size[1], 3)\n",
    "    model = create_model(input_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback to save the model's weights\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"../04_Age_Prediction/08_models/best_model.h5\", save_best_only=True)\n",
    "\n",
    "# Create a callback that stops the training if there is no improvement in the validation loss for 15 consecutive epochs\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 05:01:01.946248: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [5598]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-05-16 05:01:01.946614: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype float and shape [5598,200,200,3]\n",
      "\t [[{{node Placeholder/_10}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "INFO:tensorflow:batch_all_reduce: 113 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 113 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 05:02:16.811014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-16 05:02:18.147294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-16 05:02:19.515901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-16 05:02:20.937819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-16 05:02:22.473504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-16 05:02:23.784192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-16 05:02:25.253464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-16 05:02:25.798871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-16 05:02:33.338739: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f2b875cc140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-16 05:02:33.338777: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-16 05:02:33.338785: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-16 05:02:33.338791: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-16 05:02:33.338797: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-16 05:02:33.338803: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (4): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-16 05:02:33.338808: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (5): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-16 05:02:33.338814: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (6): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-16 05:02:33.338819: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (7): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-16 05:02:33.347927: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-16 05:02:33.517065: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - ETA: 0s - loss: 507.5802 - mae: 17.0529"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 05:03:17.240251: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [1750]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-05-16 05:03:17.240588: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [1750]\n",
      "\t [[{{node Placeholder/_11}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 147s 248ms/step - loss: 507.5802 - mae: 17.0529 - val_loss: 1037.8269 - val_mae: 24.4036\n",
      "Epoch 2/1000\n",
      "175/175 [==============================] - 33s 188ms/step - loss: 282.6443 - mae: 13.1285 - val_loss: 719.1876 - val_mae: 20.4352\n",
      "Epoch 3/1000\n",
      "175/175 [==============================] - 33s 188ms/step - loss: 216.9361 - mae: 11.3868 - val_loss: 298.8610 - val_mae: 13.1062\n",
      "Epoch 4/1000\n",
      "175/175 [==============================] - 33s 189ms/step - loss: 181.2399 - mae: 10.2181 - val_loss: 185.8701 - val_mae: 10.0608\n",
      "Epoch 5/1000\n",
      "175/175 [==============================] - 33s 189ms/step - loss: 144.2434 - mae: 9.0462 - val_loss: 142.2020 - val_mae: 9.0436\n",
      "Epoch 6/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 122.4418 - mae: 8.3395 - val_loss: 160.7651 - val_mae: 9.7834\n",
      "Epoch 7/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 105.6560 - mae: 7.6927 - val_loss: 157.2346 - val_mae: 9.2729\n",
      "Epoch 8/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 94.3567 - mae: 7.2510 - val_loss: 196.5576 - val_mae: 9.9820\n",
      "Epoch 9/1000\n",
      "175/175 [==============================] - 33s 188ms/step - loss: 77.5777 - mae: 6.6156 - val_loss: 130.9056 - val_mae: 8.3545\n",
      "Epoch 10/1000\n",
      "175/175 [==============================] - 33s 187ms/step - loss: 68.2370 - mae: 6.2254 - val_loss: 113.5971 - val_mae: 7.9257\n",
      "Epoch 11/1000\n",
      "175/175 [==============================] - 30s 173ms/step - loss: 60.9013 - mae: 5.8933 - val_loss: 125.8594 - val_mae: 7.8960\n",
      "Epoch 12/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 59.8830 - mae: 5.8202 - val_loss: 119.0789 - val_mae: 7.8755\n",
      "Epoch 13/1000\n",
      "175/175 [==============================] - 33s 189ms/step - loss: 46.1923 - mae: 5.1183 - val_loss: 113.3928 - val_mae: 7.7562\n",
      "Epoch 14/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 44.6885 - mae: 5.0412 - val_loss: 120.5582 - val_mae: 8.0185\n",
      "Epoch 15/1000\n",
      "175/175 [==============================] - 33s 188ms/step - loss: 38.9296 - mae: 4.7511 - val_loss: 99.3562 - val_mae: 7.1622\n",
      "Epoch 16/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 37.6859 - mae: 4.6167 - val_loss: 103.6018 - val_mae: 7.2369\n",
      "Epoch 17/1000\n",
      "175/175 [==============================] - 33s 186ms/step - loss: 33.0052 - mae: 4.3392 - val_loss: 95.7818 - val_mae: 6.9240\n",
      "Epoch 18/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 30.4394 - mae: 4.2015 - val_loss: 100.4927 - val_mae: 7.1944\n",
      "Epoch 19/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 29.1968 - mae: 4.0683 - val_loss: 100.8050 - val_mae: 7.1906\n",
      "Epoch 20/1000\n",
      "175/175 [==============================] - 33s 188ms/step - loss: 26.2706 - mae: 3.9442 - val_loss: 95.1603 - val_mae: 6.9487\n",
      "Epoch 21/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 26.4704 - mae: 3.9003 - val_loss: 99.7617 - val_mae: 7.2827\n",
      "Epoch 22/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 24.6263 - mae: 3.7905 - val_loss: 100.6046 - val_mae: 7.1929\n",
      "Epoch 23/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 21.2910 - mae: 3.5134 - val_loss: 100.5940 - val_mae: 7.1603\n",
      "Epoch 24/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 20.7887 - mae: 3.4915 - val_loss: 131.9006 - val_mae: 7.9801\n",
      "Epoch 25/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 26.0293 - mae: 3.8497 - val_loss: 103.5948 - val_mae: 7.1754\n",
      "Epoch 26/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 25.2793 - mae: 3.8068 - val_loss: 105.0935 - val_mae: 7.3256\n",
      "Epoch 27/1000\n",
      "175/175 [==============================] - 33s 187ms/step - loss: 20.8317 - mae: 3.4585 - val_loss: 92.8612 - val_mae: 6.7658\n",
      "Epoch 28/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 17.8962 - mae: 3.2378 - val_loss: 108.0547 - val_mae: 7.2786\n",
      "Epoch 29/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 19.4386 - mae: 3.3556 - val_loss: 112.8293 - val_mae: 7.2140\n",
      "Epoch 30/1000\n",
      "175/175 [==============================] - 33s 186ms/step - loss: 20.4025 - mae: 3.4232 - val_loss: 86.7431 - val_mae: 6.5337\n",
      "Epoch 31/1000\n",
      "175/175 [==============================] - 33s 187ms/step - loss: 16.9358 - mae: 3.1268 - val_loss: 85.5503 - val_mae: 6.5282\n",
      "Epoch 32/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 16.3623 - mae: 3.1163 - val_loss: 100.2726 - val_mae: 6.9484\n",
      "Epoch 33/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 16.9431 - mae: 3.1121 - val_loss: 94.7532 - val_mae: 6.8019\n",
      "Epoch 34/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 17.0277 - mae: 3.1535 - val_loss: 92.2829 - val_mae: 6.9466\n",
      "Epoch 35/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 15.7194 - mae: 3.0272 - val_loss: 88.5504 - val_mae: 6.6001\n",
      "Epoch 36/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 15.3552 - mae: 2.9797 - val_loss: 91.7635 - val_mae: 6.8362\n",
      "Epoch 37/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 16.4553 - mae: 3.1025 - val_loss: 102.2820 - val_mae: 7.1365\n",
      "Epoch 38/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 15.2528 - mae: 2.9869 - val_loss: 93.2523 - val_mae: 6.7567\n",
      "Epoch 39/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 14.5559 - mae: 2.8833 - val_loss: 104.7333 - val_mae: 7.1079\n",
      "Epoch 40/1000\n",
      "175/175 [==============================] - 33s 188ms/step - loss: 14.7768 - mae: 2.9142 - val_loss: 85.2276 - val_mae: 6.4956\n",
      "Epoch 41/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 14.5920 - mae: 2.8985 - val_loss: 86.7983 - val_mae: 6.5337\n",
      "Epoch 42/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 16.4446 - mae: 3.0732 - val_loss: 85.7549 - val_mae: 6.6602\n",
      "Epoch 43/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 16.5079 - mae: 3.0869 - val_loss: 89.0227 - val_mae: 6.7290\n",
      "Epoch 44/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 14.9389 - mae: 2.9500 - val_loss: 86.0845 - val_mae: 6.4677\n",
      "Epoch 45/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 13.4675 - mae: 2.7957 - val_loss: 88.0121 - val_mae: 6.5663\n",
      "Epoch 46/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 13.1022 - mae: 2.7691 - val_loss: 91.0002 - val_mae: 6.6596\n",
      "Epoch 47/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 12.0835 - mae: 2.6442 - val_loss: 93.7770 - val_mae: 6.8133\n",
      "Epoch 48/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 13.1190 - mae: 2.7772 - val_loss: 88.0271 - val_mae: 6.8587\n",
      "Epoch 49/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 12.3829 - mae: 2.6752 - val_loss: 98.0889 - val_mae: 7.1543\n",
      "Epoch 50/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 12.3541 - mae: 2.6740 - val_loss: 85.3138 - val_mae: 6.4029\n",
      "Epoch 51/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 12.7366 - mae: 2.7349 - val_loss: 85.5360 - val_mae: 6.6660\n",
      "Epoch 52/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 11.5230 - mae: 2.5855 - val_loss: 91.9972 - val_mae: 6.6542\n",
      "Epoch 53/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 10.3773 - mae: 2.4743 - val_loss: 86.4686 - val_mae: 6.6357\n",
      "Epoch 54/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 11.6548 - mae: 2.5920 - val_loss: 88.4060 - val_mae: 6.5393\n",
      "Epoch 55/1000\n",
      "175/175 [==============================] - 31s 179ms/step - loss: 10.9318 - mae: 2.5108 - val_loss: 85.8984 - val_mae: 6.4166\n",
      "Epoch 56/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 11.1796 - mae: 2.5534 - val_loss: 86.5818 - val_mae: 6.5273\n",
      "Epoch 57/1000\n",
      "175/175 [==============================] - 33s 188ms/step - loss: 11.8388 - mae: 2.6025 - val_loss: 82.7622 - val_mae: 6.3467\n",
      "Epoch 58/1000\n",
      "175/175 [==============================] - 30s 173ms/step - loss: 11.0477 - mae: 2.5211 - val_loss: 94.3047 - val_mae: 6.9393\n",
      "Epoch 59/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 11.7590 - mae: 2.5939 - val_loss: 88.1557 - val_mae: 6.8022\n",
      "Epoch 60/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 10.1700 - mae: 2.4106 - val_loss: 88.1425 - val_mae: 6.5590\n",
      "Epoch 61/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 9.7158 - mae: 2.3744 - val_loss: 84.6673 - val_mae: 6.4827\n",
      "Epoch 62/1000\n",
      "175/175 [==============================] - 33s 186ms/step - loss: 8.6200 - mae: 2.2440 - val_loss: 81.1261 - val_mae: 6.1959\n",
      "Epoch 63/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 9.3560 - mae: 2.3147 - val_loss: 82.8008 - val_mae: 6.3046\n",
      "Epoch 64/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 9.9522 - mae: 2.3897 - val_loss: 85.7160 - val_mae: 6.5303\n",
      "Epoch 65/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 9.2750 - mae: 2.3338 - val_loss: 89.9685 - val_mae: 6.6516\n",
      "Epoch 66/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 10.7612 - mae: 2.4817 - val_loss: 84.1178 - val_mae: 6.4438\n",
      "Epoch 67/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 10.3633 - mae: 2.4510 - val_loss: 98.7052 - val_mae: 7.3329\n",
      "Epoch 68/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 9.6597 - mae: 2.3581 - val_loss: 84.7964 - val_mae: 6.3325\n",
      "Epoch 69/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 10.5159 - mae: 2.4575 - val_loss: 87.9501 - val_mae: 6.6650\n",
      "Epoch 70/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 9.5133 - mae: 2.3157 - val_loss: 84.2282 - val_mae: 6.4186\n",
      "Epoch 71/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 8.5213 - mae: 2.2155 - val_loss: 81.1703 - val_mae: 6.2335\n",
      "Epoch 72/1000\n",
      "175/175 [==============================] - 33s 187ms/step - loss: 8.0557 - mae: 2.1704 - val_loss: 78.4424 - val_mae: 6.1217\n",
      "Epoch 73/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 7.4979 - mae: 2.0925 - val_loss: 85.8701 - val_mae: 6.5720\n",
      "Epoch 74/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 8.4328 - mae: 2.1836 - val_loss: 79.7785 - val_mae: 6.2231\n",
      "Epoch 75/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 8.8010 - mae: 2.2591 - val_loss: 82.8648 - val_mae: 6.2754\n",
      "Epoch 76/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 8.8512 - mae: 2.2640 - val_loss: 89.7207 - val_mae: 6.7713\n",
      "Epoch 77/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 8.3426 - mae: 2.1935 - val_loss: 80.5234 - val_mae: 6.2445\n",
      "Epoch 78/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 8.7157 - mae: 2.2403 - val_loss: 90.1758 - val_mae: 6.8081\n",
      "Epoch 79/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 8.6422 - mae: 2.2236 - val_loss: 82.0540 - val_mae: 6.3737\n",
      "Epoch 80/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 8.3046 - mae: 2.1844 - val_loss: 81.4039 - val_mae: 6.2076\n",
      "Epoch 81/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 7.2662 - mae: 2.0426 - val_loss: 91.5014 - val_mae: 6.4710\n",
      "Epoch 82/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 7.1290 - mae: 2.0292 - val_loss: 88.3036 - val_mae: 6.5159\n",
      "Epoch 83/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 7.3511 - mae: 2.0538 - val_loss: 84.7985 - val_mae: 6.5415\n",
      "Epoch 84/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.6958 - mae: 1.9859 - val_loss: 81.0406 - val_mae: 6.2434\n",
      "Epoch 85/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.7159 - mae: 1.9706 - val_loss: 79.9302 - val_mae: 6.1093\n",
      "Epoch 86/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 7.1654 - mae: 2.0416 - val_loss: 80.4018 - val_mae: 6.1020\n",
      "Epoch 87/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 8.0301 - mae: 2.1636 - val_loss: 86.4929 - val_mae: 6.6485\n",
      "Epoch 88/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 7.5764 - mae: 2.0890 - val_loss: 81.3825 - val_mae: 6.1920\n",
      "Epoch 89/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 7.1878 - mae: 2.0678 - val_loss: 87.9760 - val_mae: 6.5619\n",
      "Epoch 90/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 6.8065 - mae: 1.9898 - val_loss: 82.4183 - val_mae: 6.2559\n",
      "Epoch 91/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.3107 - mae: 1.9038 - val_loss: 95.0603 - val_mae: 7.0161\n",
      "Epoch 92/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.5850 - mae: 1.9663 - val_loss: 81.8999 - val_mae: 6.1315\n",
      "Epoch 93/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 6.2140 - mae: 1.8971 - val_loss: 83.2843 - val_mae: 6.1624\n",
      "Epoch 94/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.1834 - mae: 1.8977 - val_loss: 101.3706 - val_mae: 7.0182\n",
      "Epoch 95/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 6.6850 - mae: 1.9542 - val_loss: 79.6687 - val_mae: 6.0455\n",
      "Epoch 96/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.3895 - mae: 1.9122 - val_loss: 82.1377 - val_mae: 6.1597\n",
      "Epoch 97/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.5920 - mae: 1.9675 - val_loss: 81.7519 - val_mae: 6.1442\n",
      "Epoch 98/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.9570 - mae: 1.9954 - val_loss: 79.9923 - val_mae: 6.1381\n",
      "Epoch 99/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 8.5015 - mae: 2.2047 - val_loss: 84.7124 - val_mae: 6.3932\n",
      "Epoch 100/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 7.5905 - mae: 2.0863 - val_loss: 79.5029 - val_mae: 6.0598\n",
      "Epoch 101/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 7.1682 - mae: 2.0292 - val_loss: 82.5841 - val_mae: 6.2973\n",
      "Epoch 102/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 7.0387 - mae: 2.0216 - val_loss: 80.1095 - val_mae: 6.0940\n",
      "Epoch 103/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 5.4484 - mae: 1.7812 - val_loss: 82.0768 - val_mae: 6.3389\n",
      "Epoch 104/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 5.3545 - mae: 1.7707 - val_loss: 78.7414 - val_mae: 6.0747\n",
      "Epoch 105/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 5.7783 - mae: 1.8232 - val_loss: 84.2254 - val_mae: 6.3449\n",
      "Epoch 106/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 5.8258 - mae: 1.8372 - val_loss: 82.6313 - val_mae: 6.2266\n",
      "Epoch 107/1000\n",
      "175/175 [==============================] - 33s 188ms/step - loss: 5.5804 - mae: 1.7929 - val_loss: 75.7572 - val_mae: 5.9575\n",
      "Epoch 108/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 4.9762 - mae: 1.6990 - val_loss: 78.9355 - val_mae: 6.1397\n",
      "Epoch 109/1000\n",
      "175/175 [==============================] - 29s 169ms/step - loss: 5.1682 - mae: 1.7158 - val_loss: 79.4622 - val_mae: 6.0934\n",
      "Epoch 110/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 5.1662 - mae: 1.7266 - val_loss: 82.8823 - val_mae: 6.3644\n",
      "Epoch 111/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.8332 - mae: 1.9502 - val_loss: 76.9999 - val_mae: 6.2323\n",
      "Epoch 112/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 6.1565 - mae: 1.8588 - val_loss: 85.4403 - val_mae: 6.2478\n",
      "Epoch 113/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 5.9977 - mae: 1.8616 - val_loss: 81.7975 - val_mae: 6.2991\n",
      "Epoch 114/1000\n",
      "175/175 [==============================] - 31s 177ms/step - loss: 5.5675 - mae: 1.7876 - val_loss: 83.0129 - val_mae: 6.1778\n",
      "Epoch 115/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 5.1200 - mae: 1.7328 - val_loss: 81.3357 - val_mae: 6.1777\n",
      "Epoch 116/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 5.4541 - mae: 1.7671 - val_loss: 82.5797 - val_mae: 6.2182\n",
      "Epoch 117/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 5.5047 - mae: 1.7788 - val_loss: 79.5276 - val_mae: 6.0741\n",
      "Epoch 118/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 5.8854 - mae: 1.8155 - val_loss: 81.9559 - val_mae: 6.2976\n",
      "Epoch 119/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 5.7832 - mae: 1.8115 - val_loss: 82.0234 - val_mae: 6.0727\n",
      "Epoch 120/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 5.3000 - mae: 1.7290 - val_loss: 78.8934 - val_mae: 6.0735\n",
      "Epoch 121/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 4.9679 - mae: 1.6993 - val_loss: 79.0672 - val_mae: 6.1816\n",
      "Epoch 122/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.4423 - mae: 1.5881 - val_loss: 78.1712 - val_mae: 6.0074\n",
      "Epoch 123/1000\n",
      "175/175 [==============================] - 33s 187ms/step - loss: 4.6182 - mae: 1.6239 - val_loss: 75.2828 - val_mae: 5.9905\n",
      "Epoch 124/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 4.5381 - mae: 1.6160 - val_loss: 79.5847 - val_mae: 6.1307\n",
      "Epoch 125/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.8850 - mae: 1.6781 - val_loss: 79.4421 - val_mae: 6.0451\n",
      "Epoch 126/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 5.1780 - mae: 1.7396 - val_loss: 81.0183 - val_mae: 6.0929\n",
      "Epoch 127/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.9930 - mae: 1.6797 - val_loss: 77.6792 - val_mae: 6.1065\n",
      "Epoch 128/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.7604 - mae: 1.6676 - val_loss: 78.8571 - val_mae: 6.2229\n",
      "Epoch 129/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.6207 - mae: 1.6205 - val_loss: 82.7415 - val_mae: 6.2940\n",
      "Epoch 130/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.9111 - mae: 1.6885 - val_loss: 80.3452 - val_mae: 6.3628\n",
      "Epoch 131/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 4.5424 - mae: 1.5913 - val_loss: 79.8265 - val_mae: 6.1024\n",
      "Epoch 132/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.4217 - mae: 1.5926 - val_loss: 79.2611 - val_mae: 6.1067\n",
      "Epoch 133/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 5.1250 - mae: 1.7068 - val_loss: 83.7428 - val_mae: 6.4184\n",
      "Epoch 134/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 5.4073 - mae: 1.7521 - val_loss: 78.5951 - val_mae: 6.0199\n",
      "Epoch 135/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.9851 - mae: 1.6774 - val_loss: 79.7589 - val_mae: 6.1471\n",
      "Epoch 136/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 5.4792 - mae: 1.7686 - val_loss: 76.5867 - val_mae: 6.0385\n",
      "Epoch 137/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 5.2512 - mae: 1.7482 - val_loss: 75.9642 - val_mae: 5.9440\n",
      "Epoch 138/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 4.9175 - mae: 1.6712 - val_loss: 77.9233 - val_mae: 6.1360\n",
      "Epoch 139/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 4.3335 - mae: 1.5759 - val_loss: 79.0928 - val_mae: 6.1428\n",
      "Epoch 140/1000\n",
      "175/175 [==============================] - 33s 187ms/step - loss: 4.6421 - mae: 1.6366 - val_loss: 73.0953 - val_mae: 5.8402\n",
      "Epoch 141/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.0909 - mae: 1.5368 - val_loss: 78.0832 - val_mae: 5.9928\n",
      "Epoch 142/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.0135 - mae: 1.5142 - val_loss: 75.2466 - val_mae: 5.9530\n",
      "Epoch 143/1000\n",
      "175/175 [==============================] - 29s 169ms/step - loss: 4.6276 - mae: 1.6170 - val_loss: 77.9286 - val_mae: 6.0523\n",
      "Epoch 144/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.4123 - mae: 1.5904 - val_loss: 76.0614 - val_mae: 5.9465\n",
      "Epoch 145/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.8495 - mae: 1.4908 - val_loss: 80.2083 - val_mae: 6.2723\n",
      "Epoch 146/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.9997 - mae: 1.5073 - val_loss: 78.5869 - val_mae: 5.9834\n",
      "Epoch 147/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 4.1302 - mae: 1.5336 - val_loss: 80.9104 - val_mae: 6.2872\n",
      "Epoch 148/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.2089 - mae: 1.5525 - val_loss: 78.2345 - val_mae: 6.0139\n",
      "Epoch 149/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.2702 - mae: 1.5692 - val_loss: 78.9601 - val_mae: 6.0529\n",
      "Epoch 150/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 4.5161 - mae: 1.6087 - val_loss: 78.2157 - val_mae: 6.0236\n",
      "Epoch 151/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 4.4491 - mae: 1.5887 - val_loss: 76.7975 - val_mae: 5.9826\n",
      "Epoch 152/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.9838 - mae: 1.5235 - val_loss: 80.6275 - val_mae: 6.0133\n",
      "Epoch 153/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 4.4551 - mae: 1.5907 - val_loss: 78.1433 - val_mae: 6.1676\n",
      "Epoch 154/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 4.3208 - mae: 1.5646 - val_loss: 76.6792 - val_mae: 6.0462\n",
      "Epoch 155/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 4.3901 - mae: 1.5820 - val_loss: 79.3701 - val_mae: 6.0829\n",
      "Epoch 156/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 3.9502 - mae: 1.5067 - val_loss: 81.4997 - val_mae: 6.1086\n",
      "Epoch 157/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.6210 - mae: 1.4279 - val_loss: 76.6244 - val_mae: 5.9169\n",
      "Epoch 158/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.5356 - mae: 1.4201 - val_loss: 81.3695 - val_mae: 6.1814\n",
      "Epoch 159/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.5144 - mae: 1.4243 - val_loss: 77.5344 - val_mae: 6.0275\n",
      "Epoch 160/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 3.6987 - mae: 1.4469 - val_loss: 77.7183 - val_mae: 6.1043\n",
      "Epoch 161/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.8662 - mae: 1.4741 - val_loss: 77.7849 - val_mae: 6.1184\n",
      "Epoch 162/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 3.8826 - mae: 1.4953 - val_loss: 78.7023 - val_mae: 6.0059\n",
      "Epoch 163/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.8331 - mae: 1.4648 - val_loss: 79.3029 - val_mae: 6.0515\n",
      "Epoch 164/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.7826 - mae: 1.4775 - val_loss: 74.4943 - val_mae: 5.8980\n",
      "Epoch 165/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.7888 - mae: 1.4737 - val_loss: 78.2276 - val_mae: 6.0246\n",
      "Epoch 166/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 4.2526 - mae: 1.5348 - val_loss: 83.1399 - val_mae: 6.1651\n",
      "Epoch 167/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 5.1477 - mae: 1.7098 - val_loss: 80.7969 - val_mae: 6.2689\n",
      "Epoch 168/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 5.1627 - mae: 1.6938 - val_loss: 76.7489 - val_mae: 5.9883\n",
      "Epoch 169/1000\n",
      "175/175 [==============================] - 33s 188ms/step - loss: 4.0480 - mae: 1.5227 - val_loss: 72.6676 - val_mae: 5.7165\n",
      "Epoch 170/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.7185 - mae: 1.4653 - val_loss: 78.2953 - val_mae: 5.9814\n",
      "Epoch 171/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.1479 - mae: 1.3524 - val_loss: 81.1119 - val_mae: 6.0770\n",
      "Epoch 172/1000\n",
      "175/175 [==============================] - 31s 178ms/step - loss: 3.2325 - mae: 1.3588 - val_loss: 79.2933 - val_mae: 6.0877\n",
      "Epoch 173/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 2.9877 - mae: 1.3026 - val_loss: 77.5710 - val_mae: 5.9410\n",
      "Epoch 174/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 3.1489 - mae: 1.3462 - val_loss: 77.3905 - val_mae: 5.9339\n",
      "Epoch 175/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.0676 - mae: 1.3239 - val_loss: 76.9229 - val_mae: 5.9965\n",
      "Epoch 176/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.5040 - mae: 1.4205 - val_loss: 80.1498 - val_mae: 6.0205\n",
      "Epoch 177/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.6486 - mae: 1.4527 - val_loss: 76.5245 - val_mae: 5.9996\n",
      "Epoch 178/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.7951 - mae: 1.4738 - val_loss: 78.1038 - val_mae: 5.9511\n",
      "Epoch 179/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.4849 - mae: 1.4166 - val_loss: 81.2887 - val_mae: 6.2565\n",
      "Epoch 180/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.3849 - mae: 1.3900 - val_loss: 78.2142 - val_mae: 5.9641\n",
      "Epoch 181/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.3700 - mae: 1.3818 - val_loss: 78.2607 - val_mae: 5.9512\n",
      "Epoch 182/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.3208 - mae: 1.3622 - val_loss: 79.1029 - val_mae: 6.0442\n",
      "Epoch 183/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.3558 - mae: 1.3924 - val_loss: 78.1528 - val_mae: 5.9805\n",
      "Epoch 184/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 3.6078 - mae: 1.4449 - val_loss: 77.8349 - val_mae: 5.9319\n",
      "Epoch 185/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 3.3471 - mae: 1.3785 - val_loss: 81.4056 - val_mae: 6.1215\n",
      "Epoch 186/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 3.4903 - mae: 1.4183 - val_loss: 78.1124 - val_mae: 5.9921\n",
      "Epoch 187/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.4775 - mae: 1.4111 - val_loss: 85.5340 - val_mae: 6.2867\n",
      "Epoch 188/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.8998 - mae: 1.4824 - val_loss: 81.3702 - val_mae: 6.0595\n",
      "Epoch 189/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.5964 - mae: 1.4252 - val_loss: 82.8163 - val_mae: 6.1766\n",
      "Epoch 190/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.6715 - mae: 1.4401 - val_loss: 77.1576 - val_mae: 6.0691\n",
      "Epoch 191/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.4740 - mae: 1.4149 - val_loss: 83.0902 - val_mae: 6.1614\n",
      "Epoch 192/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.9731 - mae: 1.3080 - val_loss: 75.7460 - val_mae: 5.9095\n",
      "Epoch 193/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.0782 - mae: 1.3079 - val_loss: 78.7225 - val_mae: 6.0777\n",
      "Epoch 194/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.0488 - mae: 1.3147 - val_loss: 75.6345 - val_mae: 5.9129\n",
      "Epoch 195/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.8626 - mae: 1.2812 - val_loss: 80.5192 - val_mae: 6.0951\n",
      "Epoch 196/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 2.8323 - mae: 1.2660 - val_loss: 76.8281 - val_mae: 6.0167\n",
      "Epoch 197/1000\n",
      "175/175 [==============================] - 30s 169ms/step - loss: 2.8871 - mae: 1.2775 - val_loss: 78.9704 - val_mae: 5.9499\n",
      "Epoch 198/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.9503 - mae: 1.3023 - val_loss: 78.8226 - val_mae: 6.1210\n",
      "Epoch 199/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 2.7560 - mae: 1.2544 - val_loss: 78.3169 - val_mae: 6.0312\n",
      "Epoch 200/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.7944 - mae: 1.2586 - val_loss: 77.5991 - val_mae: 6.0510\n",
      "Epoch 201/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.7339 - mae: 1.2563 - val_loss: 78.0293 - val_mae: 6.0015\n",
      "Epoch 202/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.8809 - mae: 1.2816 - val_loss: 80.4977 - val_mae: 6.1641\n",
      "Epoch 203/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.9323 - mae: 1.2904 - val_loss: 77.6189 - val_mae: 5.9039\n",
      "Epoch 204/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.0510 - mae: 1.3134 - val_loss: 79.9143 - val_mae: 6.0125\n",
      "Epoch 205/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.4611 - mae: 1.4036 - val_loss: 78.7772 - val_mae: 6.0434\n",
      "Epoch 206/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.5262 - mae: 1.4191 - val_loss: 77.9064 - val_mae: 5.9564\n",
      "Epoch 207/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.2912 - mae: 1.3649 - val_loss: 79.0638 - val_mae: 5.9867\n",
      "Epoch 208/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.3095 - mae: 1.3748 - val_loss: 75.9962 - val_mae: 5.9170\n",
      "Epoch 209/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.4266 - mae: 1.3915 - val_loss: 78.3726 - val_mae: 6.0054\n",
      "Epoch 210/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 3.1062 - mae: 1.3261 - val_loss: 75.2237 - val_mae: 5.8965\n",
      "Epoch 211/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 2.6679 - mae: 1.2414 - val_loss: 78.5941 - val_mae: 6.0737\n",
      "Epoch 212/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.7706 - mae: 1.2561 - val_loss: 76.9022 - val_mae: 5.9426\n",
      "Epoch 213/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 2.8485 - mae: 1.2769 - val_loss: 77.4365 - val_mae: 6.0162\n",
      "Epoch 214/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 2.7368 - mae: 1.2460 - val_loss: 78.3147 - val_mae: 5.9128\n",
      "Epoch 215/1000\n",
      "175/175 [==============================] - 30s 172ms/step - loss: 2.7242 - mae: 1.2475 - val_loss: 76.6041 - val_mae: 5.9765\n",
      "Epoch 216/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.8632 - mae: 1.2840 - val_loss: 77.9277 - val_mae: 5.9778\n",
      "Epoch 217/1000\n",
      "175/175 [==============================] - 30s 170ms/step - loss: 2.6904 - mae: 1.2372 - val_loss: 74.4393 - val_mae: 5.8330\n",
      "Epoch 218/1000\n",
      "175/175 [==============================] - 30s 171ms/step - loss: 3.2126 - mae: 1.3575 - val_loss: 78.2167 - val_mae: 5.9434\n",
      "Epoch 219/1000\n",
      "175/175 [==============================] - 32s 183ms/step - loss: 2.9645 - mae: 1.2863 - val_loss: 78.3299 - val_mae: 5.9001\n"
     ]
    }
   ],
   "source": [
    "# Train the model with your data\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, Load, and Display Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model attributes\n",
    "timestamp = int(time.time())\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "description = f\"{timestamp}\"\n",
    "input_shape = str(X_train.shape[1:])\n",
    "num_params = model.count_params()\n",
    "epochs = len(history.history['loss'])\n",
    "optimizer = optimizer\n",
    "loss_function = loss\n",
    "train_mae = min(history.history['mae'])\n",
    "validation_mae = min(history.history['val_mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ../04_Age_Prediction/08_models/1684219988.h5\n"
     ]
    }
   ],
   "source": [
    "def append_model_history_to_file(file_name, description, input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae):\n",
    "    train_mae = round(train_mae, 6)\n",
    "    validation_mae = round(validation_mae, 6)\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        with open(file_name, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['Model', 'Input Shape', 'Model Params', 'Epochs', 'Batch Size', 'Optimizer', 'Loss Function', 'Train MAE', 'Validation MAE'])\n",
    "\n",
    "    with open(file_name, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        try:\n",
    "            next(csv_reader)  # Skip the header row\n",
    "        except StopIteration:\n",
    "            pass  # The CSV file is empty or only contains the header row\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if list(map(str, row[1:])) == list(map(str, [input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])):\n",
    "                print(\"Entry with the same parameters already exists.\")\n",
    "                return False\n",
    "\n",
    "    with open(file_name, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([description, input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])\n",
    "\n",
    "    return True\n",
    "\n",
    "new_entry_added = append_model_history_to_file(file_name, description, input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae)\n",
    "\n",
    "# Save model\n",
    "def save_model_architecture(model, file_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "if new_entry_added:\n",
    "    model_file = f\"../04_Age_Prediction/08_models/{description}.h5\"\n",
    "    model.save(model_file)\n",
    "    print(f\"Model saved at {model_file}\")\n",
    "else:\n",
    "    print(\"Model not saved as an entry with the same parameters already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input Shape</th>\n",
       "      <th>Model Params</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Loss Function</th>\n",
       "      <th>Train MAE</th>\n",
       "      <th>Validation MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1684111562</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>8770497</td>\n",
       "      <td>155</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>12.435976</td>\n",
       "      <td>8.629502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1684118998</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>33625217</td>\n",
       "      <td>132</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.202019</td>\n",
       "      <td>6.590096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1684126499</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>18144705</td>\n",
       "      <td>170</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.910834</td>\n",
       "      <td>6.491637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1684130813</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>9872321</td>\n",
       "      <td>213</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.675910</td>\n",
       "      <td>7.109921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1684133411</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>10344897</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.852635</td>\n",
       "      <td>7.511125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1684148562</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>77897345</td>\n",
       "      <td>212</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.725429</td>\n",
       "      <td>7.512293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1684208696</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>82101291</td>\n",
       "      <td>312</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.901710</td>\n",
       "      <td>7.452464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1684212422</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>11331745</td>\n",
       "      <td>108</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.285194</td>\n",
       "      <td>7.056834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1684219988</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>18217393</td>\n",
       "      <td>219</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>1.237246</td>\n",
       "      <td>5.716532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model    Input Shape  Model Params  Epochs  Batch Size Optimizer  \\\n",
       "0  1684111562  (200, 200, 3)       8770497     155          32      adam   \n",
       "1  1684118998  (200, 200, 3)      33625217     132          32      adam   \n",
       "2  1684126499  (200, 200, 3)      18144705     170          32      adam   \n",
       "3  1684130813  (200, 200, 3)       9872321     213          32      adam   \n",
       "4  1684133411  (200, 200, 3)      10344897     168          32      adam   \n",
       "5  1684148562  (200, 200, 3)      77897345     212          32      adam   \n",
       "6  1684208696  (200, 200, 3)      82101291     312          32      adam   \n",
       "7  1684212422  (200, 200, 3)      11331745     108          32      adam   \n",
       "8  1684219988  (200, 200, 3)      18217393     219          32      adam   \n",
       "\n",
       "        Loss Function  Train MAE  Validation MAE  \n",
       "0  mean_squared_error  12.435976        8.629502  \n",
       "1  mean_squared_error  10.202019        6.590096  \n",
       "2  mean_squared_error   9.910834        6.491637  \n",
       "3  mean_squared_error  10.675910        7.109921  \n",
       "4  mean_squared_error  10.852635        7.511125  \n",
       "5  mean_squared_error  10.725429        7.512293  \n",
       "6  mean_squared_error   9.901710        7.452464  \n",
       "7  mean_squared_error  10.285194        7.056834  \n",
       "8  mean_squared_error   1.237246        5.716532  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_to_dataframe(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"File does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "df = load_data_to_dataframe(file_name)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify model to load\n",
    "# model_to_load = \"1683270923\"\n",
    "\n",
    "# def load_and_display_model_architecture(file_name):\n",
    "#     if not os.path.exists(file_name):\n",
    "#         print(\"File does not exist.\")\n",
    "#         return None\n",
    "\n",
    "#     with open(file_name, \"r\") as json_file:\n",
    "#         model_json = json_file.read()\n",
    "    \n",
    "#     model = model_from_json(model_json)\n",
    "#     model.summary()\n",
    "\n",
    "#     return model\n",
    "\n",
    "# model_architecture_file = f\"../04_Age_Prediction/models/{model_to_load}.json\"\n",
    "# model = load_and_display_model_architecture(model_architecture_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
