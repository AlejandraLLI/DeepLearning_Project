{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 06:07:58.147641: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-15 06:07:58.200098: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-15 06:07:59.840676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, \n",
    "                          Dense, Dropout, BatchNormalization, \n",
    "                          LeakyReLU, DepthwiseConv2D, MaxPooling2D,\n",
    "                          Add)\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "\n",
    "# Set Tensorflow log level to error \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Set logger level to ERROR\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)  # Set logger level to ERROR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sets\n",
    "raw_train_path = \"../02_Data/face_age_data/face_age_balanced_train\"\n",
    "aug_train_path = \"../02_Data/face_age_data/augmented_data_train\" \n",
    "\n",
    "\n",
    "# Validation set\n",
    "raw_val_path = \"../02_Data/face_age_data/face_age_balanced_val\"\n",
    "\n",
    "# Testing set\n",
    "raw_test_path = \"../02_Data/face_age_data/face_age_balanced_test\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folders, img_size=(200, 200)):\n",
    "    X = []\n",
    "    y = []\n",
    "    for folder_path in folders:\n",
    "        for folder in os.listdir(folder_path):\n",
    "            if os.path.isdir(os.path.join(folder_path, folder)):\n",
    "                age = int(folder.replace(\"aug_\", \"\"))\n",
    "                for file in os.listdir(os.path.join(folder_path, folder)):\n",
    "                    img_path = os.path.join(folder_path, folder, file)\n",
    "                    img = Image.open(img_path)\n",
    "                    img = img.resize(img_size)\n",
    "                    img = np.array(img)\n",
    "                    X.append(img)\n",
    "                    y.append(age)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (200, 200)\n",
    "\n",
    "# Load training set\n",
    "folder_paths_train = [\n",
    "    raw_train_path, \n",
    "    # aug_train_path\n",
    "    ]\n",
    "X_train, y_train = load_data(folder_paths_train, img_size)\n",
    "X_train = X_train / 255.0  # Normalize pixel values\n",
    "\n",
    "# Load validation set\n",
    "folder_paths_val = [raw_val_path]\n",
    "X_val, y_val = load_data(folder_paths_val, img_size)\n",
    "X_val = X_val / 255.0  # Normalize pixel values\n",
    "\n",
    "# Load testing set\n",
    "folder_paths_test = [raw_test_path]\n",
    "X_test, y_test = load_data(folder_paths_test, img_size)\n",
    "X_test = X_test / 255.0  # Normalize pixel values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (5598, 200, 200, 3) \n",
      " - 5598: Number of images in the dataset \n",
      " - 200: Height of each image \n",
      " - 200: Width of each image \n",
      " - 3: Number of channels of each image (Red, Green, and Blue)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Shape of X: {X_train.shape}\", \"\\n\",\n",
    "    f\"- {X_train.shape[0]}: Number of images in the dataset\", \"\\n\",\n",
    "    f\"- {X_train.shape[1]}: Height of each image\", \"\\n\",\n",
    "    f\"- {X_train.shape[2]}: Width of each image\", \"\\n\",\n",
    "    f\"- {X_train.shape[3]}: Number of channels of each image (Red, Green, and Blue)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [32 32 32 ... 58 58 58]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels: {y_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of arrays: \n",
      " - X_train shape: (5598, 200, 200, 3) \n",
      " - y_train shape: (5598,) \n",
      " - X_val shape: (1400, 200, 200, 3) \n",
      " - y_val shape: (1400,) \n",
      " - X_test shape: (1750, 200, 200, 3) \n",
      " - y_test shape: (1750,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Size of arrays:\", \"\\n\",\n",
    "    f\"- X_train shape: {X_train.shape}\", \"\\n\",\n",
    "    f\"- y_train shape: {y_train.shape}\", \"\\n\",\n",
    "    f\"- X_val shape: {X_val.shape}\", \"\\n\",\n",
    "    f\"- y_val shape: {y_val.shape}\", \"\\n\",\n",
    "    f\"- X_test shape: {X_test.shape}\", \"\\n\",\n",
    "    f\"- y_test shape: {y_test.shape}\", \"\\n\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  8\n",
      "['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3', '/device:GPU:4', '/device:GPU:5', '/device:GPU:6', '/device:GPU:7']\n"
     ]
    }
   ],
   "source": [
    "# Check if GPUs are available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Get list of GPUS\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Extract names\n",
    "gpu_names = ['/device:GPU:' + gpu.name.split(':')[-1] for gpu in gpus]\n",
    "\n",
    "print(gpu_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "optimizer = \"adam\"\n",
    "loss = \"mean_squared_error\"\n",
    "metrics = ['mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 06:08:36.649434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9599 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "2023-05-15 06:08:36.650599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9599 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5\n",
      "2023-05-15 06:08:36.651576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9599 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5\n",
      "2023-05-15 06:08:36.652556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9599 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3e:00.0, compute capability: 7.5\n",
      "2023-05-15 06:08:36.653529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 9599 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:88:00.0, compute capability: 7.5\n",
      "2023-05-15 06:08:36.654406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 9599 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:89:00.0, compute capability: 7.5\n",
      "2023-05-15 06:08:36.655339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 9599 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b1:00.0, compute capability: 7.5\n",
      "2023-05-15 06:08:36.656280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 9599 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:b2:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "# Create a MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy(devices=gpu_names)\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (7, 7), padding='same', input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(128, (5, 5), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(1024),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(64),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# Open the strategy scope\n",
    "with strategy.scope():\n",
    "    input_shape = (img_size[0], img_size[1], 3)\n",
    "    model = create_model(input_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback to save the model's weights\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"../04_Age_Prediction/08_models/best_model.h5\", save_best_only=True)\n",
    "\n",
    "# Create a callback that stops the training if there is no improvement in the validation loss for 15 consecutive epochs\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 06:08:45.690678: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype float and shape [5598,200,200,3]\n",
      "\t [[{{node Placeholder/_10}}]]\n",
      "2023-05-15 06:08:45.691019: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype float and shape [5598,200,200,3]\n",
      "\t [[{{node Placeholder/_10}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 06:09:08.925064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-15 06:09:10.389405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-15 06:09:11.971801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-15 06:09:13.800170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-15 06:09:15.346383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-15 06:09:17.118627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-15 06:09:18.460415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-15 06:09:19.465516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8900\n",
      "2023-05-15 06:09:26.456768: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f832e499460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-15 06:09:26.456821: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-15 06:09:26.456836: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-15 06:09:26.456847: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-15 06:09:26.456857: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-15 06:09:26.456867: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (4): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-15 06:09:26.456876: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (5): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-15 06:09:26.456886: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (6): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-15 06:09:26.456895: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (7): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-05-15 06:09:26.468083: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-15 06:09:26.637278: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - ETA: 0s - loss: 1469.6007 - mae: 31.4687"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 06:09:44.032333: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [1750]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-05-15 06:09:44.032654: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [1750]\n",
      "\t [[{{node Placeholder/_11}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 63s 117ms/step - loss: 1469.6007 - mae: 31.4687 - val_loss: 850.2231 - val_mae: 22.0240\n",
      "Epoch 2/1000\n",
      "175/175 [==============================] - 16s 92ms/step - loss: 1060.4071 - mae: 27.1807 - val_loss: 588.8397 - val_mae: 19.2267\n",
      "Epoch 3/1000\n",
      "175/175 [==============================] - 15s 83ms/step - loss: 671.5253 - mae: 20.7477 - val_loss: 623.0220 - val_mae: 20.5943\n",
      "Epoch 4/1000\n",
      "175/175 [==============================] - 16s 91ms/step - loss: 428.4073 - mae: 15.9952 - val_loss: 208.4856 - val_mae: 10.9223\n",
      "Epoch 5/1000\n",
      "175/175 [==============================] - 14s 83ms/step - loss: 328.8006 - mae: 14.0338 - val_loss: 642.2393 - val_mae: 20.6211\n",
      "Epoch 6/1000\n",
      "175/175 [==============================] - 16s 91ms/step - loss: 311.0238 - mae: 13.7697 - val_loss: 164.6209 - val_mae: 9.7810\n",
      "Epoch 7/1000\n",
      "175/175 [==============================] - 14s 83ms/step - loss: 290.3718 - mae: 13.1935 - val_loss: 208.7930 - val_mae: 10.6692\n",
      "Epoch 8/1000\n",
      "175/175 [==============================] - 16s 91ms/step - loss: 287.7959 - mae: 13.2670 - val_loss: 141.5431 - val_mae: 8.9229\n",
      "Epoch 9/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 291.8419 - mae: 13.2824 - val_loss: 166.5189 - val_mae: 9.7131\n",
      "Epoch 10/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 279.5326 - mae: 13.0924 - val_loss: 219.8377 - val_mae: 11.1772\n",
      "Epoch 11/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 272.2119 - mae: 12.7835 - val_loss: 156.1567 - val_mae: 9.5220\n",
      "Epoch 12/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 268.3951 - mae: 12.8672 - val_loss: 222.7860 - val_mae: 11.0683\n",
      "Epoch 13/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 269.8645 - mae: 12.7236 - val_loss: 149.2366 - val_mae: 9.2606\n",
      "Epoch 14/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 256.2734 - mae: 12.4589 - val_loss: 203.3403 - val_mae: 10.7551\n",
      "Epoch 15/1000\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 257.9747 - mae: 12.5459 - val_loss: 131.2093 - val_mae: 8.8008\n",
      "Epoch 16/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 247.7977 - mae: 12.3774 - val_loss: 196.9148 - val_mae: 10.7980\n",
      "Epoch 17/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 297.8641 - mae: 13.5874 - val_loss: 150.2357 - val_mae: 9.4106\n",
      "Epoch 18/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 261.9563 - mae: 12.6520 - val_loss: 135.5716 - val_mae: 8.9748\n",
      "Epoch 19/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 257.3757 - mae: 12.5614 - val_loss: 135.9805 - val_mae: 9.0907\n",
      "Epoch 20/1000\n",
      "175/175 [==============================] - 15s 84ms/step - loss: 252.0817 - mae: 12.3028 - val_loss: 131.8067 - val_mae: 8.6375\n",
      "Epoch 21/1000\n",
      "175/175 [==============================] - 16s 92ms/step - loss: 237.1954 - mae: 12.0070 - val_loss: 126.1552 - val_mae: 8.3726\n",
      "Epoch 22/1000\n",
      "175/175 [==============================] - 15s 83ms/step - loss: 247.7274 - mae: 12.2400 - val_loss: 141.9288 - val_mae: 8.9634\n",
      "Epoch 23/1000\n",
      "175/175 [==============================] - 16s 91ms/step - loss: 243.2158 - mae: 12.1668 - val_loss: 123.4029 - val_mae: 8.4098\n",
      "Epoch 24/1000\n",
      "175/175 [==============================] - 16s 92ms/step - loss: 243.7824 - mae: 12.1674 - val_loss: 117.1903 - val_mae: 8.1439\n",
      "Epoch 25/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 249.4025 - mae: 12.3865 - val_loss: 127.7537 - val_mae: 8.4221\n",
      "Epoch 26/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 239.6891 - mae: 12.1445 - val_loss: 171.7479 - val_mae: 9.9383\n",
      "Epoch 27/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 242.8906 - mae: 12.1488 - val_loss: 125.5923 - val_mae: 8.4011\n",
      "Epoch 28/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 241.7670 - mae: 12.1849 - val_loss: 117.4654 - val_mae: 8.0704\n",
      "Epoch 29/1000\n",
      "175/175 [==============================] - 16s 91ms/step - loss: 240.8182 - mae: 12.1723 - val_loss: 115.7546 - val_mae: 8.0331\n",
      "Epoch 30/1000\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 230.1689 - mae: 11.8804 - val_loss: 115.6567 - val_mae: 8.1185\n",
      "Epoch 31/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 224.8518 - mae: 11.7650 - val_loss: 138.0761 - val_mae: 8.5934\n",
      "Epoch 32/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 227.6062 - mae: 11.8097 - val_loss: 148.5043 - val_mae: 9.5649\n",
      "Epoch 33/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 274.0942 - mae: 12.9510 - val_loss: 183.6664 - val_mae: 10.2503\n",
      "Epoch 34/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 241.2914 - mae: 12.1589 - val_loss: 132.1729 - val_mae: 8.5710\n",
      "Epoch 35/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 239.8281 - mae: 12.0614 - val_loss: 128.0603 - val_mae: 8.5729\n",
      "Epoch 36/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 234.1285 - mae: 11.9711 - val_loss: 128.6260 - val_mae: 8.5286\n",
      "Epoch 37/1000\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 230.1925 - mae: 11.9441 - val_loss: 113.7814 - val_mae: 7.8649\n",
      "Epoch 38/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 232.8855 - mae: 11.9942 - val_loss: 123.3835 - val_mae: 8.3013\n",
      "Epoch 39/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 228.5490 - mae: 11.9190 - val_loss: 118.2423 - val_mae: 8.0620\n",
      "Epoch 40/1000\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 218.5862 - mae: 11.6298 - val_loss: 111.4311 - val_mae: 7.9131\n",
      "Epoch 41/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 219.9265 - mae: 11.6158 - val_loss: 116.1381 - val_mae: 7.9531\n",
      "Epoch 42/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 222.8204 - mae: 11.6344 - val_loss: 119.4886 - val_mae: 8.0591\n",
      "Epoch 43/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 213.9457 - mae: 11.4273 - val_loss: 132.5927 - val_mae: 8.8350\n",
      "Epoch 44/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 220.3572 - mae: 11.6304 - val_loss: 131.7675 - val_mae: 8.6912\n",
      "Epoch 45/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 205.4090 - mae: 11.2795 - val_loss: 127.3384 - val_mae: 8.3246\n",
      "Epoch 46/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 216.7111 - mae: 11.5418 - val_loss: 125.2425 - val_mae: 8.3124\n",
      "Epoch 47/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 218.8831 - mae: 11.5691 - val_loss: 118.2449 - val_mae: 8.1408\n",
      "Epoch 48/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 225.6604 - mae: 11.8462 - val_loss: 114.1136 - val_mae: 7.9664\n",
      "Epoch 49/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 213.0199 - mae: 11.4968 - val_loss: 112.3113 - val_mae: 7.8790\n",
      "Epoch 50/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 212.5475 - mae: 11.4239 - val_loss: 131.5551 - val_mae: 8.6060\n",
      "Epoch 51/1000\n",
      "175/175 [==============================] - 15s 84ms/step - loss: 218.2549 - mae: 11.6367 - val_loss: 115.1607 - val_mae: 8.0046\n",
      "Epoch 52/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 206.3544 - mae: 11.2958 - val_loss: 117.6661 - val_mae: 7.9846\n",
      "Epoch 53/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 227.0917 - mae: 11.8252 - val_loss: 125.7709 - val_mae: 8.3732\n",
      "Epoch 54/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 215.9630 - mae: 11.4924 - val_loss: 122.0307 - val_mae: 8.3687\n",
      "Epoch 55/1000\n",
      "175/175 [==============================] - 14s 83ms/step - loss: 222.7714 - mae: 11.7267 - val_loss: 121.2756 - val_mae: 8.2263\n",
      "Epoch 56/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 213.8380 - mae: 11.5216 - val_loss: 120.3102 - val_mae: 8.2297\n",
      "Epoch 57/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 211.0301 - mae: 11.4157 - val_loss: 133.0121 - val_mae: 8.6536\n",
      "Epoch 58/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 211.6840 - mae: 11.4578 - val_loss: 151.2309 - val_mae: 9.4101\n",
      "Epoch 59/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 206.7785 - mae: 11.3016 - val_loss: 112.9500 - val_mae: 7.8529\n",
      "Epoch 60/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 211.2902 - mae: 11.4438 - val_loss: 115.0319 - val_mae: 8.0941\n",
      "Epoch 61/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 215.8514 - mae: 11.4716 - val_loss: 130.1798 - val_mae: 8.4996\n",
      "Epoch 62/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 213.6333 - mae: 11.4688 - val_loss: 121.3068 - val_mae: 8.2516\n",
      "Epoch 63/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 213.5054 - mae: 11.4786 - val_loss: 135.7277 - val_mae: 8.5770\n",
      "Epoch 64/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 218.2237 - mae: 11.6329 - val_loss: 128.4322 - val_mae: 8.4843\n",
      "Epoch 65/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 208.6841 - mae: 11.2919 - val_loss: 112.1996 - val_mae: 8.0358\n",
      "Epoch 66/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 208.9040 - mae: 11.4743 - val_loss: 158.8327 - val_mae: 9.5102\n",
      "Epoch 67/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 211.4448 - mae: 11.4659 - val_loss: 142.0715 - val_mae: 8.8274\n",
      "Epoch 68/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 193.7392 - mae: 10.9256 - val_loss: 135.9637 - val_mae: 8.7700\n",
      "Epoch 69/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 212.9931 - mae: 11.5208 - val_loss: 122.4547 - val_mae: 8.2148\n",
      "Epoch 70/1000\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 205.8998 - mae: 11.2263 - val_loss: 111.1525 - val_mae: 7.9259\n",
      "Epoch 71/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 207.3282 - mae: 11.3934 - val_loss: 111.4194 - val_mae: 8.0091\n",
      "Epoch 72/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 204.6191 - mae: 11.2699 - val_loss: 119.5921 - val_mae: 8.1097\n",
      "Epoch 73/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 218.6981 - mae: 11.6490 - val_loss: 135.2597 - val_mae: 8.6550\n",
      "Epoch 74/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 206.4133 - mae: 11.3629 - val_loss: 114.4702 - val_mae: 8.0892\n",
      "Epoch 75/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 220.0790 - mae: 11.6529 - val_loss: 119.8822 - val_mae: 8.3063\n",
      "Epoch 76/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 213.1347 - mae: 11.5013 - val_loss: 120.5411 - val_mae: 8.2568\n",
      "Epoch 77/1000\n",
      "175/175 [==============================] - 16s 92ms/step - loss: 203.0650 - mae: 11.1244 - val_loss: 108.6576 - val_mae: 7.8833\n",
      "Epoch 78/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 195.2149 - mae: 11.0161 - val_loss: 137.5330 - val_mae: 8.7062\n",
      "Epoch 79/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 200.4095 - mae: 11.1055 - val_loss: 129.6074 - val_mae: 8.4084\n",
      "Epoch 80/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 207.1752 - mae: 11.3739 - val_loss: 110.3069 - val_mae: 7.8254\n",
      "Epoch 81/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 204.6376 - mae: 11.3379 - val_loss: 117.5651 - val_mae: 8.2260\n",
      "Epoch 82/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 201.7544 - mae: 11.1509 - val_loss: 118.5030 - val_mae: 8.0556\n",
      "Epoch 83/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 205.6434 - mae: 11.3630 - val_loss: 119.8790 - val_mae: 8.3694\n",
      "Epoch 84/1000\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 208.6822 - mae: 11.4252 - val_loss: 105.8444 - val_mae: 7.7178\n",
      "Epoch 85/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 205.1936 - mae: 11.3068 - val_loss: 112.2109 - val_mae: 7.9323\n",
      "Epoch 86/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 199.9852 - mae: 11.1462 - val_loss: 124.0605 - val_mae: 8.4805\n",
      "Epoch 87/1000\n",
      "175/175 [==============================] - 16s 89ms/step - loss: 207.1419 - mae: 11.2958 - val_loss: 103.8931 - val_mae: 7.5978\n",
      "Epoch 88/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 195.9900 - mae: 11.0218 - val_loss: 110.8296 - val_mae: 7.9254\n",
      "Epoch 89/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 207.1557 - mae: 11.3144 - val_loss: 151.6757 - val_mae: 9.0649\n",
      "Epoch 90/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 196.2879 - mae: 11.0411 - val_loss: 109.5395 - val_mae: 7.8958\n",
      "Epoch 91/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 201.8835 - mae: 11.2177 - val_loss: 113.3293 - val_mae: 7.9814\n",
      "Epoch 92/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 203.1550 - mae: 11.2502 - val_loss: 114.6635 - val_mae: 7.9418\n",
      "Epoch 93/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 202.2620 - mae: 11.2780 - val_loss: 122.7331 - val_mae: 8.4266\n",
      "Epoch 94/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 200.6125 - mae: 11.1914 - val_loss: 127.2162 - val_mae: 8.3487\n",
      "Epoch 95/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 211.3914 - mae: 11.4846 - val_loss: 150.4991 - val_mae: 9.2603\n",
      "Epoch 96/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 206.3994 - mae: 11.3300 - val_loss: 145.7834 - val_mae: 9.0572\n",
      "Epoch 97/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 196.3813 - mae: 11.0400 - val_loss: 126.2670 - val_mae: 8.4354\n",
      "Epoch 98/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 199.3829 - mae: 11.1913 - val_loss: 105.7978 - val_mae: 7.7525\n",
      "Epoch 99/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 234.5641 - mae: 12.0697 - val_loss: 130.8539 - val_mae: 8.5781\n",
      "Epoch 100/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 206.2389 - mae: 11.2899 - val_loss: 130.1343 - val_mae: 8.8792\n",
      "Epoch 101/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 205.9388 - mae: 11.3164 - val_loss: 119.2004 - val_mae: 8.1876\n",
      "Epoch 102/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 200.7020 - mae: 11.1757 - val_loss: 129.1311 - val_mae: 8.7444\n",
      "Epoch 103/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 203.1381 - mae: 11.1872 - val_loss: 125.0266 - val_mae: 8.5849\n",
      "Epoch 104/1000\n",
      "175/175 [==============================] - 15s 84ms/step - loss: 198.3149 - mae: 11.0454 - val_loss: 104.9054 - val_mae: 7.7457\n",
      "Epoch 105/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 199.1856 - mae: 11.1812 - val_loss: 115.2828 - val_mae: 8.1741\n",
      "Epoch 106/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 199.1382 - mae: 11.0684 - val_loss: 110.9582 - val_mae: 7.9888\n",
      "Epoch 107/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 197.0786 - mae: 11.0526 - val_loss: 112.9611 - val_mae: 7.8257\n",
      "Epoch 108/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 201.3463 - mae: 11.2191 - val_loss: 119.5165 - val_mae: 8.0907\n",
      "Epoch 109/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 203.6873 - mae: 11.3198 - val_loss: 112.1055 - val_mae: 7.8443\n",
      "Epoch 110/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 203.4662 - mae: 11.2343 - val_loss: 108.1328 - val_mae: 7.7200\n",
      "Epoch 111/1000\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 191.5667 - mae: 10.9888 - val_loss: 103.1636 - val_mae: 7.5877\n",
      "Epoch 112/1000\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 198.6338 - mae: 11.1490 - val_loss: 102.8300 - val_mae: 7.5111\n",
      "Epoch 113/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 197.7032 - mae: 11.1005 - val_loss: 103.8987 - val_mae: 7.5727\n",
      "Epoch 114/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 205.1130 - mae: 11.3323 - val_loss: 111.7008 - val_mae: 7.8753\n",
      "Epoch 115/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 208.9269 - mae: 11.4585 - val_loss: 104.7249 - val_mae: 7.5805\n",
      "Epoch 116/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 198.5760 - mae: 11.2145 - val_loss: 108.5777 - val_mae: 7.8144\n",
      "Epoch 117/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 214.3430 - mae: 11.5663 - val_loss: 115.1468 - val_mae: 7.9214\n",
      "Epoch 118/1000\n",
      "175/175 [==============================] - 16s 90ms/step - loss: 208.5569 - mae: 11.4129 - val_loss: 101.3372 - val_mae: 7.5210\n",
      "Epoch 119/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 210.3202 - mae: 11.5157 - val_loss: 133.7464 - val_mae: 8.6376\n",
      "Epoch 120/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 193.8301 - mae: 11.0044 - val_loss: 107.2716 - val_mae: 7.6746\n",
      "Epoch 121/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 195.6562 - mae: 10.9752 - val_loss: 109.0241 - val_mae: 7.7434\n",
      "Epoch 122/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 203.6429 - mae: 11.3317 - val_loss: 112.4992 - val_mae: 7.8856\n",
      "Epoch 123/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 203.8579 - mae: 11.1729 - val_loss: 110.0631 - val_mae: 7.8738\n",
      "Epoch 124/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 205.1400 - mae: 11.3589 - val_loss: 105.4938 - val_mae: 7.6074\n",
      "Epoch 125/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 202.3055 - mae: 11.2508 - val_loss: 102.0923 - val_mae: 7.5253\n",
      "Epoch 126/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 199.6218 - mae: 11.1920 - val_loss: 114.7139 - val_mae: 8.0600\n",
      "Epoch 127/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 197.9399 - mae: 11.1451 - val_loss: 109.4380 - val_mae: 7.7127\n",
      "Epoch 128/1000\n",
      "175/175 [==============================] - 15s 84ms/step - loss: 203.6003 - mae: 11.2475 - val_loss: 117.4322 - val_mae: 8.0665\n",
      "Epoch 129/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 197.6776 - mae: 11.0926 - val_loss: 123.6170 - val_mae: 8.3656\n",
      "Epoch 130/1000\n",
      "175/175 [==============================] - 14s 83ms/step - loss: 194.8324 - mae: 11.0255 - val_loss: 106.4323 - val_mae: 7.7047\n",
      "Epoch 131/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 196.5591 - mae: 11.0723 - val_loss: 105.3160 - val_mae: 7.6702\n",
      "Epoch 132/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 191.2328 - mae: 10.9180 - val_loss: 122.3524 - val_mae: 8.3339\n",
      "Epoch 133/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 208.1806 - mae: 11.3821 - val_loss: 136.0959 - val_mae: 8.7380\n",
      "Epoch 134/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 203.3223 - mae: 11.2684 - val_loss: 131.2632 - val_mae: 8.5308\n",
      "Epoch 135/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 192.0613 - mae: 10.8976 - val_loss: 118.9314 - val_mae: 8.1957\n",
      "Epoch 136/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 208.7397 - mae: 11.3741 - val_loss: 115.3640 - val_mae: 8.0139\n",
      "Epoch 137/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 195.2840 - mae: 10.9957 - val_loss: 109.2492 - val_mae: 7.7197\n",
      "Epoch 138/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 195.1194 - mae: 11.0548 - val_loss: 116.1426 - val_mae: 7.9523\n",
      "Epoch 139/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 193.6792 - mae: 10.9569 - val_loss: 116.5283 - val_mae: 8.1627\n",
      "Epoch 140/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 191.7361 - mae: 10.9028 - val_loss: 107.1924 - val_mae: 7.7427\n",
      "Epoch 141/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 196.4912 - mae: 11.1430 - val_loss: 107.9326 - val_mae: 7.8145\n",
      "Epoch 142/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 193.6053 - mae: 11.0553 - val_loss: 103.9460 - val_mae: 7.6328\n",
      "Epoch 143/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 205.8721 - mae: 11.3986 - val_loss: 114.0770 - val_mae: 7.9594\n",
      "Epoch 144/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 202.5762 - mae: 11.2283 - val_loss: 104.1630 - val_mae: 7.6510\n",
      "Epoch 145/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 194.4002 - mae: 10.9968 - val_loss: 113.3763 - val_mae: 8.0970\n",
      "Epoch 146/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 192.8949 - mae: 10.9420 - val_loss: 126.5189 - val_mae: 8.4936\n",
      "Epoch 147/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 196.9658 - mae: 11.0605 - val_loss: 118.7950 - val_mae: 8.1526\n",
      "Epoch 148/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 210.2744 - mae: 11.4631 - val_loss: 123.8464 - val_mae: 8.4734\n",
      "Epoch 149/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 203.8457 - mae: 11.2087 - val_loss: 107.8854 - val_mae: 7.8127\n",
      "Epoch 150/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 205.6787 - mae: 11.3956 - val_loss: 137.2833 - val_mae: 9.1629\n",
      "Epoch 151/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 194.7313 - mae: 10.9698 - val_loss: 104.4581 - val_mae: 7.6043\n",
      "Epoch 152/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 196.0332 - mae: 11.0452 - val_loss: 108.6502 - val_mae: 7.9164\n",
      "Epoch 153/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 194.7380 - mae: 10.9673 - val_loss: 104.4309 - val_mae: 7.6969\n",
      "Epoch 154/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 206.6530 - mae: 11.3374 - val_loss: 108.3206 - val_mae: 7.8469\n",
      "Epoch 155/1000\n",
      "175/175 [==============================] - 15s 85ms/step - loss: 201.5284 - mae: 11.2615 - val_loss: 124.5261 - val_mae: 8.2573\n",
      "Epoch 156/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 198.9711 - mae: 11.0811 - val_loss: 103.7376 - val_mae: 7.6099\n",
      "Epoch 157/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 191.4235 - mae: 10.9805 - val_loss: 105.4992 - val_mae: 7.6794\n",
      "Epoch 158/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 201.0130 - mae: 11.2164 - val_loss: 104.9575 - val_mae: 7.6844\n",
      "Epoch 159/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 191.3741 - mae: 10.9555 - val_loss: 128.2693 - val_mae: 8.4186\n",
      "Epoch 160/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 192.3885 - mae: 10.9029 - val_loss: 106.1747 - val_mae: 7.6986\n",
      "Epoch 161/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 197.6107 - mae: 11.1634 - val_loss: 109.3127 - val_mae: 7.8074\n",
      "Epoch 162/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 192.1662 - mae: 11.0316 - val_loss: 112.8202 - val_mae: 7.9591\n",
      "Epoch 163/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 192.8555 - mae: 11.0449 - val_loss: 105.3948 - val_mae: 7.6901\n",
      "Epoch 164/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 195.7015 - mae: 10.9882 - val_loss: 109.9811 - val_mae: 7.7790\n",
      "Epoch 165/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 193.7393 - mae: 11.1272 - val_loss: 117.0930 - val_mae: 8.1072\n",
      "Epoch 166/1000\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 204.4789 - mae: 11.2992 - val_loss: 109.7219 - val_mae: 7.8279\n",
      "Epoch 167/1000\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 192.5603 - mae: 10.8946 - val_loss: 109.4743 - val_mae: 7.7515\n",
      "Epoch 168/1000\n",
      "175/175 [==============================] - 15s 85ms/step - loss: 190.5696 - mae: 10.8526 - val_loss: 107.8793 - val_mae: 7.7898\n"
     ]
    }
   ],
   "source": [
    "# Train the model with your data\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, Load, and Display Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model attributes\n",
    "timestamp = int(time.time())\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "description = f\"{timestamp}\"\n",
    "input_shape = str(X_train.shape[1:])\n",
    "num_params = model.count_params()\n",
    "epochs = len(history.history['loss'])\n",
    "optimizer = optimizer\n",
    "loss_function = loss\n",
    "train_mae = min(history.history['mae'])\n",
    "validation_mae = min(history.history['val_mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ../04_Age_Prediction/08_models/1684133411.h5\n"
     ]
    }
   ],
   "source": [
    "def append_model_history_to_file(file_name, description, input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae):\n",
    "    train_mae = round(train_mae, 6)\n",
    "    validation_mae = round(validation_mae, 6)\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        with open(file_name, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['Model', 'Input Shape', 'Model Params', 'Epochs', 'Batch Size', 'Optimizer', 'Loss Function', 'Train MAE', 'Validation MAE'])\n",
    "\n",
    "    with open(file_name, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        try:\n",
    "            next(csv_reader)  # Skip the header row\n",
    "        except StopIteration:\n",
    "            pass  # The CSV file is empty or only contains the header row\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if list(map(str, row[1:])) == list(map(str, [input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])):\n",
    "                print(\"Entry with the same parameters already exists.\")\n",
    "                return False\n",
    "\n",
    "    with open(file_name, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([description, input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])\n",
    "\n",
    "    return True\n",
    "\n",
    "new_entry_added = append_model_history_to_file(file_name, description, input_shape, num_params, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae)\n",
    "\n",
    "# Save model\n",
    "def save_model_architecture(model, file_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "if new_entry_added:\n",
    "    model_file = f\"../04_Age_Prediction/08_models/{description}.h5\"\n",
    "    model.save(model_file)\n",
    "    print(f\"Model saved at {model_file}\")\n",
    "else:\n",
    "    print(\"Model not saved as an entry with the same parameters already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input Shape</th>\n",
       "      <th>Model Params</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Loss Function</th>\n",
       "      <th>Train MAE</th>\n",
       "      <th>Validation MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1684111562</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>8770497</td>\n",
       "      <td>155</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>12.435976</td>\n",
       "      <td>8.629502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1684118998</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>33625217</td>\n",
       "      <td>132</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.202019</td>\n",
       "      <td>6.590096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1684126499</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>18144705</td>\n",
       "      <td>170</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.910834</td>\n",
       "      <td>6.491637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1684130813</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>9872321</td>\n",
       "      <td>213</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.675910</td>\n",
       "      <td>7.109921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1684133411</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>10344897</td>\n",
       "      <td>168</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.852635</td>\n",
       "      <td>7.511125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model    Input Shape  Model Params  Epochs  Batch Size Optimizer  \\\n",
       "0  1684111562  (200, 200, 3)       8770497     155          32      adam   \n",
       "1  1684118998  (200, 200, 3)      33625217     132          32      adam   \n",
       "2  1684126499  (200, 200, 3)      18144705     170          32      adam   \n",
       "3  1684130813  (200, 200, 3)       9872321     213          32      adam   \n",
       "4  1684133411  (200, 200, 3)      10344897     168          32      adam   \n",
       "\n",
       "        Loss Function  Train MAE  Validation MAE  \n",
       "0  mean_squared_error  12.435976        8.629502  \n",
       "1  mean_squared_error  10.202019        6.590096  \n",
       "2  mean_squared_error   9.910834        6.491637  \n",
       "3  mean_squared_error  10.675910        7.109921  \n",
       "4  mean_squared_error  10.852635        7.511125  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_to_dataframe(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"File does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "df = load_data_to_dataframe(file_name)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify model to load\n",
    "# model_to_load = \"1683270923\"\n",
    "\n",
    "# def load_and_display_model_architecture(file_name):\n",
    "#     if not os.path.exists(file_name):\n",
    "#         print(\"File does not exist.\")\n",
    "#         return None\n",
    "\n",
    "#     with open(file_name, \"r\") as json_file:\n",
    "#         model_json = json_file.read()\n",
    "    \n",
    "#     model = model_from_json(model_json)\n",
    "#     model.summary()\n",
    "\n",
    "#     return model\n",
    "\n",
    "# model_architecture_file = f\"../04_Age_Prediction/models/{model_to_load}.json\"\n",
    "# model = load_and_display_model_architecture(model_architecture_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
