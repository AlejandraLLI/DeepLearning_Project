{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 20:23:32.263980: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-10 20:23:32.320371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 20:23:35.207664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, LeakyReLU, DepthwiseConv2D\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Data Path\n",
    "image_path = '../02_Data/face_age' # Added path to gitingnore, you will have to add data to this path\n",
    "\n",
    "# Augmented Data Path\n",
    "# image_path = '../02_Data/augmented_data' # Added path to gitingnore, you will have to add data to this path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path, img_size=(200, 200)):\n",
    "    X = []\n",
    "    y = []\n",
    "    for folder in os.listdir(folder_path):\n",
    "        if os.path.isdir(os.path.join(folder_path, folder)):\n",
    "            age = int(folder.replace(\"aug_\", \"\"))\n",
    "            for file in os.listdir(os.path.join(folder_path, folder)):\n",
    "                img_path = os.path.join(folder_path, folder, file)\n",
    "                img = Image.open(img_path)\n",
    "                img = img.resize(img_size)\n",
    "                img = np.array(img)\n",
    "                X.append(img)\n",
    "                y.append(age)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "folder_path = image_path\n",
    "img_size = (200, 200)\n",
    "X, y = load_data(folder_path, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Shape of X: {X.shape}\", \"\\n\",\n",
    "    f\"- {X.shape[0]}: Number of images in the dataset\", \"\\n\",\n",
    "    f\"- {X.shape[1]}: Height of each image\", \"\\n\",\n",
    "    f\"- {X.shape[2]}: Width of each image\", \"\\n\",\n",
    "    f\"- {X.shape[3]}: Number of channels of each image (Red, Green, and Blue)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Labels: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "X = X / 255.0\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Size of arrays:\", \"\\n\",\n",
    "    f\"- X_train shape: {X_train.shape}\", \"\\n\",\n",
    "    f\"- X_test shape: {X_test.shape}\", \"\\n\",\n",
    "    f\"- y_train shape: {y_train.shape}\", \"\\n\",\n",
    "    f\"- y_test shape: {y_test.shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train test split\n",
    "# np.save(\"X_train.npy\", X_train)\n",
    "# np.save(\"X_test.npy\", X_test)\n",
    "# np.save(\"y_train.npy\", y_train)\n",
    "# np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "# # Load saved train-test split data\n",
    "X_train = np.load(\"X_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_test = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPUs are available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 650\n",
    "batch_size = 32\n",
    "optimizer = \"adam\"\n",
    "loss = \"mean_squared_error\"\n",
    "metrics = ['mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Create a MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\", \"GPU:4\"])\n",
    "\n",
    "# Residual block\n",
    "def residual_block(input_layer, filters):\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(input_layer)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, input_layer])\n",
    "    return x\n",
    "\n",
    "def create_model(input_shape):\n",
    "    input_layer = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), padding='same')(input_layer)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    for _ in range(3):\n",
    "        x = residual_block(x, 32)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    for _ in range(4):\n",
    "        x = residual_block(x, 64)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    for _ in range(6):\n",
    "        x = residual_block(x, 128)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    for _ in range(8):\n",
    "        x = residual_block(x, 256)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2048)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(1024)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(512)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(256)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# Open the strategy scope\n",
    "with strategy.scope():\n",
    "    input_shape = (img_size[0], img_size[1], 3)\n",
    "    model = create_model(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 08:07:17.994043: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype float and shape [7822,200,200,3]\n",
      "\t [[{{node Placeholder/_10}}]]\n",
      "2023-05-07 08:07:17.994420: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_10' with dtype float and shape [7822,200,200,3]\n",
      "\t [[{{node Placeholder/_10}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/650\n",
      "245/245 [==============================] - ETA: 0s - loss: 1062.9139 - mae: 26.1782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 08:09:51.971928: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [1956]\n",
      "\t [[{{node Placeholder/_11}}]]\n",
      "2023-05-07 08:09:51.972327: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_11' with dtype int64 and shape [1956]\n",
      "\t [[{{node Placeholder/_11}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 162s 226ms/step - loss: 1062.9139 - mae: 26.1782 - val_loss: 626.4092 - val_mae: 19.0167\n",
      "Epoch 2/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 401.5969 - mae: 15.1582 - val_loss: 869.4304 - val_mae: 25.4035\n",
      "Epoch 3/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 293.2780 - mae: 12.9755 - val_loss: 954.4639 - val_mae: 26.6929\n",
      "Epoch 4/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 278.7328 - mae: 12.7570 - val_loss: 171.8366 - val_mae: 9.7522\n",
      "Epoch 5/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 255.5325 - mae: 12.0728 - val_loss: 293.7106 - val_mae: 12.7066\n",
      "Epoch 6/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 248.1056 - mae: 11.8836 - val_loss: 141.3206 - val_mae: 8.9341\n",
      "Epoch 7/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 244.1530 - mae: 11.8362 - val_loss: 149.5491 - val_mae: 9.0259\n",
      "Epoch 8/650\n",
      "245/245 [==============================] - 46s 186ms/step - loss: 233.5808 - mae: 11.6815 - val_loss: 124.8462 - val_mae: 8.3018\n",
      "Epoch 9/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 222.7103 - mae: 11.4300 - val_loss: 424.1232 - val_mae: 15.2867\n",
      "Epoch 10/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 214.9088 - mae: 11.1926 - val_loss: 152.0511 - val_mae: 9.0122\n",
      "Epoch 11/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 214.9114 - mae: 11.1862 - val_loss: 198.5839 - val_mae: 9.9719\n",
      "Epoch 12/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 204.7044 - mae: 10.9037 - val_loss: 150.4142 - val_mae: 9.2383\n",
      "Epoch 13/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 201.1588 - mae: 10.8490 - val_loss: 182.2251 - val_mae: 9.3710\n",
      "Epoch 14/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 195.4972 - mae: 10.7316 - val_loss: 671.1770 - val_mae: 21.1761\n",
      "Epoch 15/650\n",
      "245/245 [==============================] - 45s 186ms/step - loss: 337.0235 - mae: 14.0831 - val_loss: 198.8640 - val_mae: 10.0436\n",
      "Epoch 16/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 248.4022 - mae: 11.9947 - val_loss: 152.3586 - val_mae: 9.0389\n",
      "Epoch 17/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 236.8731 - mae: 11.7547 - val_loss: 190.1802 - val_mae: 11.0085\n",
      "Epoch 18/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 227.5545 - mae: 11.5791 - val_loss: 132.5113 - val_mae: 8.9701\n",
      "Epoch 19/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 220.6154 - mae: 11.3662 - val_loss: 187.1318 - val_mae: 10.8287\n",
      "Epoch 20/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 216.2585 - mae: 11.1826 - val_loss: 167.1491 - val_mae: 9.1361\n",
      "Epoch 21/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 203.9219 - mae: 10.9835 - val_loss: 192.6392 - val_mae: 9.9189\n",
      "Epoch 22/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 194.0312 - mae: 10.5790 - val_loss: 183.4579 - val_mae: 9.7422\n",
      "Epoch 23/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 199.5001 - mae: 10.8811 - val_loss: 161.3029 - val_mae: 8.9957\n",
      "Epoch 24/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 222.1028 - mae: 11.3660 - val_loss: 145.6842 - val_mae: 8.4805\n",
      "Epoch 25/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 206.1195 - mae: 11.0410 - val_loss: 159.1807 - val_mae: 9.6537\n",
      "Epoch 26/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 202.9516 - mae: 10.9237 - val_loss: 109.7935 - val_mae: 7.8344\n",
      "Epoch 27/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 186.6567 - mae: 10.4855 - val_loss: 237.5292 - val_mae: 10.8920\n",
      "Epoch 28/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 192.8747 - mae: 10.6623 - val_loss: 157.7521 - val_mae: 8.9393\n",
      "Epoch 29/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 180.2656 - mae: 10.3914 - val_loss: 190.5281 - val_mae: 9.8428\n",
      "Epoch 30/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 198.8979 - mae: 10.7983 - val_loss: 123.8270 - val_mae: 7.9815\n",
      "Epoch 31/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 194.9412 - mae: 10.7608 - val_loss: 168.8525 - val_mae: 9.2236\n",
      "Epoch 32/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 185.0177 - mae: 10.5596 - val_loss: 120.8714 - val_mae: 8.2303\n",
      "Epoch 33/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 188.4754 - mae: 10.5552 - val_loss: 123.4227 - val_mae: 8.2801\n",
      "Epoch 34/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 174.1737 - mae: 10.1932 - val_loss: 130.2727 - val_mae: 8.9645\n",
      "Epoch 35/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 185.2555 - mae: 10.5348 - val_loss: 279.1737 - val_mae: 12.3087\n",
      "Epoch 36/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 181.3859 - mae: 10.3571 - val_loss: 97.4426 - val_mae: 7.4547\n",
      "Epoch 37/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 176.0278 - mae: 10.3023 - val_loss: 139.3444 - val_mae: 8.8150\n",
      "Epoch 38/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 166.7947 - mae: 10.0158 - val_loss: 223.4227 - val_mae: 10.7824\n",
      "Epoch 39/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 188.5155 - mae: 10.5280 - val_loss: 107.1683 - val_mae: 7.3251\n",
      "Epoch 40/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 177.3725 - mae: 10.3280 - val_loss: 105.9619 - val_mae: 7.8543\n",
      "Epoch 41/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 177.0152 - mae: 10.3301 - val_loss: 135.7363 - val_mae: 8.8568\n",
      "Epoch 42/650\n",
      "245/245 [==============================] - 48s 197ms/step - loss: 165.5560 - mae: 10.1160 - val_loss: 101.0002 - val_mae: 7.1761\n",
      "Epoch 43/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 163.6205 - mae: 10.0198 - val_loss: 183.8520 - val_mae: 10.1628\n",
      "Epoch 44/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 164.2388 - mae: 10.0479 - val_loss: 155.4536 - val_mae: 9.5049\n",
      "Epoch 45/650\n",
      "245/245 [==============================] - 48s 197ms/step - loss: 166.1106 - mae: 10.1127 - val_loss: 149.5333 - val_mae: 9.4190\n",
      "Epoch 46/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 176.3755 - mae: 10.3754 - val_loss: 240.7506 - val_mae: 11.2401\n",
      "Epoch 47/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 160.0643 - mae: 9.8007 - val_loss: 113.9989 - val_mae: 8.3209\n",
      "Epoch 48/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 149.7789 - mae: 9.6126 - val_loss: 108.6987 - val_mae: 7.5912\n",
      "Epoch 49/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 156.8401 - mae: 9.8368 - val_loss: 178.4593 - val_mae: 10.6318\n",
      "Epoch 50/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 167.2840 - mae: 10.0278 - val_loss: 197.1185 - val_mae: 10.1546\n",
      "Epoch 51/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 174.0850 - mae: 10.1879 - val_loss: 135.8544 - val_mae: 8.8619\n",
      "Epoch 52/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 160.8652 - mae: 9.9348 - val_loss: 95.7878 - val_mae: 7.3711\n",
      "Epoch 53/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 162.6856 - mae: 9.8675 - val_loss: 126.2002 - val_mae: 8.0732\n",
      "Epoch 54/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 162.0619 - mae: 9.8999 - val_loss: 147.8348 - val_mae: 8.9613\n",
      "Epoch 55/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 152.3468 - mae: 9.6035 - val_loss: 116.7196 - val_mae: 7.9361\n",
      "Epoch 56/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 142.2577 - mae: 9.3194 - val_loss: 106.0492 - val_mae: 7.3121\n",
      "Epoch 57/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 148.3263 - mae: 9.5041 - val_loss: 99.8124 - val_mae: 7.5370\n",
      "Epoch 58/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 144.9991 - mae: 9.5193 - val_loss: 148.5482 - val_mae: 9.0131\n",
      "Epoch 59/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 143.5604 - mae: 9.4293 - val_loss: 104.7156 - val_mae: 7.7562\n",
      "Epoch 60/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 141.3500 - mae: 9.2515 - val_loss: 130.1750 - val_mae: 8.5178\n",
      "Epoch 61/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 132.0010 - mae: 9.0464 - val_loss: 94.6369 - val_mae: 7.2534\n",
      "Epoch 62/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 158.6774 - mae: 9.8743 - val_loss: 179.6863 - val_mae: 9.9067\n",
      "Epoch 63/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 143.3652 - mae: 9.3862 - val_loss: 202.8077 - val_mae: 11.5069\n",
      "Epoch 64/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 149.2067 - mae: 9.5277 - val_loss: 132.0635 - val_mae: 8.1483\n",
      "Epoch 65/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 142.6447 - mae: 9.3684 - val_loss: 190.3783 - val_mae: 9.5849\n",
      "Epoch 66/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 134.6343 - mae: 9.0973 - val_loss: 122.4456 - val_mae: 8.2107\n",
      "Epoch 67/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 138.6977 - mae: 9.3044 - val_loss: 240.0926 - val_mae: 10.9451\n",
      "Epoch 68/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 136.1158 - mae: 9.1520 - val_loss: 105.5658 - val_mae: 7.6078\n",
      "Epoch 69/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 132.5815 - mae: 9.1133 - val_loss: 104.9677 - val_mae: 7.7434\n",
      "Epoch 70/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 136.5770 - mae: 9.2288 - val_loss: 179.9433 - val_mae: 9.1495\n",
      "Epoch 71/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 150.6531 - mae: 9.5973 - val_loss: 1137.8605 - val_mae: 24.0485\n",
      "Epoch 72/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 157.5271 - mae: 9.7969 - val_loss: 95.3761 - val_mae: 6.9901\n",
      "Epoch 73/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 135.8145 - mae: 9.1493 - val_loss: 92.5194 - val_mae: 7.1066\n",
      "Epoch 74/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 140.4127 - mae: 9.3791 - val_loss: 130.4936 - val_mae: 8.3180\n",
      "Epoch 75/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 129.7232 - mae: 8.9177 - val_loss: 124.1378 - val_mae: 8.0645\n",
      "Epoch 76/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 131.8339 - mae: 9.0965 - val_loss: 138.2017 - val_mae: 8.1262\n",
      "Epoch 77/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 133.3208 - mae: 9.1548 - val_loss: 143.3956 - val_mae: 8.5292\n",
      "Epoch 78/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 137.3044 - mae: 9.1880 - val_loss: 252.1067 - val_mae: 11.2438\n",
      "Epoch 79/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 130.5882 - mae: 9.0139 - val_loss: 111.3401 - val_mae: 7.9089\n",
      "Epoch 80/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 132.7143 - mae: 9.1081 - val_loss: 139.6239 - val_mae: 7.8654\n",
      "Epoch 81/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 126.2081 - mae: 8.9293 - val_loss: 109.8042 - val_mae: 7.2809\n",
      "Epoch 82/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 129.1628 - mae: 9.0392 - val_loss: 109.3830 - val_mae: 7.8367\n",
      "Epoch 83/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 123.2501 - mae: 8.8082 - val_loss: 101.2856 - val_mae: 7.4553\n",
      "Epoch 84/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 126.6038 - mae: 8.8842 - val_loss: 91.6566 - val_mae: 7.0366\n",
      "Epoch 85/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 121.8947 - mae: 8.7465 - val_loss: 119.2131 - val_mae: 7.7812\n",
      "Epoch 86/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 123.5323 - mae: 8.7515 - val_loss: 165.4501 - val_mae: 8.7197\n",
      "Epoch 87/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 122.9458 - mae: 8.7332 - val_loss: 216.9738 - val_mae: 10.0661\n",
      "Epoch 88/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 135.6246 - mae: 9.1096 - val_loss: 242.0195 - val_mae: 10.9172\n",
      "Epoch 89/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 150.0214 - mae: 9.5616 - val_loss: 263.7322 - val_mae: 10.8469\n",
      "Epoch 90/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 124.6350 - mae: 8.8376 - val_loss: 155.4090 - val_mae: 8.7322\n",
      "Epoch 91/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 171.3650 - mae: 10.1536 - val_loss: 181.9482 - val_mae: 9.3549\n",
      "Epoch 92/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 141.0267 - mae: 9.2843 - val_loss: 152.1729 - val_mae: 8.9345\n",
      "Epoch 93/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 128.6734 - mae: 8.9437 - val_loss: 131.2757 - val_mae: 8.2951\n",
      "Epoch 94/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 131.2720 - mae: 9.0357 - val_loss: 105.7504 - val_mae: 7.3525\n",
      "Epoch 95/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 123.7224 - mae: 8.8155 - val_loss: 148.5753 - val_mae: 8.6816\n",
      "Epoch 96/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 121.2579 - mae: 8.7452 - val_loss: 157.1053 - val_mae: 8.4255\n",
      "Epoch 97/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 124.4230 - mae: 8.7905 - val_loss: 161.3659 - val_mae: 8.8401\n",
      "Epoch 98/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 116.4530 - mae: 8.5185 - val_loss: 389.5211 - val_mae: 10.2072\n",
      "Epoch 99/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 123.1852 - mae: 8.7556 - val_loss: 122.4074 - val_mae: 7.8240\n",
      "Epoch 100/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 129.9328 - mae: 8.9829 - val_loss: 153.3953 - val_mae: 8.4838\n",
      "Epoch 101/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 130.6009 - mae: 9.0794 - val_loss: 144.7188 - val_mae: 8.2797\n",
      "Epoch 102/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 120.8999 - mae: 8.7177 - val_loss: 133.4148 - val_mae: 8.1898\n",
      "Epoch 103/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 128.0267 - mae: 8.9615 - val_loss: 164.2844 - val_mae: 8.7294\n",
      "Epoch 104/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 119.7169 - mae: 8.7026 - val_loss: 118.5308 - val_mae: 7.9820\n",
      "Epoch 105/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 128.7523 - mae: 8.9781 - val_loss: 535.9753 - val_mae: 12.9637\n",
      "Epoch 106/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 123.1188 - mae: 8.7938 - val_loss: 104.0247 - val_mae: 7.6658\n",
      "Epoch 107/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 129.2568 - mae: 8.9656 - val_loss: 618.8245 - val_mae: 11.5489\n",
      "Epoch 108/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 129.0956 - mae: 9.0696 - val_loss: 591.0627 - val_mae: 11.8572\n",
      "Epoch 109/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 115.9597 - mae: 8.4801 - val_loss: 686.3154 - val_mae: 11.7885\n",
      "Epoch 110/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 118.1138 - mae: 8.5596 - val_loss: 829.1867 - val_mae: 11.8228\n",
      "Epoch 111/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 124.0831 - mae: 8.9338 - val_loss: 3810.0449 - val_mae: 20.0108\n",
      "Epoch 112/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 121.2416 - mae: 8.7372 - val_loss: 329.1745 - val_mae: 9.6421\n",
      "Epoch 113/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 130.4996 - mae: 9.0001 - val_loss: 374.9634 - val_mae: 10.2019\n",
      "Epoch 114/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 116.2020 - mae: 8.5482 - val_loss: 361.8568 - val_mae: 10.0701\n",
      "Epoch 115/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 133.7467 - mae: 9.1351 - val_loss: 179.5693 - val_mae: 8.6930\n",
      "Epoch 116/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 121.7242 - mae: 8.7764 - val_loss: 192.6093 - val_mae: 8.5413\n",
      "Epoch 117/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 117.9359 - mae: 8.5949 - val_loss: 203.9214 - val_mae: 9.1597\n",
      "Epoch 118/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 119.9848 - mae: 8.6966 - val_loss: 221.8346 - val_mae: 9.6894\n",
      "Epoch 119/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 131.2837 - mae: 9.0580 - val_loss: 597.3281 - val_mae: 10.7074\n",
      "Epoch 120/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.5940 - mae: 8.4833 - val_loss: 1214.6243 - val_mae: 13.5862\n",
      "Epoch 121/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 116.3503 - mae: 8.5389 - val_loss: 474.7801 - val_mae: 9.8034\n",
      "Epoch 122/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 121.8104 - mae: 8.8179 - val_loss: 2502.5803 - val_mae: 17.8455\n",
      "Epoch 123/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 119.8828 - mae: 8.6687 - val_loss: 321.0745 - val_mae: 9.9266\n",
      "Epoch 124/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 132.3802 - mae: 9.0309 - val_loss: 1296.4731 - val_mae: 13.6507\n",
      "Epoch 125/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 124.0190 - mae: 8.8685 - val_loss: 1421.1923 - val_mae: 15.5839\n",
      "Epoch 126/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 116.4017 - mae: 8.6687 - val_loss: 876.1075 - val_mae: 13.3901\n",
      "Epoch 127/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.2928 - mae: 8.5098 - val_loss: 566.4875 - val_mae: 10.8018\n",
      "Epoch 128/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 116.2395 - mae: 8.6170 - val_loss: 210.1651 - val_mae: 9.6743\n",
      "Epoch 129/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 112.9112 - mae: 8.3592 - val_loss: 693.8401 - val_mae: 11.4706\n",
      "Epoch 130/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 120.8964 - mae: 8.7697 - val_loss: 50865.9219 - val_mae: 128.3789\n",
      "Epoch 131/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 127.2848 - mae: 8.9128 - val_loss: 376.6283 - val_mae: 9.6751\n",
      "Epoch 132/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 119.3039 - mae: 8.6303 - val_loss: 1112.5745 - val_mae: 11.6716\n",
      "Epoch 133/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 117.7171 - mae: 8.6369 - val_loss: 1851.4956 - val_mae: 13.8737\n",
      "Epoch 134/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 112.3077 - mae: 8.4103 - val_loss: 644.5474 - val_mae: 11.7413\n",
      "Epoch 135/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 123.1606 - mae: 8.7763 - val_loss: 3953.2158 - val_mae: 25.2312\n",
      "Epoch 136/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 111.5543 - mae: 8.3161 - val_loss: 496.7188 - val_mae: 9.6109\n",
      "Epoch 137/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 131.4174 - mae: 9.1310 - val_loss: 1448.5248 - val_mae: 13.4044\n",
      "Epoch 138/650\n",
      "245/245 [==============================] - 45s 186ms/step - loss: 124.8347 - mae: 8.8609 - val_loss: 2417.3569 - val_mae: 16.7779\n",
      "Epoch 139/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 123.7053 - mae: 8.8412 - val_loss: 1401.1979 - val_mae: 15.5178\n",
      "Epoch 140/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 119.6702 - mae: 8.6593 - val_loss: 1746.7397 - val_mae: 15.7944\n",
      "Epoch 141/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 117.4282 - mae: 8.5304 - val_loss: 2035.1329 - val_mae: 15.4497\n",
      "Epoch 142/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 119.0534 - mae: 8.7091 - val_loss: 624.2427 - val_mae: 12.0607\n",
      "Epoch 143/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 114.0302 - mae: 8.4205 - val_loss: 592.4499 - val_mae: 10.9036\n",
      "Epoch 144/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 117.7660 - mae: 8.5989 - val_loss: 760.6323 - val_mae: 10.5576\n",
      "Epoch 145/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 120.4049 - mae: 8.6980 - val_loss: 1357.5134 - val_mae: 13.9342\n",
      "Epoch 146/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 118.3040 - mae: 8.6386 - val_loss: 1136.9435 - val_mae: 13.2880\n",
      "Epoch 147/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.2850 - mae: 8.4042 - val_loss: 1045.1418 - val_mae: 12.4920\n",
      "Epoch 148/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 112.6966 - mae: 8.4402 - val_loss: 1065.6516 - val_mae: 14.0771\n",
      "Epoch 149/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 115.6137 - mae: 8.4827 - val_loss: 482.5222 - val_mae: 10.6787\n",
      "Epoch 150/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 114.5920 - mae: 8.4791 - val_loss: 641.5059 - val_mae: 10.5726\n",
      "Epoch 151/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 113.9902 - mae: 8.4611 - val_loss: 790.0349 - val_mae: 11.5248\n",
      "Epoch 152/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 117.2326 - mae: 8.6435 - val_loss: 1122.9919 - val_mae: 12.9118\n",
      "Epoch 153/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 115.8260 - mae: 8.5431 - val_loss: 810.2257 - val_mae: 12.4138\n",
      "Epoch 154/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 117.8689 - mae: 8.5828 - val_loss: 3611.0491 - val_mae: 21.2237\n",
      "Epoch 155/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.4704 - mae: 8.5133 - val_loss: 623.8416 - val_mae: 11.7474\n",
      "Epoch 156/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 124.8181 - mae: 8.8379 - val_loss: 989.0057 - val_mae: 12.7808\n",
      "Epoch 157/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 117.8251 - mae: 8.5796 - val_loss: 3479.5557 - val_mae: 20.1732\n",
      "Epoch 158/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 119.1161 - mae: 8.6748 - val_loss: 186.9100 - val_mae: 9.4327\n",
      "Epoch 159/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 117.6404 - mae: 8.5949 - val_loss: 180.8270 - val_mae: 8.9975\n",
      "Epoch 160/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.4834 - mae: 8.4036 - val_loss: 201.3852 - val_mae: 8.9869\n",
      "Epoch 161/650\n",
      "245/245 [==============================] - 45s 185ms/step - loss: 132.2463 - mae: 9.1653 - val_loss: 603.2892 - val_mae: 11.9831\n",
      "Epoch 162/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 117.5735 - mae: 8.6495 - val_loss: 360.1765 - val_mae: 10.0609\n",
      "Epoch 163/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 118.6098 - mae: 8.6863 - val_loss: 385.5728 - val_mae: 10.6439\n",
      "Epoch 164/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 120.7765 - mae: 8.7538 - val_loss: 1958.6591 - val_mae: 14.5634\n",
      "Epoch 165/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 113.8925 - mae: 8.3969 - val_loss: 638.4183 - val_mae: 10.6134\n",
      "Epoch 166/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 109.1815 - mae: 8.2890 - val_loss: 588.7644 - val_mae: 10.8867\n",
      "Epoch 167/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.7431 - mae: 8.5133 - val_loss: 491.5131 - val_mae: 10.5692\n",
      "Epoch 168/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 115.5780 - mae: 8.5081 - val_loss: 1916.0294 - val_mae: 15.4737\n",
      "Epoch 169/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 105.5736 - mae: 8.1783 - val_loss: 2424.4937 - val_mae: 14.5023\n",
      "Epoch 170/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 112.9096 - mae: 8.4499 - val_loss: 1244.0668 - val_mae: 12.2731\n",
      "Epoch 171/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 121.3025 - mae: 8.7544 - val_loss: 1961.1294 - val_mae: 12.6616\n",
      "Epoch 172/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.6729 - mae: 8.4321 - val_loss: 1400.8525 - val_mae: 11.9500\n",
      "Epoch 173/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 117.4945 - mae: 8.6200 - val_loss: 2566.4963 - val_mae: 15.8642\n",
      "Epoch 174/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.8124 - mae: 8.4893 - val_loss: 875.1612 - val_mae: 11.6728\n",
      "Epoch 175/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 112.9529 - mae: 8.3516 - val_loss: 1748.2316 - val_mae: 14.7842\n",
      "Epoch 176/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 119.0087 - mae: 8.5971 - val_loss: 3134.0457 - val_mae: 17.8253\n",
      "Epoch 177/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 120.0797 - mae: 8.6796 - val_loss: 507.4505 - val_mae: 10.6379\n",
      "Epoch 178/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 114.9364 - mae: 8.5500 - val_loss: 563.7816 - val_mae: 10.2998\n",
      "Epoch 179/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 116.2072 - mae: 8.6371 - val_loss: 2366.7959 - val_mae: 15.2296\n",
      "Epoch 180/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.5585 - mae: 8.4987 - val_loss: 1362.3435 - val_mae: 13.5161\n",
      "Epoch 181/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 111.0771 - mae: 8.3423 - val_loss: 2125.9443 - val_mae: 13.6766\n",
      "Epoch 182/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 120.2188 - mae: 8.6257 - val_loss: 1747.4374 - val_mae: 13.5242\n",
      "Epoch 183/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 121.7697 - mae: 8.7620 - val_loss: 1604.7262 - val_mae: 14.9923\n",
      "Epoch 184/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 118.5320 - mae: 8.6072 - val_loss: 1505.0129 - val_mae: 14.1632\n",
      "Epoch 185/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 116.1457 - mae: 8.5538 - val_loss: 1500.5771 - val_mae: 13.1004\n",
      "Epoch 186/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 119.8201 - mae: 8.6634 - val_loss: 4380.9385 - val_mae: 20.0670\n",
      "Epoch 187/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 119.1774 - mae: 8.6629 - val_loss: 1972.8431 - val_mae: 15.1296\n",
      "Epoch 188/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 111.8723 - mae: 8.4053 - val_loss: 777.9611 - val_mae: 12.0874\n",
      "Epoch 189/650\n",
      "245/245 [==============================] - 51s 207ms/step - loss: 109.1692 - mae: 8.3853 - val_loss: 774.7330 - val_mae: 12.3834\n",
      "Epoch 190/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.8369 - mae: 8.5099 - val_loss: 1257.1158 - val_mae: 13.9226\n",
      "Epoch 191/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 125.7345 - mae: 8.9045 - val_loss: 1099.3613 - val_mae: 14.8483\n",
      "Epoch 192/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 112.0215 - mae: 8.4915 - val_loss: 296.6577 - val_mae: 10.2955\n",
      "Epoch 193/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 119.7508 - mae: 8.6620 - val_loss: 1267.9911 - val_mae: 14.0119\n",
      "Epoch 194/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 114.1255 - mae: 8.4841 - val_loss: 347.5460 - val_mae: 10.5343\n",
      "Epoch 195/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 109.9182 - mae: 8.3082 - val_loss: 288.9408 - val_mae: 9.6703\n",
      "Epoch 196/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 110.6443 - mae: 8.3615 - val_loss: 763.4438 - val_mae: 12.7989\n",
      "Epoch 197/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 108.0131 - mae: 8.3016 - val_loss: 274.3878 - val_mae: 9.7490\n",
      "Epoch 198/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 118.0787 - mae: 8.5584 - val_loss: 932.9320 - val_mae: 12.3518\n",
      "Epoch 199/650\n",
      "245/245 [==============================] - 48s 196ms/step - loss: 118.1038 - mae: 8.6478 - val_loss: 884.7009 - val_mae: 12.7085\n",
      "Epoch 200/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 119.9141 - mae: 8.7088 - val_loss: 288.3100 - val_mae: 9.9017\n",
      "Epoch 201/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 109.6490 - mae: 8.2512 - val_loss: 4745.8813 - val_mae: 20.5816\n",
      "Epoch 202/650\n",
      "245/245 [==============================] - 48s 196ms/step - loss: 119.5142 - mae: 8.6119 - val_loss: 692.9344 - val_mae: 10.8502\n",
      "Epoch 203/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 121.4679 - mae: 8.7114 - val_loss: 4519.7896 - val_mae: 20.0710\n",
      "Epoch 204/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 116.1661 - mae: 8.4983 - val_loss: 2010.3571 - val_mae: 14.0876\n",
      "Epoch 205/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 115.2146 - mae: 8.5100 - val_loss: 1535.1429 - val_mae: 13.5499\n",
      "Epoch 206/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.4193 - mae: 8.4699 - val_loss: 1232.5703 - val_mae: 12.5653\n",
      "Epoch 207/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 110.0606 - mae: 8.3024 - val_loss: 1117.7141 - val_mae: 12.3890\n",
      "Epoch 208/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 114.6655 - mae: 8.5534 - val_loss: 794.2935 - val_mae: 12.9109\n",
      "Epoch 209/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 113.9200 - mae: 8.4704 - val_loss: 473.8420 - val_mae: 10.9756\n",
      "Epoch 210/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 115.0257 - mae: 8.5789 - val_loss: 450.4811 - val_mae: 11.1842\n",
      "Epoch 211/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 108.9595 - mae: 8.4202 - val_loss: 226.6245 - val_mae: 9.0364\n",
      "Epoch 212/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 116.1091 - mae: 8.5687 - val_loss: 165.6821 - val_mae: 8.8179\n",
      "Epoch 213/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 115.4736 - mae: 8.4518 - val_loss: 394.4663 - val_mae: 10.7955\n",
      "Epoch 214/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 111.8051 - mae: 8.3686 - val_loss: 452.3540 - val_mae: 10.6694\n",
      "Epoch 215/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 121.6863 - mae: 8.8459 - val_loss: 913.2695 - val_mae: 12.8967\n",
      "Epoch 216/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 112.9580 - mae: 8.4405 - val_loss: 901.1387 - val_mae: 13.6839\n",
      "Epoch 217/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 115.3675 - mae: 8.6082 - val_loss: 644.4928 - val_mae: 11.8456\n",
      "Epoch 218/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 112.2270 - mae: 8.3198 - val_loss: 592.7368 - val_mae: 11.4840\n",
      "Epoch 219/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 116.7046 - mae: 8.6350 - val_loss: 377.6972 - val_mae: 10.3841\n",
      "Epoch 220/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 119.3845 - mae: 8.6536 - val_loss: 645.4896 - val_mae: 11.5730\n",
      "Epoch 221/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.2737 - mae: 8.5004 - val_loss: 521.9962 - val_mae: 10.9947\n",
      "Epoch 222/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 115.2080 - mae: 8.4567 - val_loss: 896.4873 - val_mae: 12.0606\n",
      "Epoch 223/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 112.6439 - mae: 8.4730 - val_loss: 904.6699 - val_mae: 13.1380\n",
      "Epoch 224/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 116.9297 - mae: 8.5495 - val_loss: 1467.2869 - val_mae: 14.0126\n",
      "Epoch 225/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 114.3417 - mae: 8.4717 - val_loss: 4759.9858 - val_mae: 19.2978\n",
      "Epoch 226/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 117.4949 - mae: 8.5961 - val_loss: 1160.2986 - val_mae: 13.9424\n",
      "Epoch 227/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 115.3261 - mae: 8.5406 - val_loss: 6528.0566 - val_mae: 25.5270\n",
      "Epoch 228/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.8808 - mae: 8.4557 - val_loss: 2238.4956 - val_mae: 16.2563\n",
      "Epoch 229/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 111.3186 - mae: 8.2916 - val_loss: 1430.0112 - val_mae: 13.3012\n",
      "Epoch 230/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.9784 - mae: 8.4124 - val_loss: 708.7733 - val_mae: 11.6733\n",
      "Epoch 231/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 113.5715 - mae: 8.4643 - val_loss: 601.0693 - val_mae: 12.0532\n",
      "Epoch 232/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 110.1127 - mae: 8.4435 - val_loss: 1324.3674 - val_mae: 13.5741\n",
      "Epoch 233/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 109.9231 - mae: 8.2801 - val_loss: 516.1231 - val_mae: 10.6413\n",
      "Epoch 234/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 114.3595 - mae: 8.4086 - val_loss: 1877.0548 - val_mae: 17.1309\n",
      "Epoch 235/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.1115 - mae: 8.5081 - val_loss: 1439.7240 - val_mae: 13.7328\n",
      "Epoch 236/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.9822 - mae: 8.4068 - val_loss: 1309.2222 - val_mae: 14.1171\n",
      "Epoch 237/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 113.8544 - mae: 8.3955 - val_loss: 1636.6156 - val_mae: 14.5638\n",
      "Epoch 238/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 116.9339 - mae: 8.5381 - val_loss: 470.6314 - val_mae: 11.1298\n",
      "Epoch 239/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 113.3086 - mae: 8.3807 - val_loss: 748.2800 - val_mae: 11.8124\n",
      "Epoch 240/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 106.1068 - mae: 8.1031 - val_loss: 584.6359 - val_mae: 10.8759\n",
      "Epoch 241/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 105.7324 - mae: 8.1632 - val_loss: 1684.6301 - val_mae: 16.7472\n",
      "Epoch 242/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 112.9032 - mae: 8.4708 - val_loss: 364.2850 - val_mae: 9.8068\n",
      "Epoch 243/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.0784 - mae: 8.3297 - val_loss: 443.9542 - val_mae: 10.6968\n",
      "Epoch 244/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 106.8756 - mae: 8.2193 - val_loss: 1185.8566 - val_mae: 14.3479\n",
      "Epoch 245/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 113.8863 - mae: 8.5012 - val_loss: 445.9010 - val_mae: 11.2769\n",
      "Epoch 246/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 110.5852 - mae: 8.3558 - val_loss: 297.2217 - val_mae: 10.4600\n",
      "Epoch 247/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 116.3058 - mae: 8.5556 - val_loss: 659.0204 - val_mae: 12.5188\n",
      "Epoch 248/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 112.8218 - mae: 8.4157 - val_loss: 225.6525 - val_mae: 9.3281\n",
      "Epoch 249/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 105.1656 - mae: 8.1462 - val_loss: 565.4252 - val_mae: 11.9956\n",
      "Epoch 250/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 113.8063 - mae: 8.4743 - val_loss: 1204.0504 - val_mae: 13.2097\n",
      "Epoch 251/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 112.2533 - mae: 8.3254 - val_loss: 1061.9712 - val_mae: 13.1247\n",
      "Epoch 252/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.8073 - mae: 8.4955 - val_loss: 1140.4393 - val_mae: 13.9488\n",
      "Epoch 253/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.7828 - mae: 8.5378 - val_loss: 1340.6624 - val_mae: 12.5165\n",
      "Epoch 254/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 113.7225 - mae: 8.4475 - val_loss: 3099.0803 - val_mae: 17.5729\n",
      "Epoch 255/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 119.9107 - mae: 8.6861 - val_loss: 70366.1172 - val_mae: 66.8963\n",
      "Epoch 256/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 117.8499 - mae: 8.6330 - val_loss: 40859.5000 - val_mae: 68.6319\n",
      "Epoch 257/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 117.4276 - mae: 8.6405 - val_loss: 2162.9978 - val_mae: 14.3270\n",
      "Epoch 258/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 108.8602 - mae: 8.3511 - val_loss: 2534.2561 - val_mae: 14.8179\n",
      "Epoch 259/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 117.1989 - mae: 8.5537 - val_loss: 1540.5294 - val_mae: 13.0302\n",
      "Epoch 260/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 107.8452 - mae: 8.2107 - val_loss: 3347.2644 - val_mae: 19.7328\n",
      "Epoch 261/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 108.4706 - mae: 8.2366 - val_loss: 1829.3414 - val_mae: 15.1667\n",
      "Epoch 262/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 113.3139 - mae: 8.4870 - val_loss: 1855.2810 - val_mae: 15.0718\n",
      "Epoch 263/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.9205 - mae: 8.2716 - val_loss: 1071.4917 - val_mae: 13.4376\n",
      "Epoch 264/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 107.7084 - mae: 8.2126 - val_loss: 3421.1028 - val_mae: 19.3952\n",
      "Epoch 265/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 107.7427 - mae: 8.2333 - val_loss: 1498.4160 - val_mae: 13.8930\n",
      "Epoch 266/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.1519 - mae: 8.3346 - val_loss: 746.6116 - val_mae: 11.3555\n",
      "Epoch 267/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 109.1857 - mae: 8.3110 - val_loss: 565.0430 - val_mae: 11.5264\n",
      "Epoch 268/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.2758 - mae: 8.3949 - val_loss: 2687.7246 - val_mae: 20.3166\n",
      "Epoch 269/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 123.6917 - mae: 8.8452 - val_loss: 4008.4099 - val_mae: 19.8883\n",
      "Epoch 270/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 116.1690 - mae: 8.4750 - val_loss: 2110.6321 - val_mae: 17.3231\n",
      "Epoch 271/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 114.1704 - mae: 8.4088 - val_loss: 1428.5804 - val_mae: 14.2413\n",
      "Epoch 272/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.6804 - mae: 8.5292 - val_loss: 2526.0962 - val_mae: 17.5371\n",
      "Epoch 273/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 112.7789 - mae: 8.3908 - val_loss: 1445.2216 - val_mae: 14.1880\n",
      "Epoch 274/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 112.0827 - mae: 8.3170 - val_loss: 4334.6543 - val_mae: 20.1598\n",
      "Epoch 275/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 110.6103 - mae: 8.3183 - val_loss: 1623.6836 - val_mae: 14.4064\n",
      "Epoch 276/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 115.8008 - mae: 8.5307 - val_loss: 1241.5300 - val_mae: 14.1887\n",
      "Epoch 277/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 110.2338 - mae: 8.3145 - val_loss: 1692.6676 - val_mae: 15.8468\n",
      "Epoch 278/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.0139 - mae: 8.2927 - val_loss: 1343.5516 - val_mae: 13.8430\n",
      "Epoch 279/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 106.8337 - mae: 8.2075 - val_loss: 2621.8232 - val_mae: 16.7171\n",
      "Epoch 280/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 118.3826 - mae: 8.6378 - val_loss: 554.7508 - val_mae: 11.6021\n",
      "Epoch 281/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 121.6382 - mae: 8.7206 - val_loss: 1213.6661 - val_mae: 13.1715\n",
      "Epoch 282/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 118.2625 - mae: 8.6062 - val_loss: 1512.0413 - val_mae: 14.4056\n",
      "Epoch 283/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 109.4693 - mae: 8.3511 - val_loss: 1863.1117 - val_mae: 15.4434\n",
      "Epoch 284/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.9743 - mae: 8.4450 - val_loss: 658.2134 - val_mae: 11.4259\n",
      "Epoch 285/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 121.4844 - mae: 8.8053 - val_loss: 1879.2837 - val_mae: 14.9634\n",
      "Epoch 286/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.9751 - mae: 8.4574 - val_loss: 1470.9630 - val_mae: 14.0614\n",
      "Epoch 287/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 113.5099 - mae: 8.5190 - val_loss: 998.3615 - val_mae: 12.8285\n",
      "Epoch 288/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 108.6757 - mae: 8.3828 - val_loss: 1240.8077 - val_mae: 14.0751\n",
      "Epoch 289/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 101.9988 - mae: 7.9398 - val_loss: 1679.7806 - val_mae: 14.1866\n",
      "Epoch 290/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 108.4858 - mae: 8.2880 - val_loss: 2978.3269 - val_mae: 16.0096\n",
      "Epoch 291/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 111.9142 - mae: 8.4071 - val_loss: 1528.6920 - val_mae: 12.9401\n",
      "Epoch 292/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 104.9156 - mae: 8.1792 - val_loss: 3962.1812 - val_mae: 18.1102\n",
      "Epoch 293/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 109.9419 - mae: 8.4201 - val_loss: 1465.3054 - val_mae: 13.6431\n",
      "Epoch 294/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 110.5942 - mae: 8.3157 - val_loss: 773.4174 - val_mae: 11.9500\n",
      "Epoch 295/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.1784 - mae: 8.4230 - val_loss: 5538.1450 - val_mae: 20.1405\n",
      "Epoch 296/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 113.4037 - mae: 8.4379 - val_loss: 2691.4954 - val_mae: 15.6548\n",
      "Epoch 297/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 117.0516 - mae: 8.5006 - val_loss: 1413.4628 - val_mae: 12.1455\n",
      "Epoch 298/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 114.1283 - mae: 8.4042 - val_loss: 4221.8560 - val_mae: 17.6934\n",
      "Epoch 299/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 111.7346 - mae: 8.4625 - val_loss: 2831.2207 - val_mae: 17.1099\n",
      "Epoch 300/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 112.1416 - mae: 8.4589 - val_loss: 901.9276 - val_mae: 12.7319\n",
      "Epoch 301/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 113.9133 - mae: 8.5641 - val_loss: 632.9294 - val_mae: 11.8095\n",
      "Epoch 302/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.1039 - mae: 8.2364 - val_loss: 526.2639 - val_mae: 11.0387\n",
      "Epoch 303/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 107.9708 - mae: 8.3594 - val_loss: 828.7125 - val_mae: 11.7852\n",
      "Epoch 304/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 115.7396 - mae: 8.5756 - val_loss: 1239.9059 - val_mae: 13.0458\n",
      "Epoch 305/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 108.7765 - mae: 8.3103 - val_loss: 2921.8079 - val_mae: 17.5571\n",
      "Epoch 306/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 115.3335 - mae: 8.5132 - val_loss: 483.2576 - val_mae: 10.6275\n",
      "Epoch 307/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 113.2394 - mae: 8.4824 - val_loss: 1036.5088 - val_mae: 13.3982\n",
      "Epoch 308/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.9100 - mae: 8.3628 - val_loss: 244.6427 - val_mae: 9.4557\n",
      "Epoch 309/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 108.8760 - mae: 8.2527 - val_loss: 230.0034 - val_mae: 9.6532\n",
      "Epoch 310/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 109.2194 - mae: 8.2930 - val_loss: 359.4428 - val_mae: 10.4474\n",
      "Epoch 311/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.0708 - mae: 8.2885 - val_loss: 470.9583 - val_mae: 11.1374\n",
      "Epoch 312/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 107.1906 - mae: 8.1437 - val_loss: 866.9755 - val_mae: 12.3251\n",
      "Epoch 313/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.6169 - mae: 8.3326 - val_loss: 710.0070 - val_mae: 11.5249\n",
      "Epoch 314/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.3126 - mae: 8.4968 - val_loss: 1302.9603 - val_mae: 12.5417\n",
      "Epoch 315/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 115.2269 - mae: 8.4881 - val_loss: 3344.0117 - val_mae: 17.8951\n",
      "Epoch 316/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 115.1365 - mae: 8.5849 - val_loss: 2077.3953 - val_mae: 14.3124\n",
      "Epoch 317/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 111.3449 - mae: 8.3638 - val_loss: 3658.2339 - val_mae: 17.8106\n",
      "Epoch 318/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 108.4458 - mae: 8.3299 - val_loss: 1564.8185 - val_mae: 14.2046\n",
      "Epoch 319/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.6394 - mae: 8.3736 - val_loss: 750.6738 - val_mae: 10.9005\n",
      "Epoch 320/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 116.3035 - mae: 8.4966 - val_loss: 3526.8440 - val_mae: 15.8976\n",
      "Epoch 321/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 108.7910 - mae: 8.3212 - val_loss: 2632.5796 - val_mae: 16.0144\n",
      "Epoch 322/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.1387 - mae: 8.3983 - val_loss: 4325.8999 - val_mae: 20.7619\n",
      "Epoch 323/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 108.4570 - mae: 8.3180 - val_loss: 1007.9322 - val_mae: 11.9669\n",
      "Epoch 324/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 118.0357 - mae: 8.6460 - val_loss: 2048.8386 - val_mae: 15.3431\n",
      "Epoch 325/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 102.8973 - mae: 8.0752 - val_loss: 1574.4447 - val_mae: 14.9074\n",
      "Epoch 326/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.6789 - mae: 8.3822 - val_loss: 1391.1559 - val_mae: 13.9102\n",
      "Epoch 327/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 115.0842 - mae: 8.5412 - val_loss: 2059.4465 - val_mae: 17.7237\n",
      "Epoch 328/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 108.5476 - mae: 8.2577 - val_loss: 434.2459 - val_mae: 10.5327\n",
      "Epoch 329/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 105.4577 - mae: 8.1272 - val_loss: 3532.1140 - val_mae: 19.2876\n",
      "Epoch 330/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 107.9123 - mae: 8.3001 - val_loss: 1369.9685 - val_mae: 14.8924\n",
      "Epoch 331/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 113.3147 - mae: 8.4823 - val_loss: 1136.9941 - val_mae: 13.1711\n",
      "Epoch 332/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 106.4619 - mae: 8.1945 - val_loss: 669.0480 - val_mae: 11.2437\n",
      "Epoch 333/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 105.3629 - mae: 8.1574 - val_loss: 1038.9501 - val_mae: 13.3197\n",
      "Epoch 334/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 110.2075 - mae: 8.2841 - val_loss: 2660.0176 - val_mae: 17.3762\n",
      "Epoch 335/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.6471 - mae: 8.3384 - val_loss: 3209.5303 - val_mae: 18.2682\n",
      "Epoch 336/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 111.9235 - mae: 8.3874 - val_loss: 2582.1145 - val_mae: 19.6762\n",
      "Epoch 337/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 114.0688 - mae: 8.5057 - val_loss: 1298.2361 - val_mae: 13.8554\n",
      "Epoch 338/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.0090 - mae: 8.4696 - val_loss: 2910.4792 - val_mae: 15.9844\n",
      "Epoch 339/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.9101 - mae: 8.3139 - val_loss: 4882.3921 - val_mae: 20.5403\n",
      "Epoch 340/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.3504 - mae: 8.2329 - val_loss: 4702.4814 - val_mae: 19.3111\n",
      "Epoch 341/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 114.2221 - mae: 8.5258 - val_loss: 1666.6785 - val_mae: 13.9734\n",
      "Epoch 342/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 108.6289 - mae: 8.3133 - val_loss: 3100.7761 - val_mae: 17.9364\n",
      "Epoch 343/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 107.5427 - mae: 8.2205 - val_loss: 2909.0688 - val_mae: 16.5706\n",
      "Epoch 344/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 105.9569 - mae: 8.1038 - val_loss: 181.2236 - val_mae: 8.7157\n",
      "Epoch 345/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 113.3245 - mae: 8.5105 - val_loss: 1477.9895 - val_mae: 13.7032\n",
      "Epoch 346/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 116.0917 - mae: 8.5616 - val_loss: 2008.7350 - val_mae: 14.6690\n",
      "Epoch 347/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 109.9144 - mae: 8.3708 - val_loss: 1436.7159 - val_mae: 13.5425\n",
      "Epoch 348/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 120.6899 - mae: 8.7777 - val_loss: 3296.9861 - val_mae: 16.2898\n",
      "Epoch 349/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 116.1722 - mae: 8.5732 - val_loss: 1687.9064 - val_mae: 13.7819\n",
      "Epoch 350/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 110.3171 - mae: 8.3565 - val_loss: 1098.0887 - val_mae: 12.8390\n",
      "Epoch 351/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 115.0850 - mae: 8.5115 - val_loss: 1492.0519 - val_mae: 14.0799\n",
      "Epoch 352/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 111.6688 - mae: 8.3351 - val_loss: 766.3989 - val_mae: 11.9171\n",
      "Epoch 353/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 113.1670 - mae: 8.4669 - val_loss: 712.8029 - val_mae: 11.4466\n",
      "Epoch 354/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 115.9038 - mae: 8.6001 - val_loss: 475.5283 - val_mae: 11.3652\n",
      "Epoch 355/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 106.9346 - mae: 8.2124 - val_loss: 574.6669 - val_mae: 10.6619\n",
      "Epoch 356/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 119.9316 - mae: 8.6670 - val_loss: 1625.7584 - val_mae: 13.3917\n",
      "Epoch 357/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 106.2218 - mae: 8.2350 - val_loss: 1123.1595 - val_mae: 12.0470\n",
      "Epoch 358/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 112.2592 - mae: 8.4335 - val_loss: 923.1639 - val_mae: 10.6922\n",
      "Epoch 359/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 108.9777 - mae: 8.3463 - val_loss: 1242.7070 - val_mae: 11.9307\n",
      "Epoch 360/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.5095 - mae: 8.2398 - val_loss: 3664.3328 - val_mae: 15.9330\n",
      "Epoch 361/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 109.4954 - mae: 8.2611 - val_loss: 3106.9766 - val_mae: 16.0791\n",
      "Epoch 362/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.1354 - mae: 8.4898 - val_loss: 8591.7451 - val_mae: 20.5341\n",
      "Epoch 363/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 117.3531 - mae: 8.6043 - val_loss: 5212.1094 - val_mae: 20.2441\n",
      "Epoch 364/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 112.8236 - mae: 8.4749 - val_loss: 1701.3054 - val_mae: 12.1161\n",
      "Epoch 365/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 115.0754 - mae: 8.4692 - val_loss: 5701.4009 - val_mae: 19.2777\n",
      "Epoch 366/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 112.9099 - mae: 8.4218 - val_loss: 5647.0400 - val_mae: 22.3651\n",
      "Epoch 367/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.6121 - mae: 8.4832 - val_loss: 32631.1133 - val_mae: 35.1514\n",
      "Epoch 368/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 115.9361 - mae: 8.4676 - val_loss: 3845.0640 - val_mae: 17.0617\n",
      "Epoch 369/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 111.3002 - mae: 8.4121 - val_loss: 1142.4331 - val_mae: 11.7159\n",
      "Epoch 370/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 111.1630 - mae: 8.4666 - val_loss: 3652.6582 - val_mae: 15.5544\n",
      "Epoch 371/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 114.4850 - mae: 8.4734 - val_loss: 1727.6174 - val_mae: 13.4403\n",
      "Epoch 372/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.9557 - mae: 8.2807 - val_loss: 1776.1227 - val_mae: 12.5563\n",
      "Epoch 373/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 107.8175 - mae: 8.2172 - val_loss: 1133.9092 - val_mae: 13.6412\n",
      "Epoch 374/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 115.5683 - mae: 8.6291 - val_loss: 1744.7430 - val_mae: 14.6973\n",
      "Epoch 375/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 111.7217 - mae: 8.4160 - val_loss: 6280.2705 - val_mae: 19.6711\n",
      "Epoch 376/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 102.6891 - mae: 8.0185 - val_loss: 11511.0225 - val_mae: 21.2140\n",
      "Epoch 377/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 116.4933 - mae: 8.5875 - val_loss: 3693.6475 - val_mae: 14.6236\n",
      "Epoch 378/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 110.1336 - mae: 8.3166 - val_loss: 12700.8193 - val_mae: 24.4802\n",
      "Epoch 379/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 116.8606 - mae: 8.6310 - val_loss: 8516.2861 - val_mae: 21.6286\n",
      "Epoch 380/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 103.9554 - mae: 8.0663 - val_loss: 2257.1677 - val_mae: 16.3187\n",
      "Epoch 381/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 108.3897 - mae: 8.2730 - val_loss: 7824.9814 - val_mae: 24.4019\n",
      "Epoch 382/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 117.8812 - mae: 8.6294 - val_loss: 3908.5322 - val_mae: 17.2469\n",
      "Epoch 383/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 113.7966 - mae: 8.5020 - val_loss: 4668.4653 - val_mae: 18.1510\n",
      "Epoch 384/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 112.5402 - mae: 8.4790 - val_loss: 3168.6228 - val_mae: 16.1827\n",
      "Epoch 385/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.1738 - mae: 8.3403 - val_loss: 1277.4094 - val_mae: 12.6753\n",
      "Epoch 386/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 111.6388 - mae: 8.3595 - val_loss: 2313.1567 - val_mae: 14.2756\n",
      "Epoch 387/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 114.2001 - mae: 8.4195 - val_loss: 1039.7411 - val_mae: 12.6302\n",
      "Epoch 388/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 117.1760 - mae: 8.5684 - val_loss: 3547.8037 - val_mae: 17.2103\n",
      "Epoch 389/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 108.3853 - mae: 8.2179 - val_loss: 1266.7295 - val_mae: 13.8155\n",
      "Epoch 390/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 112.1124 - mae: 8.3299 - val_loss: 2166.0186 - val_mae: 14.4972\n",
      "Epoch 391/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 106.7615 - mae: 8.1448 - val_loss: 6515.3711 - val_mae: 19.7485\n",
      "Epoch 392/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 111.0732 - mae: 8.4660 - val_loss: 9792.5361 - val_mae: 21.8536\n",
      "Epoch 393/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 107.1550 - mae: 8.0891 - val_loss: 2600.4497 - val_mae: 15.9611\n",
      "Epoch 394/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.7792 - mae: 8.3455 - val_loss: 1805.1365 - val_mae: 14.9649\n",
      "Epoch 395/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 107.5173 - mae: 8.2458 - val_loss: 1056.1732 - val_mae: 12.5874\n",
      "Epoch 396/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.7825 - mae: 8.4949 - val_loss: 385.4098 - val_mae: 10.5302\n",
      "Epoch 397/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.6632 - mae: 8.4186 - val_loss: 821.3347 - val_mae: 11.6504\n",
      "Epoch 398/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 106.4276 - mae: 8.2300 - val_loss: 2727.7883 - val_mae: 13.4461\n",
      "Epoch 399/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.8435 - mae: 8.2879 - val_loss: 2479.5337 - val_mae: 14.4023\n",
      "Epoch 400/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 113.9563 - mae: 8.4999 - val_loss: 792.1312 - val_mae: 10.9764\n",
      "Epoch 401/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 113.8097 - mae: 8.4158 - val_loss: 2076.1179 - val_mae: 13.8668\n",
      "Epoch 402/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 108.0108 - mae: 8.2669 - val_loss: 2835.6409 - val_mae: 17.1310\n",
      "Epoch 403/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 108.8489 - mae: 8.3266 - val_loss: 3006.7463 - val_mae: 14.4491\n",
      "Epoch 404/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 110.7451 - mae: 8.3019 - val_loss: 12019.9121 - val_mae: 19.4438\n",
      "Epoch 405/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 108.3722 - mae: 8.1059 - val_loss: 44368.0859 - val_mae: 35.0840\n",
      "Epoch 406/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 107.8758 - mae: 8.1920 - val_loss: 17745.8164 - val_mae: 24.0213\n",
      "Epoch 407/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 102.1011 - mae: 7.9946 - val_loss: 65153.8164 - val_mae: 33.2309\n",
      "Epoch 408/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 108.6809 - mae: 8.3755 - val_loss: 24996.9766 - val_mae: 25.6830\n",
      "Epoch 409/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 100.8717 - mae: 7.9360 - val_loss: 11510.1289 - val_mae: 20.8734\n",
      "Epoch 410/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.2703 - mae: 8.3750 - val_loss: 12662.6602 - val_mae: 22.1367\n",
      "Epoch 411/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 110.1164 - mae: 8.3819 - val_loss: 2836.1404 - val_mae: 14.7501\n",
      "Epoch 412/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 112.3741 - mae: 8.4434 - val_loss: 1525.9629 - val_mae: 12.3998\n",
      "Epoch 413/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.5277 - mae: 8.3477 - val_loss: 1787.1166 - val_mae: 13.1914\n",
      "Epoch 414/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 109.9457 - mae: 8.3333 - val_loss: 6208.2998 - val_mae: 22.3355\n",
      "Epoch 415/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.4999 - mae: 8.2742 - val_loss: 4105.5186 - val_mae: 16.8467\n",
      "Epoch 416/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 106.4740 - mae: 8.2210 - val_loss: 1555.7574 - val_mae: 12.5421\n",
      "Epoch 417/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 103.0441 - mae: 8.0513 - val_loss: 2120.7869 - val_mae: 11.9068\n",
      "Epoch 418/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 110.5668 - mae: 8.3573 - val_loss: 16919.1152 - val_mae: 25.5537\n",
      "Epoch 419/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 110.1414 - mae: 8.3428 - val_loss: 12216.3301 - val_mae: 22.7373\n",
      "Epoch 420/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.1531 - mae: 8.4945 - val_loss: 8683.3135 - val_mae: 21.1608\n",
      "Epoch 421/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 112.9509 - mae: 8.4394 - val_loss: 6485.9639 - val_mae: 17.8318\n",
      "Epoch 422/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.4780 - mae: 8.4585 - val_loss: 3296.8345 - val_mae: 15.8302\n",
      "Epoch 423/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 111.5226 - mae: 8.4060 - val_loss: 1817.0853 - val_mae: 12.9437\n",
      "Epoch 424/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 107.5760 - mae: 8.1426 - val_loss: 2984.2822 - val_mae: 16.5222\n",
      "Epoch 425/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 116.9874 - mae: 8.7237 - val_loss: 4933.0039 - val_mae: 17.2341\n",
      "Epoch 426/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 114.9729 - mae: 8.5008 - val_loss: 5109.5347 - val_mae: 17.6114\n",
      "Epoch 427/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 110.3852 - mae: 8.3516 - val_loss: 5549.2671 - val_mae: 18.2682\n",
      "Epoch 428/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 115.9114 - mae: 8.6014 - val_loss: 5153.6616 - val_mae: 17.2319\n",
      "Epoch 429/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 109.0540 - mae: 8.2725 - val_loss: 2513.0710 - val_mae: 16.3742\n",
      "Epoch 430/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.9983 - mae: 8.5232 - val_loss: 1474.9624 - val_mae: 13.7526\n",
      "Epoch 431/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 110.5571 - mae: 8.3956 - val_loss: 1599.6477 - val_mae: 14.3168\n",
      "Epoch 432/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 111.3731 - mae: 8.4031 - val_loss: 1845.8944 - val_mae: 13.1316\n",
      "Epoch 433/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 108.5441 - mae: 8.3362 - val_loss: 1879.6486 - val_mae: 14.0432\n",
      "Epoch 434/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 107.3345 - mae: 8.3702 - val_loss: 2172.5916 - val_mae: 14.9715\n",
      "Epoch 435/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 107.6042 - mae: 8.3310 - val_loss: 712.3589 - val_mae: 10.5808\n",
      "Epoch 436/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 118.6572 - mae: 8.6990 - val_loss: 1065.7150 - val_mae: 12.2958\n",
      "Epoch 437/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 106.4354 - mae: 8.1917 - val_loss: 1404.4235 - val_mae: 13.5052\n",
      "Epoch 438/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 112.0696 - mae: 8.4447 - val_loss: 661.5035 - val_mae: 11.2317\n",
      "Epoch 439/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 115.7112 - mae: 8.5392 - val_loss: 2096.0200 - val_mae: 14.1327\n",
      "Epoch 440/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 108.7655 - mae: 8.2783 - val_loss: 2465.6531 - val_mae: 14.3900\n",
      "Epoch 441/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 110.0458 - mae: 8.2328 - val_loss: 3833.7100 - val_mae: 17.2807\n",
      "Epoch 442/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 105.5278 - mae: 8.2590 - val_loss: 4586.8403 - val_mae: 17.6204\n",
      "Epoch 443/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 102.9473 - mae: 7.9772 - val_loss: 3442.4448 - val_mae: 16.4566\n",
      "Epoch 444/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 115.6015 - mae: 8.4995 - val_loss: 3691.4167 - val_mae: 16.5144\n",
      "Epoch 445/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 103.3247 - mae: 8.0519 - val_loss: 2919.5283 - val_mae: 14.5978\n",
      "Epoch 446/650\n",
      "245/245 [==============================] - 50s 205ms/step - loss: 109.7711 - mae: 8.3196 - val_loss: 5581.7627 - val_mae: 17.5653\n",
      "Epoch 447/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 107.4219 - mae: 8.2685 - val_loss: 1014.8179 - val_mae: 11.7922\n",
      "Epoch 448/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 105.8983 - mae: 8.1771 - val_loss: 3710.1321 - val_mae: 15.8236\n",
      "Epoch 449/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 104.0689 - mae: 8.1769 - val_loss: 4389.5254 - val_mae: 16.4574\n",
      "Epoch 450/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 108.3082 - mae: 8.2615 - val_loss: 17904.9551 - val_mae: 29.2169\n",
      "Epoch 451/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 115.3607 - mae: 8.5545 - val_loss: 10328.0596 - val_mae: 23.4001\n",
      "Epoch 452/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 108.9011 - mae: 8.3596 - val_loss: 5963.6768 - val_mae: 19.2137\n",
      "Epoch 453/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 112.1902 - mae: 8.3910 - val_loss: 10712.0098 - val_mae: 22.5959\n",
      "Epoch 454/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 110.0761 - mae: 8.2939 - val_loss: 27494.5938 - val_mae: 32.5356\n",
      "Epoch 455/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.8494 - mae: 8.4455 - val_loss: 45555.3789 - val_mae: 35.5505\n",
      "Epoch 456/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 103.5109 - mae: 8.1144 - val_loss: 33828.8359 - val_mae: 31.1294\n",
      "Epoch 457/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 112.4763 - mae: 8.4397 - val_loss: 47961.7227 - val_mae: 32.1391\n",
      "Epoch 458/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.2594 - mae: 8.4224 - val_loss: 27630.9688 - val_mae: 24.3350\n",
      "Epoch 459/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 112.5415 - mae: 8.4641 - val_loss: 11363.5967 - val_mae: 20.5729\n",
      "Epoch 460/650\n",
      "245/245 [==============================] - 48s 196ms/step - loss: 109.0193 - mae: 8.2380 - val_loss: 16534.0488 - val_mae: 21.1283\n",
      "Epoch 461/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 117.7241 - mae: 8.5939 - val_loss: 26640.1914 - val_mae: 29.1354\n",
      "Epoch 462/650\n",
      "245/245 [==============================] - 48s 197ms/step - loss: 113.6435 - mae: 8.5229 - val_loss: 21212.8691 - val_mae: 25.5969\n",
      "Epoch 463/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 105.4731 - mae: 8.1702 - val_loss: 9559.4141 - val_mae: 20.3415\n",
      "Epoch 464/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 105.0216 - mae: 8.0943 - val_loss: 10578.6875 - val_mae: 22.5152\n",
      "Epoch 465/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 112.3617 - mae: 8.3395 - val_loss: 5656.8735 - val_mae: 17.2921\n",
      "Epoch 466/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 107.9290 - mae: 8.2626 - val_loss: 2231.5144 - val_mae: 14.6890\n",
      "Epoch 467/650\n",
      "245/245 [==============================] - 48s 196ms/step - loss: 103.0044 - mae: 8.1774 - val_loss: 3588.0916 - val_mae: 16.0142\n",
      "Epoch 468/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 115.8968 - mae: 8.5125 - val_loss: 3087.1675 - val_mae: 15.6410\n",
      "Epoch 469/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 113.9531 - mae: 8.4231 - val_loss: 8748.1992 - val_mae: 20.9763\n",
      "Epoch 470/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 105.7476 - mae: 8.1845 - val_loss: 11685.6621 - val_mae: 22.6588\n",
      "Epoch 471/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 103.4027 - mae: 8.1538 - val_loss: 15081.3506 - val_mae: 21.9784\n",
      "Epoch 472/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 111.4099 - mae: 8.3188 - val_loss: 3159.1719 - val_mae: 13.0529\n",
      "Epoch 473/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 108.1711 - mae: 8.2302 - val_loss: 7530.9819 - val_mae: 22.2489\n",
      "Epoch 474/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 107.1191 - mae: 8.1724 - val_loss: 4572.3623 - val_mae: 18.3123\n",
      "Epoch 475/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 102.9930 - mae: 8.1010 - val_loss: 1326.8916 - val_mae: 12.3929\n",
      "Epoch 476/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 109.7164 - mae: 8.3218 - val_loss: 4163.7803 - val_mae: 18.5834\n",
      "Epoch 477/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 109.5217 - mae: 8.2761 - val_loss: 5318.6001 - val_mae: 19.6573\n",
      "Epoch 478/650\n",
      "245/245 [==============================] - 45s 186ms/step - loss: 108.5082 - mae: 8.2664 - val_loss: 3453.3042 - val_mae: 16.0644\n",
      "Epoch 479/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 96.9841 - mae: 7.8311 - val_loss: 6822.2954 - val_mae: 20.6322\n",
      "Epoch 480/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 106.1983 - mae: 8.2117 - val_loss: 2452.0466 - val_mae: 15.4291\n",
      "Epoch 481/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 108.3813 - mae: 8.2602 - val_loss: 2137.2678 - val_mae: 14.0011\n",
      "Epoch 482/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 102.9450 - mae: 8.1175 - val_loss: 3556.5786 - val_mae: 18.3145\n",
      "Epoch 483/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.1230 - mae: 8.3231 - val_loss: 3111.2000 - val_mae: 17.6085\n",
      "Epoch 484/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 112.1841 - mae: 8.4535 - val_loss: 1492.4916 - val_mae: 13.6008\n",
      "Epoch 485/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 109.0286 - mae: 8.3613 - val_loss: 1665.8239 - val_mae: 15.1195\n",
      "Epoch 486/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 108.2531 - mae: 8.2316 - val_loss: 1134.6892 - val_mae: 13.1850\n",
      "Epoch 487/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 109.0404 - mae: 8.3010 - val_loss: 1086.1708 - val_mae: 13.2860\n",
      "Epoch 488/650\n",
      "245/245 [==============================] - 48s 196ms/step - loss: 112.0261 - mae: 8.4280 - val_loss: 2440.4634 - val_mae: 15.3526\n",
      "Epoch 489/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 108.4124 - mae: 8.3152 - val_loss: 4642.2251 - val_mae: 18.6268\n",
      "Epoch 490/650\n",
      "245/245 [==============================] - 48s 196ms/step - loss: 108.1367 - mae: 8.3002 - val_loss: 935.8537 - val_mae: 11.2472\n",
      "Epoch 491/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 103.9858 - mae: 8.0034 - val_loss: 4037.1882 - val_mae: 16.7972\n",
      "Epoch 492/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 109.9780 - mae: 8.3938 - val_loss: 1952.7987 - val_mae: 13.8594\n",
      "Epoch 493/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 105.6011 - mae: 8.1334 - val_loss: 1609.4646 - val_mae: 13.4326\n",
      "Epoch 494/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 104.4238 - mae: 8.0664 - val_loss: 5865.5688 - val_mae: 23.1363\n",
      "Epoch 495/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.9953 - mae: 8.3547 - val_loss: 1304.3069 - val_mae: 13.1139\n",
      "Epoch 496/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.6400 - mae: 8.5311 - val_loss: 861.1431 - val_mae: 12.6554\n",
      "Epoch 497/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.3114 - mae: 8.3692 - val_loss: 981.8286 - val_mae: 12.8990\n",
      "Epoch 498/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 115.5605 - mae: 8.5610 - val_loss: 1930.1637 - val_mae: 14.5766\n",
      "Epoch 499/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.5221 - mae: 8.5513 - val_loss: 927.7642 - val_mae: 13.1562\n",
      "Epoch 500/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 109.3491 - mae: 8.3362 - val_loss: 1293.9354 - val_mae: 13.3695\n",
      "Epoch 501/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 112.8830 - mae: 8.4339 - val_loss: 3096.1587 - val_mae: 16.0806\n",
      "Epoch 502/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 106.6773 - mae: 8.2568 - val_loss: 2446.9741 - val_mae: 15.8053\n",
      "Epoch 503/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 104.3189 - mae: 8.1937 - val_loss: 2350.0088 - val_mae: 16.2942\n",
      "Epoch 504/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.2961 - mae: 8.4208 - val_loss: 4659.0635 - val_mae: 19.2486\n",
      "Epoch 505/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.5470 - mae: 8.3544 - val_loss: 3897.8335 - val_mae: 18.3683\n",
      "Epoch 506/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 105.9761 - mae: 8.2537 - val_loss: 2028.5100 - val_mae: 15.2760\n",
      "Epoch 507/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.0277 - mae: 8.3203 - val_loss: 1596.4491 - val_mae: 14.1401\n",
      "Epoch 508/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.4339 - mae: 8.4135 - val_loss: 1487.9432 - val_mae: 13.7200\n",
      "Epoch 509/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 106.2678 - mae: 8.2362 - val_loss: 1811.3480 - val_mae: 14.0020\n",
      "Epoch 510/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 105.0667 - mae: 8.2208 - val_loss: 533.8770 - val_mae: 10.3278\n",
      "Epoch 511/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 110.6495 - mae: 8.3250 - val_loss: 2638.7280 - val_mae: 15.3127\n",
      "Epoch 512/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 112.0914 - mae: 8.4824 - val_loss: 3804.4207 - val_mae: 18.7445\n",
      "Epoch 513/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 108.5819 - mae: 8.3008 - val_loss: 2116.4392 - val_mae: 14.8633\n",
      "Epoch 514/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 107.2560 - mae: 8.2050 - val_loss: 3304.8523 - val_mae: 17.2011\n",
      "Epoch 515/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 103.8813 - mae: 8.0930 - val_loss: 2836.1042 - val_mae: 14.7175\n",
      "Epoch 516/650\n",
      "245/245 [==============================] - 48s 196ms/step - loss: 107.7626 - mae: 8.2047 - val_loss: 3110.0833 - val_mae: 16.2290\n",
      "Epoch 517/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 111.1131 - mae: 8.4566 - val_loss: 3560.9136 - val_mae: 16.1467\n",
      "Epoch 518/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.3316 - mae: 8.5177 - val_loss: 7542.8306 - val_mae: 20.5011\n",
      "Epoch 519/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 113.2747 - mae: 8.4586 - val_loss: 13398.0703 - val_mae: 24.5133\n",
      "Epoch 520/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 111.6473 - mae: 8.4106 - val_loss: 1063.0184 - val_mae: 12.6782\n",
      "Epoch 521/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 113.4139 - mae: 8.4655 - val_loss: 6205.4146 - val_mae: 20.6601\n",
      "Epoch 522/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 105.2719 - mae: 8.1522 - val_loss: 7953.4258 - val_mae: 19.0942\n",
      "Epoch 523/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.6814 - mae: 8.5533 - val_loss: 15059.9600 - val_mae: 24.5499\n",
      "Epoch 524/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 114.3678 - mae: 8.5844 - val_loss: 21617.1445 - val_mae: 29.2523\n",
      "Epoch 525/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.4203 - mae: 8.4119 - val_loss: 12523.7725 - val_mae: 22.9455\n",
      "Epoch 526/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 100.8743 - mae: 7.9780 - val_loss: 6363.2168 - val_mae: 18.4050\n",
      "Epoch 527/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 112.3382 - mae: 8.4147 - val_loss: 7397.7061 - val_mae: 19.8406\n",
      "Epoch 528/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 109.8195 - mae: 8.3549 - val_loss: 15355.3389 - val_mae: 23.6179\n",
      "Epoch 529/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.1426 - mae: 8.3962 - val_loss: 28883.5254 - val_mae: 33.3370\n",
      "Epoch 530/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 105.2911 - mae: 8.2153 - val_loss: 18595.1562 - val_mae: 23.6702\n",
      "Epoch 531/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 107.6795 - mae: 8.3114 - val_loss: 18054.8867 - val_mae: 25.2662\n",
      "Epoch 532/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 110.0329 - mae: 8.2410 - val_loss: 33665.4258 - val_mae: 34.7837\n",
      "Epoch 533/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.8743 - mae: 8.2937 - val_loss: 73149.3203 - val_mae: 42.9841\n",
      "Epoch 534/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 100.3862 - mae: 7.8589 - val_loss: 45340.9688 - val_mae: 35.1228\n",
      "Epoch 535/650\n",
      "245/245 [==============================] - 48s 195ms/step - loss: 106.3120 - mae: 8.2270 - val_loss: 98425.3828 - val_mae: 47.5085\n",
      "Epoch 536/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 111.9807 - mae: 8.3982 - val_loss: 38541.4141 - val_mae: 32.7384\n",
      "Epoch 537/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 113.9399 - mae: 8.5236 - val_loss: 170693.2656 - val_mae: 54.7435\n",
      "Epoch 538/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 107.1398 - mae: 8.1926 - val_loss: 8613.2734 - val_mae: 20.5827\n",
      "Epoch 539/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 117.2380 - mae: 8.6867 - val_loss: 10125.5918 - val_mae: 22.8926\n",
      "Epoch 540/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 113.4935 - mae: 8.4915 - val_loss: 37080.3008 - val_mae: 32.9585\n",
      "Epoch 541/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 112.3502 - mae: 8.5066 - val_loss: 27994.5039 - val_mae: 30.3985\n",
      "Epoch 542/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 107.8461 - mae: 8.2665 - val_loss: 22418.3555 - val_mae: 30.7076\n",
      "Epoch 543/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.1110 - mae: 8.3206 - val_loss: 16387.2715 - val_mae: 23.0428\n",
      "Epoch 544/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.7762 - mae: 8.3138 - val_loss: 55024.2305 - val_mae: 43.1635\n",
      "Epoch 545/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 102.1118 - mae: 8.0039 - val_loss: 6185.4399 - val_mae: 20.5408\n",
      "Epoch 546/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 113.5964 - mae: 8.4610 - val_loss: 5748.7837 - val_mae: 19.5390\n",
      "Epoch 547/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 108.9410 - mae: 8.1789 - val_loss: 5993.6299 - val_mae: 20.1221\n",
      "Epoch 548/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 114.0169 - mae: 8.4545 - val_loss: 2202.9868 - val_mae: 14.8757\n",
      "Epoch 549/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 111.4260 - mae: 8.3849 - val_loss: 4201.7871 - val_mae: 14.5626\n",
      "Epoch 550/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 117.2987 - mae: 8.6734 - val_loss: 6320.4243 - val_mae: 18.5144\n",
      "Epoch 551/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 109.1100 - mae: 8.3410 - val_loss: 15308.0195 - val_mae: 23.2080\n",
      "Epoch 552/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 114.9699 - mae: 8.5581 - val_loss: 75964.6328 - val_mae: 43.0295\n",
      "Epoch 553/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.0348 - mae: 8.3271 - val_loss: 16225.2725 - val_mae: 21.8865\n",
      "Epoch 554/650\n",
      "245/245 [==============================] - 48s 194ms/step - loss: 106.9237 - mae: 8.2527 - val_loss: 262007.0938 - val_mae: 72.3578\n",
      "Epoch 555/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 109.7403 - mae: 8.2947 - val_loss: 72579.2031 - val_mae: 42.3711\n",
      "Epoch 556/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.0739 - mae: 8.2374 - val_loss: 39757.8398 - val_mae: 29.9714\n",
      "Epoch 557/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 114.0783 - mae: 8.4615 - val_loss: 64839.9414 - val_mae: 44.1507\n",
      "Epoch 558/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 105.9976 - mae: 8.1426 - val_loss: 10099.1816 - val_mae: 21.3162\n",
      "Epoch 559/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.9923 - mae: 8.3713 - val_loss: 3215.9202 - val_mae: 15.8820\n",
      "Epoch 560/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 107.4793 - mae: 8.2829 - val_loss: 13324.4414 - val_mae: 22.2666\n",
      "Epoch 561/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 110.9145 - mae: 8.3579 - val_loss: 3771.1279 - val_mae: 15.2510\n",
      "Epoch 562/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 105.4297 - mae: 8.1201 - val_loss: 7595.0405 - val_mae: 16.9158\n",
      "Epoch 563/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 116.4357 - mae: 8.6447 - val_loss: 28772.0957 - val_mae: 29.4205\n",
      "Epoch 564/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.8368 - mae: 8.3779 - val_loss: 25756.9531 - val_mae: 27.8831\n",
      "Epoch 565/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 112.1164 - mae: 8.4485 - val_loss: 7521.0581 - val_mae: 15.9625\n",
      "Epoch 566/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.1255 - mae: 8.5870 - val_loss: 75243.2344 - val_mae: 44.1488\n",
      "Epoch 567/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 114.6203 - mae: 8.5531 - val_loss: 15843.1797 - val_mae: 21.5866\n",
      "Epoch 568/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 107.8872 - mae: 8.2558 - val_loss: 76110.3125 - val_mae: 37.8251\n",
      "Epoch 569/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 105.0650 - mae: 8.2054 - val_loss: 21552.9414 - val_mae: 24.8132\n",
      "Epoch 570/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 107.6470 - mae: 8.3383 - val_loss: 25411.4785 - val_mae: 24.8631\n",
      "Epoch 571/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 110.1416 - mae: 8.3795 - val_loss: 57615.5859 - val_mae: 43.2518\n",
      "Epoch 572/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 106.2914 - mae: 8.1696 - val_loss: 9948.7578 - val_mae: 14.7646\n",
      "Epoch 573/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 108.2290 - mae: 8.3314 - val_loss: 12283.1279 - val_mae: 20.1366\n",
      "Epoch 574/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 114.6118 - mae: 8.5823 - val_loss: 2165.4077 - val_mae: 15.9033\n",
      "Epoch 575/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 109.8584 - mae: 8.2474 - val_loss: 8602.9082 - val_mae: 23.2586\n",
      "Epoch 576/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 109.6856 - mae: 8.3072 - val_loss: 761.6381 - val_mae: 11.7577\n",
      "Epoch 577/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 107.9770 - mae: 8.2296 - val_loss: 450.1700 - val_mae: 10.0966\n",
      "Epoch 578/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 109.5320 - mae: 8.3126 - val_loss: 1471.8828 - val_mae: 14.0029\n",
      "Epoch 579/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 112.6398 - mae: 8.4902 - val_loss: 682.0892 - val_mae: 11.5237\n",
      "Epoch 580/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 109.4905 - mae: 8.3612 - val_loss: 2451.0261 - val_mae: 17.4318\n",
      "Epoch 581/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 106.1234 - mae: 8.1517 - val_loss: 1080.4031 - val_mae: 12.3366\n",
      "Epoch 582/650\n",
      "245/245 [==============================] - 46s 186ms/step - loss: 108.0293 - mae: 8.2666 - val_loss: 10372.6357 - val_mae: 24.0839\n",
      "Epoch 583/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 105.7069 - mae: 8.2222 - val_loss: 1363.6790 - val_mae: 13.2025\n",
      "Epoch 584/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 111.0368 - mae: 8.4123 - val_loss: 3655.1392 - val_mae: 17.9520\n",
      "Epoch 585/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.6879 - mae: 8.3546 - val_loss: 2336.6482 - val_mae: 15.6554\n",
      "Epoch 586/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 110.6269 - mae: 8.4106 - val_loss: 1409.8619 - val_mae: 13.3871\n",
      "Epoch 587/650\n",
      "245/245 [==============================] - 47s 194ms/step - loss: 112.4432 - mae: 8.4274 - val_loss: 592.1194 - val_mae: 11.5486\n",
      "Epoch 588/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 101.4134 - mae: 7.9865 - val_loss: 501.2967 - val_mae: 10.3186\n",
      "Epoch 589/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 111.7045 - mae: 8.4690 - val_loss: 1776.2587 - val_mae: 14.6489\n",
      "Epoch 590/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 110.4453 - mae: 8.3703 - val_loss: 1100.1453 - val_mae: 13.2744\n",
      "Epoch 591/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 108.7792 - mae: 8.2154 - val_loss: 5810.7222 - val_mae: 22.5648\n",
      "Epoch 592/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 109.0526 - mae: 8.3139 - val_loss: 2737.2327 - val_mae: 15.5487\n",
      "Epoch 593/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.4593 - mae: 8.3898 - val_loss: 1877.4890 - val_mae: 14.7003\n",
      "Epoch 594/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 109.0814 - mae: 8.2196 - val_loss: 2012.3252 - val_mae: 14.5645\n",
      "Epoch 595/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 105.4477 - mae: 8.1178 - val_loss: 786.4237 - val_mae: 12.2005\n",
      "Epoch 596/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 104.8106 - mae: 8.0724 - val_loss: 2394.4207 - val_mae: 14.6255\n",
      "Epoch 597/650\n",
      "245/245 [==============================] - 46s 187ms/step - loss: 112.0328 - mae: 8.4254 - val_loss: 1242.9839 - val_mae: 12.6632\n",
      "Epoch 598/650\n",
      "245/245 [==============================] - 45s 182ms/step - loss: 115.6160 - mae: 8.4848 - val_loss: 5951.3042 - val_mae: 21.7354\n",
      "Epoch 599/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.3395 - mae: 8.3317 - val_loss: 6515.9575 - val_mae: 19.3668\n",
      "Epoch 600/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 103.1796 - mae: 7.9935 - val_loss: 6357.5928 - val_mae: 19.3784\n",
      "Epoch 601/650\n",
      "245/245 [==============================] - 46s 186ms/step - loss: 113.2019 - mae: 8.5042 - val_loss: 2207.9011 - val_mae: 15.6218\n",
      "Epoch 602/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 103.9268 - mae: 8.1658 - val_loss: 526.2686 - val_mae: 11.3023\n",
      "Epoch 603/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 108.9246 - mae: 8.2917 - val_loss: 890.1455 - val_mae: 12.1391\n",
      "Epoch 604/650\n",
      "245/245 [==============================] - 84s 345ms/step - loss: 113.6391 - mae: 8.4639 - val_loss: 1466.1956 - val_mae: 13.3307\n",
      "Epoch 605/650\n",
      "245/245 [==============================] - 94s 384ms/step - loss: 112.1415 - mae: 8.5229 - val_loss: 1965.5146 - val_mae: 13.7051\n",
      "Epoch 606/650\n",
      "245/245 [==============================] - 93s 380ms/step - loss: 112.9431 - mae: 8.4601 - val_loss: 403.2387 - val_mae: 10.7911\n",
      "Epoch 607/650\n",
      "245/245 [==============================] - 92s 376ms/step - loss: 105.6848 - mae: 8.1533 - val_loss: 1118.8156 - val_mae: 14.6600\n",
      "Epoch 608/650\n",
      "245/245 [==============================] - 60s 244ms/step - loss: 109.2353 - mae: 8.2673 - val_loss: 1256.7727 - val_mae: 14.0229\n",
      "Epoch 609/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.7522 - mae: 8.3921 - val_loss: 743.5477 - val_mae: 11.7904\n",
      "Epoch 610/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 106.2387 - mae: 8.2004 - val_loss: 4146.5273 - val_mae: 18.1848\n",
      "Epoch 611/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 105.6118 - mae: 8.2104 - val_loss: 2199.3459 - val_mae: 13.2433\n",
      "Epoch 612/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 109.1055 - mae: 8.2675 - val_loss: 14578.4805 - val_mae: 25.1377\n",
      "Epoch 613/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 105.2930 - mae: 8.1675 - val_loss: 1321.9674 - val_mae: 13.3101\n",
      "Epoch 614/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 109.0842 - mae: 8.2141 - val_loss: 1025.4943 - val_mae: 12.5831\n",
      "Epoch 615/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.7710 - mae: 8.5444 - val_loss: 2246.1858 - val_mae: 14.7483\n",
      "Epoch 616/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 110.9656 - mae: 8.3675 - val_loss: 2131.6853 - val_mae: 16.6119\n",
      "Epoch 617/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 109.8385 - mae: 8.3144 - val_loss: 1142.1920 - val_mae: 13.5769\n",
      "Epoch 618/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 106.3229 - mae: 8.2466 - val_loss: 1754.3888 - val_mae: 15.9333\n",
      "Epoch 619/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 103.9353 - mae: 8.1871 - val_loss: 1884.7740 - val_mae: 14.9397\n",
      "Epoch 620/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 113.6793 - mae: 8.5076 - val_loss: 19732.0488 - val_mae: 25.9077\n",
      "Epoch 621/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 107.8069 - mae: 8.3151 - val_loss: 4530.8389 - val_mae: 17.3408\n",
      "Epoch 622/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 103.3810 - mae: 8.1062 - val_loss: 10097.0537 - val_mae: 25.3325\n",
      "Epoch 623/650\n",
      "245/245 [==============================] - 45s 183ms/step - loss: 109.1676 - mae: 8.2722 - val_loss: 6147.5405 - val_mae: 20.6226\n",
      "Epoch 624/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 113.3001 - mae: 8.5272 - val_loss: 3676.4495 - val_mae: 18.0560\n",
      "Epoch 625/650\n",
      "245/245 [==============================] - 45s 185ms/step - loss: 106.7864 - mae: 8.2485 - val_loss: 829.4727 - val_mae: 12.0703\n",
      "Epoch 626/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 116.2018 - mae: 8.5645 - val_loss: 1653.1125 - val_mae: 14.9429\n",
      "Epoch 627/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.3647 - mae: 8.4466 - val_loss: 355.9974 - val_mae: 10.3956\n",
      "Epoch 628/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 106.9245 - mae: 8.1864 - val_loss: 581.3287 - val_mae: 11.4420\n",
      "Epoch 629/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 109.5618 - mae: 8.3612 - val_loss: 218.6448 - val_mae: 9.1207\n",
      "Epoch 630/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 109.7263 - mae: 8.3595 - val_loss: 890.4589 - val_mae: 13.4018\n",
      "Epoch 631/650\n",
      "245/245 [==============================] - 46s 190ms/step - loss: 106.8454 - mae: 8.2032 - val_loss: 452.7328 - val_mae: 11.5320\n",
      "Epoch 632/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 101.8970 - mae: 7.9562 - val_loss: 740.4348 - val_mae: 12.3998\n",
      "Epoch 633/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.7542 - mae: 8.2986 - val_loss: 516.7134 - val_mae: 11.1537\n",
      "Epoch 634/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 102.5522 - mae: 8.0402 - val_loss: 308.8014 - val_mae: 10.3685\n",
      "Epoch 635/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 111.9733 - mae: 8.4301 - val_loss: 491.9940 - val_mae: 11.1486\n",
      "Epoch 636/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 107.6222 - mae: 8.2003 - val_loss: 3196.9187 - val_mae: 20.5593\n",
      "Epoch 637/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 114.9378 - mae: 8.4235 - val_loss: 349.2943 - val_mae: 9.8960\n",
      "Epoch 638/650\n",
      "245/245 [==============================] - 45s 185ms/step - loss: 105.4658 - mae: 8.0937 - val_loss: 902.0508 - val_mae: 11.5584\n",
      "Epoch 639/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 111.1017 - mae: 8.3171 - val_loss: 2630.0986 - val_mae: 15.5581\n",
      "Epoch 640/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 106.6592 - mae: 8.1748 - val_loss: 7026.4395 - val_mae: 24.1601\n",
      "Epoch 641/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 108.0781 - mae: 8.2837 - val_loss: 1240.0249 - val_mae: 13.3495\n",
      "Epoch 642/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 109.0429 - mae: 8.2713 - val_loss: 1390.4534 - val_mae: 13.8594\n",
      "Epoch 643/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 111.9347 - mae: 8.4662 - val_loss: 1639.3168 - val_mae: 13.6327\n",
      "Epoch 644/650\n",
      "245/245 [==============================] - 47s 191ms/step - loss: 105.2144 - mae: 8.1346 - val_loss: 1822.7434 - val_mae: 14.8327\n",
      "Epoch 645/650\n",
      "245/245 [==============================] - 47s 192ms/step - loss: 108.1103 - mae: 8.2441 - val_loss: 1006.4745 - val_mae: 12.5406\n",
      "Epoch 646/650\n",
      "245/245 [==============================] - 46s 188ms/step - loss: 104.9022 - mae: 8.0928 - val_loss: 2740.5388 - val_mae: 15.8685\n",
      "Epoch 647/650\n",
      "245/245 [==============================] - 47s 190ms/step - loss: 107.6356 - mae: 8.3368 - val_loss: 1197.6892 - val_mae: 12.7894\n",
      "Epoch 648/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 108.5163 - mae: 8.2185 - val_loss: 8178.3066 - val_mae: 22.9188\n",
      "Epoch 649/650\n",
      "245/245 [==============================] - 46s 189ms/step - loss: 107.9393 - mae: 8.2934 - val_loss: 5862.3145 - val_mae: 19.5601\n",
      "Epoch 650/650\n",
      "245/245 [==============================] - 47s 193ms/step - loss: 113.7960 - mae: 8.5644 - val_loss: 6504.9473 - val_mae: 19.4386\n"
     ]
    }
   ],
   "source": [
    "# Train the model with your data\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, Load, and Display Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 32\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\n\u001b[1;32m     33\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     34\u001b[0m train_mae \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "def append_model_history_to_file(file_name, description, input_shape, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae):\n",
    "    train_mae = round(train_mae, 6)\n",
    "    validation_mae = round(validation_mae, 6)\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        with open(file_name, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['Model', 'Input Shape', 'Epochs', 'Batch Size', 'Optimizer', 'Loss Function', 'Train MAE', 'Validation MAE'])\n",
    "\n",
    "    with open(file_name, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        try:\n",
    "            next(csv_reader)  # Skip the header row\n",
    "        except StopIteration:\n",
    "            pass  # The CSV file is empty or only contains the header row\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if list(map(str, row[1:])) == list(map(str, [input_shape, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])):\n",
    "                print(\"Entry with the same parameters already exists.\")\n",
    "                return False\n",
    "\n",
    "    with open(file_name, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([description, input_shape, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])\n",
    "\n",
    "    return True\n",
    "\n",
    "timestamp = int(time.time())\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "description = f\"{timestamp}\"\n",
    "input_shape = str(X_train.shape[1:])\n",
    "optimizer = optimizer\n",
    "loss_function = loss\n",
    "train_mae = history.history['mae'][-1]\n",
    "validation_mae = history.history['val_mae'][-1]\n",
    "\n",
    "new_entry_added = append_model_history_to_file(file_name, description, input_shape, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae)\n",
    "\n",
    "# Save model\n",
    "def save_model_architecture(model, file_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "if new_entry_added:\n",
    "    model_architecture_file = f\"../04_Age_Prediction/models/{description}.json\"\n",
    "    save_model_architecture(model, model_architecture_file)\n",
    "else:\n",
    "    print(\"Model not saved as an entry with the same parameters already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input Shape</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Loss Function</th>\n",
       "      <th>Train MAE</th>\n",
       "      <th>Validation MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1683270923</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.241365</td>\n",
       "      <td>8.678753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1683286770</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>1000</td>\n",
       "      <td>16</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>6.187893</td>\n",
       "      <td>5.758430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1683433197</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.656551</td>\n",
       "      <td>7.317583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1683438058</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>10.114231</td>\n",
       "      <td>7.153706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1683439878</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>8.796492</td>\n",
       "      <td>7.445862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1683441408</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.388038</td>\n",
       "      <td>6.963918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1683442226</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>11.328677</td>\n",
       "      <td>8.919698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1683445490</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>9.128469</td>\n",
       "      <td>6.906603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1683477636</td>\n",
       "      <td>(200, 200, 3)</td>\n",
       "      <td>650</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>8.564434</td>\n",
       "      <td>19.438639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model    Input Shape  Epochs  Batch Size Optimizer  \\\n",
       "0  1683270923  (200, 200, 3)      10          16      adam   \n",
       "1  1683286770  (200, 200, 3)    1000          16      adam   \n",
       "2  1683433197  (200, 200, 3)      50          32      adam   \n",
       "3  1683438058  (200, 200, 3)      50          32      adam   \n",
       "4  1683439878  (200, 200, 3)      50          32      adam   \n",
       "5  1683441408  (200, 200, 3)      50          32      adam   \n",
       "6  1683442226  (200, 200, 3)      50          32      adam   \n",
       "7  1683445490  (200, 200, 3)      50          32      adam   \n",
       "8  1683477636  (200, 200, 3)     650          32      adam   \n",
       "\n",
       "        Loss Function  Train MAE  Validation MAE  \n",
       "0  mean_squared_error  10.241365        8.678753  \n",
       "1  mean_squared_error   6.187893        5.758430  \n",
       "2  mean_squared_error   9.656551        7.317583  \n",
       "3  mean_squared_error  10.114231        7.153706  \n",
       "4  mean_squared_error   8.796492        7.445862  \n",
       "5  mean_squared_error   9.388038        6.963918  \n",
       "6  mean_squared_error  11.328677        8.919698  \n",
       "7  mean_squared_error   9.128469        6.906603  \n",
       "8  mean_squared_error   8.564434       19.438639  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_to_dataframe(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"File does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "df = load_data_to_dataframe(file_name)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model to load\n",
    "model_to_load = \"1683270923\"\n",
    "\n",
    "def load_and_display_model_architecture(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"File does not exist.\")\n",
    "        return None\n",
    "\n",
    "    with open(file_name, \"r\") as json_file:\n",
    "        model_json = json_file.read()\n",
    "    \n",
    "    model = model_from_json(model_json)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "model_architecture_file = f\"../04_Age_Prediction/models/{model_to_load}.json\"\n",
    "model = load_and_display_model_architecture(model_architecture_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
