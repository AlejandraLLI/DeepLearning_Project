{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "image_path = '../02_Data/face_age' # Added path to gitingnore, you will have to add data to this path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_path, img_size=(200, 200)):\n",
    "    X = []\n",
    "    y = []\n",
    "    for folder in os.listdir(folder_path):\n",
    "        if os.path.isdir(os.path.join(folder_path, folder)):\n",
    "            age = int(folder)\n",
    "            for file in os.listdir(os.path.join(folder_path, folder)):\n",
    "                img_path = os.path.join(folder_path, folder, file)\n",
    "                img = Image.open(img_path)\n",
    "                img = img.resize(img_size)\n",
    "                img = np.array(img)\n",
    "                X.append(img)\n",
    "                y.append(age)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "folder_path = image_path\n",
    "img_size = (200, 200)\n",
    "X, y = load_data(folder_path, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (9778, 200, 200, 3) \n",
      " - 9778: Number of images in the dataset \n",
      " - 200: Height of each image \n",
      " - 200: Width of each image \n",
      " - 3: Number of channels of each image (Red, Green, and Blue)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Shape of X: {X.shape}\", \"\\n\",\n",
    "    f\"- {X.shape[0]}: Number of images in the dataset\", \"\\n\",\n",
    "    f\"- {X.shape[1]}: Height of each image\", \"\\n\",\n",
    "    f\"- {X.shape[2]}: Width of each image\", \"\\n\",\n",
    "    f\"- {X.shape[3]}: Number of channels of each image (Red, Green, and Blue)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [  1   1   1 ... 101 110 110]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "X = X / 255.0\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of arrays: \n",
      " - X_train shape: (7822, 200, 200, 3) \n",
      " - X_test shape: (1956, 200, 200, 3) \n",
      " - y_train shape: (7822,) \n",
      " - y_test shape: (1956,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Size of arrays:\", \"\\n\",\n",
    "    f\"- X_train shape: {X_train.shape}\", \"\\n\",\n",
    "    f\"- X_test shape: {X_test.shape}\", \"\\n\",\n",
    "    f\"- y_train shape: {y_train.shape}\", \"\\n\",\n",
    "    f\"- y_test shape: {y_test.shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = \"adam\"\n",
    "loss = \"mean_squared_error\"\n",
    "metrics = ['mae']\n",
    "\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (5, 5), padding='same', input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(64),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "input_shape = (img_size[0], img_size[1], 3)\n",
    "model = create_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "489/489 [==============================] - 26s 35ms/step - loss: 935.9125 - mae: 24.6942 - val_loss: 781.6656 - val_mae: 23.1869\n",
      "Epoch 2/1000\n",
      "489/489 [==============================] - 14s 29ms/step - loss: 288.4002 - mae: 12.9678 - val_loss: 174.8781 - val_mae: 9.5067\n",
      "Epoch 3/1000\n",
      "489/489 [==============================] - 15s 30ms/step - loss: 189.3459 - mae: 10.3354 - val_loss: 110.8776 - val_mae: 7.5183\n",
      "Epoch 4/1000\n",
      "489/489 [==============================] - 15s 30ms/step - loss: 169.0360 - mae: 9.7430 - val_loss: 100.1402 - val_mae: 7.3854\n",
      "Epoch 5/1000\n",
      "489/489 [==============================] - 13s 27ms/step - loss: 161.7164 - mae: 9.4485 - val_loss: 114.0884 - val_mae: 7.9139\n",
      "Epoch 6/1000\n",
      "489/489 [==============================] - 14s 30ms/step - loss: 145.7326 - mae: 8.8815 - val_loss: 101.3391 - val_mae: 7.0285\n",
      "Epoch 7/1000\n",
      "489/489 [==============================] - 14s 28ms/step - loss: 145.5965 - mae: 8.9583 - val_loss: 89.0942 - val_mae: 6.7216\n",
      "Epoch 8/1000\n",
      "489/489 [==============================] - 15s 30ms/step - loss: 137.2162 - mae: 8.7161 - val_loss: 91.0994 - val_mae: 6.5336\n",
      "Epoch 9/1000\n",
      "412/489 [========================>.....] - ETA: 1s - loss: 124.8879 - mae: 8.3563"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 1000\n",
    "batch_size = 16\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, Load, and Display Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_model_history_to_file(file_name, description, input_shape, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae):\n",
    "    train_mae = round(train_mae, 6)\n",
    "    validation_mae = round(validation_mae, 6)\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        with open(file_name, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['Model', 'Input Shape', 'Epochs', 'Batch Size', 'Optimizer', 'Loss Function', 'Train MAE', 'Validation MAE'])\n",
    "\n",
    "    with open(file_name, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "        try:\n",
    "            next(csv_reader)  # Skip the header row\n",
    "        except StopIteration:\n",
    "            pass  # The CSV file is empty or only contains the header row\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if list(map(str, row[1:])) == list(map(str, [input_shape, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])):\n",
    "                print(\"Entry with the same parameters already exists.\")\n",
    "                return False\n",
    "\n",
    "    with open(file_name, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([description, input_shape, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae])\n",
    "\n",
    "    return True\n",
    "\n",
    "timestamp = int(time.time())\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "description = f\"{timestamp}\"\n",
    "input_shape = str(X_train.shape[1:])\n",
    "optimizer = optimizer\n",
    "loss_function = loss\n",
    "train_mae = history.history['mae'][-1]\n",
    "validation_mae = history.history['val_mae'][-1]\n",
    "\n",
    "new_entry_added = append_model_history_to_file(file_name, description, input_shape, epochs, batch_size, optimizer, loss_function, train_mae, validation_mae)\n",
    "\n",
    "# Save model\n",
    "def save_model_architecture(model, file_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(file_name, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "if new_entry_added:\n",
    "    model_architecture_file = f\"../04_Age_Prediction/models/{description}.json\"\n",
    "    save_model_architecture(model, model_architecture_file)\n",
    "else:\n",
    "    print(\"Model not saved as an entry with the same parameters already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_dataframe(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"File does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "file_name = '../04_Age_Prediction/model_history.csv'\n",
    "df = load_data_to_dataframe(file_name)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model to load\n",
    "model_to_load = \"1683270923\"\n",
    "\n",
    "def load_and_display_model_architecture(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"File does not exist.\")\n",
    "        return None\n",
    "\n",
    "    with open(file_name, \"r\") as json_file:\n",
    "        model_json = json_file.read()\n",
    "    \n",
    "    model = model_from_json(model_json)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "model_architecture_file = f\"../04_Age_Prediction/models/{model_to_load}.json\"\n",
    "model = load_and_display_model_architecture(model_architecture_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
